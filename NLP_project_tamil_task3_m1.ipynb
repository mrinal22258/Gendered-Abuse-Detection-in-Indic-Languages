{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a55e55-9f0a-41cd-a8f4-59376d0cb8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from skmultilearn.adapt import MLkNN\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import hamming_loss, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2653bbc-d3fb-4f22-98e6-d3426e3a85bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged dataset shape: (6779, 6)\n",
      "  unique_id                                               text binary_label_1  \\\n",
      "0      id_0     *1. à®®à¯à®°à®šà¯Šà®²à®¿ à®…à®²à¯à®µà®²à®•à®®à¯ à®…à®®à¯ˆà®¨à¯à®¤à¯à®³à¯à®³ à®‡à®Ÿà®®à¯ à®ªà®à¯à®šà®®à®¿...       not_hate   \n",
      "1      id_1     à®šà¯‹à®¤à¯à®¤à¯à®•à¯à®•à¯ à®ªà®¿à®šà¯à®šà¯ˆ à®à®Ÿà¯à®•à¯à®•à®¿à®± à®•à®Ÿà®™à¯à®•à®¾à®° à®¨à®¾à®¯à¯à®•à®³à¯à®•...       not_hate   \n",
      "2      id_2           à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®©à¯à®©à¯ à®à®¤à®¾à®µà®¤à¯ à®ªà¯à®°à®¿à®¯à¯à®¤à®¾       not_hate   \n",
      "3      id_3      à®ªà®šà¯à®šà¯ˆ à®®à¯Šà®³à®•à®¾ à®•à®¾à®°à®®à¯ vicky à®…à®®à¯à®®à®¾ à®ªà¯à®£à¯à®Ÿà¯ˆ à®¨à®¾à®±à¯à®®à¯ ğŸ˜†           hate   \n",
      "4      id_4    à®à®©à¯à®© à®‰à®Ÿà®®à¯à®ªà¯ à®Ÿà®¾ à®šà®¾à®®à®¿- à®šà¯à®®à¯à®®à®¾ à®µà®³à¯à®µà®³à¯à®©à¯.. à®®à¯à®²à¯ˆ ...           hate   \n",
      "\n",
      "   label_1 binary_label_3  label_3  \n",
      "0        0   not_explicit        0  \n",
      "1        0       explicit        1  \n",
      "2        0   not_explicit        0  \n",
      "3        1       explicit        1  \n",
      "4        1       explicit        1  \n",
      "\n",
      "Label distribution:\n",
      "Label 1 (gendered abuse): {'not_hate': 3822, 'hate': 2957}\n",
      "Label 3 (explicit/aggressive): {'explicit': 4400, 'not_explicit': 2379}\n"
     ]
    }
   ],
   "source": [
    "# Process label 1 dataset\n",
    "df_l1 = pd.read_csv('train_ta_l1.csv')\n",
    "df_l1 = df_l1.rename(columns={'key': 'unique_id', 'sentence': 'text'})\n",
    "\n",
    "# Convert annotator columns to numeric without replacing NaNs\n",
    "df_l1[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5', 'ta_a6']] = df_l1[\n",
    "    ['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5', 'ta_a6']\n",
    "].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Compute 'label_1' based on majority voting while ignoring NaNs\n",
    "df_l1['label_1'] = (df_l1[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5', 'ta_a6']].mean(axis=1, skipna=True) >= 0.5).astype(int)\n",
    "\n",
    "# Create proper binary label for task 1 (gendered abuse)\n",
    "df_l1['binary_label_1'] = df_l1['label_1'].apply(lambda x: 'hate' if x == 1 else 'not_hate')\n",
    "\n",
    "# Process label 3 dataset\n",
    "df_l3 = pd.read_csv('train_ta_l3.csv')\n",
    "df_l3 = df_l3.rename(columns={'key': 'unique_id', 'sentence': 'text'})\n",
    "\n",
    "# Convert annotator columns to numeric without replacing NaNs\n",
    "df_l3[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5', 'ta_a6']] = df_l3[\n",
    "    ['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5', 'ta_a6']\n",
    "].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Compute 'label_3' based on majority voting while ignoring NaNs\n",
    "df_l3['label_3'] = (df_l3[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5', 'ta_a6']].mean(axis=1, skipna=True) >= 0.5).astype(int)\n",
    "\n",
    "# Create proper binary label for task 3 (explicit/aggressive)\n",
    "df_l3['binary_label_3'] = df_l3['label_3'].apply(lambda x: 'explicit' if x == 1 else 'not_explicit')\n",
    "\n",
    "# Select columns for merging\n",
    "df_l1_slim = df_l1[['text', 'label_1', 'binary_label_1']]\n",
    "df_l3_slim = df_l3[['text', 'label_3', 'binary_label_3']]\n",
    "\n",
    "# Merge the datasets based on text field\n",
    "merged_df = pd.merge(df_l1_slim, df_l3_slim, on='text', how='inner')\n",
    "\n",
    "# Add a unique_id column to the merged dataset\n",
    "merged_df['unique_id'] = [f'id_{i}' for i in range(len(merged_df))]\n",
    "\n",
    "# Reorder columns\n",
    "merged_df = merged_df[['unique_id', 'text', 'binary_label_1', 'label_1', 'binary_label_3', 'label_3']]\n",
    "\n",
    "# Save the merged dataset\n",
    "merged_df.to_csv('train_ta_task3.csv', index=False)\n",
    "\n",
    "# Display information\n",
    "print(f\"Final merged dataset shape: {merged_df.shape}\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# Check label distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(f\"Label 1 (gendered abuse): {merged_df['binary_label_1'].value_counts().to_dict()}\")\n",
    "print(f\"Label 3 (explicit/aggressive): {merged_df['binary_label_3'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d1d608c-4c5b-4c20-bd3d-6abce1419d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original vs Processed Text Samples:\n",
      "Original:    *1. à®®à¯à®°à®šà¯Šà®²à®¿ à®…à®²à¯à®µà®²à®•à®®à¯ à®…à®®à¯ˆà®¨à¯à®¤à¯à®³à¯à®³ à®‡à®Ÿà®®à¯ à®ªà®à¯à®šà®®à®¿ à®¨à®¿à®²à®®à¯ à®‡à®²à¯à®²à¯ˆ à®à®©à¯à®ªà®¤à¯ˆ à®¨à®¿à®°à¯‚à®ªà®¿à®•à¯à®• 1985-à®†à®®à¯ à®†à®£à¯à®Ÿà¯ à®µà®¾à®™à¯à®•à®ªà¯à®ªà®Ÿà¯à®Ÿ à®ªà®Ÿà¯à®Ÿà®¾à®µà¯ˆ à®†à®¤à®¾à®°à®®à®¾à®•à®•à¯ à®•à®¾à®Ÿà¯à®Ÿà®¿à®¯à®¿à®°à¯à®•à¯à®•à®¿à®±à®¾à®°à¯ à®®à¯.à®•.à®¸à¯à®Ÿà®¾à®²à®¿à®©à¯.  à®‡à®¤à®±à¯à®•à¯ à®•à®¾à®Ÿà¯à®Ÿ à®µà¯‡à®£à¯à®Ÿà®¿à®¯...  \n",
      "Processed:        à®®à¯à®°à®šà¯Šà®²à®¿ à®…à®²à¯à®µà®²à®•à®®à¯ à®…à®®à¯ˆà®¨à¯à®¤à¯à®³à¯à®³ à®‡à®Ÿà®®à¯ à®ªà®à¯à®šà®®à®¿ à®¨à®¿à®²à®®à¯ à®‡à®²à¯à®²à¯ˆ à®à®©à¯à®ªà®¤à¯ˆ à®¨à®¿à®°à¯‚à®ªà®¿à®•à¯à®•   à®†à®®à¯ à®†à®£à¯à®Ÿà¯ à®µà®¾à®™à¯à®•à®ªà¯à®ªà®Ÿà¯à®Ÿ à®ªà®Ÿà¯à®Ÿà®¾à®µà¯ˆ à®†à®¤à®¾à®°à®®à®¾à®•à®•à¯ à®•à®¾à®Ÿà¯à®Ÿà®¿à®¯à®¿à®°à¯à®•à¯à®•à®¿à®±à®¾à®°à¯ à®®à¯ à®• à®¸à¯à®Ÿà®¾à®²à®¿à®©à¯   à®‡à®¤à®±à¯à®•à¯ à®•à®¾à®Ÿà¯à®Ÿ à®µà¯‡à®£à¯à®Ÿà®¿à®¯     \n",
      "--------------------------------------------------\n",
      "Original:    à®šà¯‹à®¤à¯à®¤à¯à®•à¯à®•à¯ à®ªà®¿à®šà¯à®šà¯ˆ à®à®Ÿà¯à®•à¯à®•à®¿à®± à®•à®Ÿà®™à¯à®•à®¾à®° à®¨à®¾à®¯à¯à®•à®³à¯à®•à¯à®•à¯ à®ªà¯‡à®šà¯à®šà¯ à®ªà¯à®£à¯à®Ÿà¯ˆà®¯à¯ˆ  à®ªà®¾à®°à¯..   à®ªà¯‹à®¯à¯ à®šà¯€à®©à®¾ à®•à®¾à®°à®©à¯à®•à¯à®•à¯ à®šà¯‚à®¤à¯à®¤à¯ à®•à¯à®Ÿà¯ à®ªà¯‹\n",
      "Processed:    à®šà¯‹à®¤à¯à®¤à¯à®•à¯à®•à¯ à®ªà®¿à®šà¯à®šà¯ˆ à®à®Ÿà¯à®•à¯à®•à®¿à®± à®•à®Ÿà®™à¯à®•à®¾à®° à®¨à®¾à®¯à¯à®•à®³à¯à®•à¯à®•à¯ à®ªà¯‡à®šà¯à®šà¯ à®ªà¯à®£à¯à®Ÿà¯ˆà®¯à¯ˆ  à®ªà®¾à®°à¯     à®ªà¯‹à®¯à¯ à®šà¯€à®©à®¾ à®•à®¾à®°à®©à¯à®•à¯à®•à¯ à®šà¯‚à®¤à¯à®¤à¯ à®•à¯à®Ÿà¯ à®ªà¯‹\n",
      "--------------------------------------------------\n",
      "Original:    à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®©à¯à®©à¯ à®à®¤à®¾à®µà®¤à¯ à®ªà¯à®°à®¿à®¯à¯à®¤à®¾\n",
      "Processed:    à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®©à¯à®©à¯ à®à®¤à®¾à®µà®¤à¯ à®ªà¯à®°à®¿à®¯à¯à®¤à®¾\n",
      "--------------------------------------------------\n",
      "\n",
      "Processed dataset shape: (6779, 7)\n",
      "  unique_id                                               text  \\\n",
      "0      id_0     *1. à®®à¯à®°à®šà¯Šà®²à®¿ à®…à®²à¯à®µà®²à®•à®®à¯ à®…à®®à¯ˆà®¨à¯à®¤à¯à®³à¯à®³ à®‡à®Ÿà®®à¯ à®ªà®à¯à®šà®®à®¿...   \n",
      "1      id_1     à®šà¯‹à®¤à¯à®¤à¯à®•à¯à®•à¯ à®ªà®¿à®šà¯à®šà¯ˆ à®à®Ÿà¯à®•à¯à®•à®¿à®± à®•à®Ÿà®™à¯à®•à®¾à®° à®¨à®¾à®¯à¯à®•à®³à¯à®•...   \n",
      "2      id_2           à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®©à¯à®©à¯ à®à®¤à®¾à®µà®¤à¯ à®ªà¯à®°à®¿à®¯à¯à®¤à®¾   \n",
      "3      id_3      à®ªà®šà¯à®šà¯ˆ à®®à¯Šà®³à®•à®¾ à®•à®¾à®°à®®à¯ vicky à®…à®®à¯à®®à®¾ à®ªà¯à®£à¯à®Ÿà¯ˆ à®¨à®¾à®±à¯à®®à¯ ğŸ˜†   \n",
      "4      id_4    à®à®©à¯à®© à®‰à®Ÿà®®à¯à®ªà¯ à®Ÿà®¾ à®šà®¾à®®à®¿- à®šà¯à®®à¯à®®à®¾ à®µà®³à¯à®µà®³à¯à®©à¯.. à®®à¯à®²à¯ˆ ...   \n",
      "\n",
      "                                      processed_text binary_label_1  label_1  \\\n",
      "0         à®®à¯à®°à®šà¯Šà®²à®¿ à®…à®²à¯à®µà®²à®•à®®à¯ à®…à®®à¯ˆà®¨à¯à®¤à¯à®³à¯à®³ à®‡à®Ÿà®®à¯ à®ªà®à¯à®šà®®à®¿...       not_hate        0   \n",
      "1     à®šà¯‹à®¤à¯à®¤à¯à®•à¯à®•à¯ à®ªà®¿à®šà¯à®šà¯ˆ à®à®Ÿà¯à®•à¯à®•à®¿à®± à®•à®Ÿà®™à¯à®•à®¾à®° à®¨à®¾à®¯à¯à®•à®³à¯à®•...       not_hate        0   \n",
      "2           à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®©à¯à®©à¯ à®à®¤à®¾à®µà®¤à¯ à®ªà¯à®°à®¿à®¯à¯à®¤à®¾       not_hate        0   \n",
      "3       à®ªà®šà¯à®šà¯ˆ à®®à¯Šà®³à®•à®¾ à®•à®¾à®°à®®à¯ vicky à®…à®®à¯à®®à®¾ à®ªà¯à®£à¯à®Ÿà¯ˆ à®¨à®¾à®±à¯à®®à¯            hate        1   \n",
      "4    à®à®©à¯à®© à®‰à®Ÿà®®à¯à®ªà¯ à®Ÿà®¾ à®šà®¾à®®à®¿  à®šà¯à®®à¯à®®à®¾ à®µà®³à¯à®µà®³à¯à®©à¯   à®®à¯à®²à¯ˆ ...           hate        1   \n",
      "\n",
      "  binary_label_3  label_3  \n",
      "0   not_explicit        0  \n",
      "1       explicit        1  \n",
      "2   not_explicit        0  \n",
      "3       explicit        1  \n",
      "4       explicit        1  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"\n",
    "                               u\"\\U0001F700-\\U0001F77F\"\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', ' ', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'<.*?>+', ' ', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', ' ', text)\n",
    "    text = re.sub(r'<handle replaced>', '', text)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "# Load the multi-task dataset\n",
    "merged_df = pd.read_csv('train_ta_task3.csv')\n",
    "\n",
    "# Apply text normalization\n",
    "merged_df['processed_text'] = merged_df['text'].apply(lambda x: normalize_text(x))\n",
    "\n",
    "# Further processing to remove '...'\n",
    "merged_df['processed_text'] = merged_df['processed_text'].str.replace('...', '')\n",
    "\n",
    "# Display samples of processed text\n",
    "print(\"\\nOriginal vs Processed Text Samples:\")\n",
    "for i in range(3):\n",
    "    print(f\"Original: {merged_df['text'].iloc[i]}\")\n",
    "    print(f\"Processed: {merged_df['processed_text'].iloc[i]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Keep all columns but add processed text\n",
    "merged_df_final = merged_df[['unique_id', 'text', 'processed_text', 'binary_label_1', 'label_1', 'binary_label_3', 'label_3']]\n",
    "\n",
    "\n",
    "print(f\"\\nProcessed dataset shape: {merged_df_final.shape}\")\n",
    "print(merged_df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63919df9-e6d4-4c9b-bad5-519a039af052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (processed text)\n",
    "X = list(merged_df_final['processed_text'])\n",
    "\n",
    "# Extract labels for both tasks\n",
    "y_task1 = merged_df_final['label_1'].values\n",
    "y_task3 = merged_df_final['label_3'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8e370b2-becf-48c2-810d-6c12d0bcb801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1557 2466  444 ...    0    0    0]\n",
      " [1396  323 2468 ...    0    0    0]\n",
      " [ 135  520 2470 ...    0    0    0]\n",
      " ...\n",
      " [3879 2142 1100 ...    0    0    0]\n",
      " [ 406  430  602 ...    0    0    0]\n",
      " [   4  850   13 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Activation, Dropout, Dense, Flatten,\n",
    "    Bidirectional, GRU, concatenate, SpatialDropout1D,\n",
    "    GlobalMaxPooling1D, GlobalAveragePooling1D, Conv1D,\n",
    "    Embedding, Input, Concatenate\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "######## Textual Features for Embedding ###################\n",
    "max_len = 100\n",
    "max_features = 4479\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding\n",
    "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "\n",
    "print(X)  # Check the processed sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fc435f6-bb3e-4f8c-be84-f16fb84222de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_task1 = label_encoder.fit_transform(y_task1)\n",
    "y_task3 = label_encoder.fit_transform(y_task3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ea26c49-37ea-4d3e-9a91-e2a0cb29e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_task1 = to_categorical(y_task1, num_classes=2)\n",
    "y_task3 = to_categorical(y_task3, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9800ac9-2cd3-499f-b9cd-4144d567f6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (4479, 50)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load GloVe embeddings from JSON\n",
    "with open('glove_embeddings.json', encoding=\"utf8\") as f:\n",
    "    embeddings_list = json.load(f)\n",
    "\n",
    "# Convert the list of vectors to a dictionary with word indices as keys\n",
    "embeddings_dictionary = {str(i): vector for i, vector in enumerate(embeddings_list)}\n",
    "\n",
    "# Define tokenizer \n",
    "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_features, vocab_size)  # Limit vocab to max_features\n",
    "\n",
    "# Get embedding dimension (from first vector in list)\n",
    "embed_size = len(embeddings_list[0]) if embeddings_list else 0\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "\n",
    "# Fill embedding matrix with corresponding word vectors\n",
    "for word, index in word_index.items():\n",
    "    if index >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_dictionary.get(word) or embeddings_dictionary.get(str(index))\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = np.asarray(embedding_vector, dtype=np.float32)\n",
    "\n",
    "print(\"Embedding matrix shape:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8dc67ab-21cf-42c7-90f9-c73df69fae61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5423\n",
      "Validation samples: 1356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krmri\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)           â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">223,950</span> â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ spatial_dropout1d             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)           â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)            â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,464</span> â”‚ spatial_dropout1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">197,632</span> â”‚ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task1_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚ global_average_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task3_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚ global_average_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ task1_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ task3_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task1_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                 â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> â”‚ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task3_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                 â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> â”‚ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m50\u001b[0m)           â”‚         \u001b[38;5;34m223,950\u001b[0m â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ spatial_dropout1d             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m50\u001b[0m)           â”‚               \u001b[38;5;34m0\u001b[0m â”‚ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”‚ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)            â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)           â”‚           \u001b[38;5;34m6,464\u001b[0m â”‚ spatial_dropout1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚         \u001b[38;5;34m197,632\u001b[0m â”‚ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task1_dense (\u001b[38;5;33mDense\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               â”‚          \u001b[38;5;34m32,896\u001b[0m â”‚ global_average_pooling1d[\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task3_dense (\u001b[38;5;33mDense\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               â”‚          \u001b[38;5;34m32,896\u001b[0m â”‚ global_average_pooling1d[\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ task1_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ task3_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task1_output (\u001b[38;5;33mDense\u001b[0m)          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                 â”‚             \u001b[38;5;34m258\u001b[0m â”‚ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task3_output (\u001b[38;5;33mDense\u001b[0m)          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                 â”‚             \u001b[38;5;34m258\u001b[0m â”‚ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">494,354</span> (1.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m494,354\u001b[0m (1.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">270,404</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m270,404\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">223,950</span> (874.80 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m223,950\u001b[0m (874.80 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.3422 - task1_output_accuracy: 0.5496 - task1_output_loss: 0.6877 - task1_output_macro_f1_score: 0.5496 - task3_output_accuracy: 0.6516 - task3_output_loss: 0.6545 - task3_output_macro_f1_score: 0.6516  \n",
      "Epoch 1: val_loss improved from inf to 1.32825, saving model to models_ta_task3_m1\\best_model_ta_task3_m1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 124ms/step - loss: 1.3421 - task1_output_accuracy: 0.5497 - task1_output_loss: 0.6877 - task1_output_macro_f1_score: 0.5497 - task3_output_accuracy: 0.6516 - task3_output_loss: 0.6544 - task3_output_macro_f1_score: 0.6516 - val_loss: 1.3283 - val_task1_output_accuracy: 0.5642 - val_task1_output_loss: 0.6834 - val_task1_output_macro_f1_score: 0.5642 - val_task3_output_accuracy: 0.6527 - val_task3_output_loss: 0.6436 - val_task3_output_macro_f1_score: 0.6527\n",
      "Epoch 2/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 1.3267 - task1_output_accuracy: 0.5641 - task1_output_loss: 0.6819 - task1_output_macro_f1_score: 0.5641 - task3_output_accuracy: 0.6468 - task3_output_loss: 0.6449 - task3_output_macro_f1_score: 0.6468  \n",
      "Epoch 2: val_loss improved from 1.32825 to 1.30570, saving model to models_ta_task3_m1\\best_model_ta_task3_m1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 103ms/step - loss: 1.3267 - task1_output_accuracy: 0.5641 - task1_output_loss: 0.6819 - task1_output_macro_f1_score: 0.5641 - task3_output_accuracy: 0.6468 - task3_output_loss: 0.6449 - task3_output_macro_f1_score: 0.6468 - val_loss: 1.3057 - val_task1_output_accuracy: 0.5737 - val_task1_output_loss: 0.6743 - val_task1_output_macro_f1_score: 0.5737 - val_task3_output_accuracy: 0.6593 - val_task3_output_loss: 0.6308 - val_task3_output_macro_f1_score: 0.6593\n",
      "Epoch 3/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 1.3037 - task1_output_accuracy: 0.5842 - task1_output_loss: 0.6731 - task1_output_macro_f1_score: 0.5842 - task3_output_accuracy: 0.6536 - task3_output_loss: 0.6306 - task3_output_macro_f1_score: 0.6536  \n",
      "Epoch 3: val_loss improved from 1.30570 to 1.28204, saving model to models_ta_task3_m1\\best_model_ta_task3_m1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 106ms/step - loss: 1.3037 - task1_output_accuracy: 0.5842 - task1_output_loss: 0.6731 - task1_output_macro_f1_score: 0.5842 - task3_output_accuracy: 0.6536 - task3_output_loss: 0.6306 - task3_output_macro_f1_score: 0.6536 - val_loss: 1.2820 - val_task1_output_accuracy: 0.5988 - val_task1_output_loss: 0.6639 - val_task1_output_macro_f1_score: 0.5988 - val_task3_output_accuracy: 0.6652 - val_task3_output_loss: 0.6181 - val_task3_output_macro_f1_score: 0.6652\n",
      "Epoch 4/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 1.2904 - task1_output_accuracy: 0.5832 - task1_output_loss: 0.6705 - task1_output_macro_f1_score: 0.5832 - task3_output_accuracy: 0.6703 - task3_output_loss: 0.6199 - task3_output_macro_f1_score: 0.6703 \n",
      "Epoch 4: val_loss did not improve from 1.28204\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 128ms/step - loss: 1.2904 - task1_output_accuracy: 0.5832 - task1_output_loss: 0.6705 - task1_output_macro_f1_score: 0.5832 - task3_output_accuracy: 0.6702 - task3_output_loss: 0.6199 - task3_output_macro_f1_score: 0.6702 - val_loss: 1.2823 - val_task1_output_accuracy: 0.6010 - val_task1_output_loss: 0.6664 - val_task1_output_macro_f1_score: 0.6010 - val_task3_output_accuracy: 0.6593 - val_task3_output_loss: 0.6150 - val_task3_output_macro_f1_score: 0.6593\n",
      "Epoch 5/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 1.2809 - task1_output_accuracy: 0.5832 - task1_output_loss: 0.6630 - task1_output_macro_f1_score: 0.5832 - task3_output_accuracy: 0.6598 - task3_output_loss: 0.6178 - task3_output_macro_f1_score: 0.6598 \n",
      "Epoch 5: val_loss did not improve from 1.28204\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 114ms/step - loss: 1.2808 - task1_output_accuracy: 0.5832 - task1_output_loss: 0.6630 - task1_output_macro_f1_score: 0.5832 - task3_output_accuracy: 0.6598 - task3_output_loss: 0.6178 - task3_output_macro_f1_score: 0.6598 - val_loss: 1.2901 - val_task1_output_accuracy: 0.5914 - val_task1_output_loss: 0.6679 - val_task1_output_macro_f1_score: 0.5914 - val_task3_output_accuracy: 0.6475 - val_task3_output_loss: 0.6210 - val_task3_output_macro_f1_score: 0.6475\n",
      "Epoch 6/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 1.2641 - task1_output_accuracy: 0.5891 - task1_output_loss: 0.6546 - task1_output_macro_f1_score: 0.5891 - task3_output_accuracy: 0.6792 - task3_output_loss: 0.6095 - task3_output_macro_f1_score: 0.6792 \n",
      "Epoch 6: val_loss improved from 1.28204 to 1.27609, saving model to models_ta_task3_m1\\best_model_ta_task3_m1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 123ms/step - loss: 1.2641 - task1_output_accuracy: 0.5891 - task1_output_loss: 0.6546 - task1_output_macro_f1_score: 0.5891 - task3_output_accuracy: 0.6792 - task3_output_loss: 0.6095 - task3_output_macro_f1_score: 0.6792 - val_loss: 1.2761 - val_task1_output_accuracy: 0.5914 - val_task1_output_loss: 0.6654 - val_task1_output_macro_f1_score: 0.5914 - val_task3_output_accuracy: 0.6681 - val_task3_output_loss: 0.6107 - val_task3_output_macro_f1_score: 0.6681\n",
      "Epoch 7/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 1.2484 - task1_output_accuracy: 0.5941 - task1_output_loss: 0.6490 - task1_output_macro_f1_score: 0.5941 - task3_output_accuracy: 0.6890 - task3_output_loss: 0.5995 - task3_output_macro_f1_score: 0.6890 \n",
      "Epoch 7: val_loss improved from 1.27609 to 1.27368, saving model to models_ta_task3_m1\\best_model_ta_task3_m1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 123ms/step - loss: 1.2485 - task1_output_accuracy: 0.5941 - task1_output_loss: 0.6490 - task1_output_macro_f1_score: 0.5941 - task3_output_accuracy: 0.6890 - task3_output_loss: 0.5995 - task3_output_macro_f1_score: 0.6890 - val_loss: 1.2737 - val_task1_output_accuracy: 0.6069 - val_task1_output_loss: 0.6632 - val_task1_output_macro_f1_score: 0.6069 - val_task3_output_accuracy: 0.6637 - val_task3_output_loss: 0.6101 - val_task3_output_macro_f1_score: 0.6637\n",
      "Epoch 8/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 1.2549 - task1_output_accuracy: 0.5905 - task1_output_loss: 0.6530 - task1_output_macro_f1_score: 0.5905 - task3_output_accuracy: 0.6877 - task3_output_loss: 0.6018 - task3_output_macro_f1_score: 0.6877 \n",
      "Epoch 8: val_loss did not improve from 1.27368\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 109ms/step - loss: 1.2548 - task1_output_accuracy: 0.5905 - task1_output_loss: 0.6530 - task1_output_macro_f1_score: 0.5905 - task3_output_accuracy: 0.6877 - task3_output_loss: 0.6018 - task3_output_macro_f1_score: 0.6877 - val_loss: 1.2779 - val_task1_output_accuracy: 0.5914 - val_task1_output_loss: 0.6646 - val_task1_output_macro_f1_score: 0.5914 - val_task3_output_accuracy: 0.6704 - val_task3_output_loss: 0.6133 - val_task3_output_macro_f1_score: 0.6704\n",
      "Epoch 9/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 1.2389 - task1_output_accuracy: 0.6160 - task1_output_loss: 0.6407 - task1_output_macro_f1_score: 0.6160 - task3_output_accuracy: 0.6873 - task3_output_loss: 0.5983 - task3_output_macro_f1_score: 0.6873 \n",
      "Epoch 9: val_loss did not improve from 1.27368\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 104ms/step - loss: 1.2389 - task1_output_accuracy: 0.6160 - task1_output_loss: 0.6407 - task1_output_macro_f1_score: 0.6160 - task3_output_accuracy: 0.6874 - task3_output_loss: 0.5982 - task3_output_macro_f1_score: 0.6874 - val_loss: 1.2886 - val_task1_output_accuracy: 0.5885 - val_task1_output_loss: 0.6761 - val_task1_output_macro_f1_score: 0.5885 - val_task3_output_accuracy: 0.6777 - val_task3_output_loss: 0.6118 - val_task3_output_macro_f1_score: 0.6777\n",
      "Epoch 10/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 1.2309 - task1_output_accuracy: 0.6056 - task1_output_loss: 0.6404 - task1_output_macro_f1_score: 0.6056 - task3_output_accuracy: 0.6948 - task3_output_loss: 0.5905 - task3_output_macro_f1_score: 0.6948 \n",
      "Epoch 10: val_loss did not improve from 1.27368\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 105ms/step - loss: 1.2308 - task1_output_accuracy: 0.6056 - task1_output_loss: 0.6404 - task1_output_macro_f1_score: 0.6056 - task3_output_accuracy: 0.6948 - task3_output_loss: 0.5905 - task3_output_macro_f1_score: 0.6948 - val_loss: 1.2840 - val_task1_output_accuracy: 0.5804 - val_task1_output_loss: 0.6751 - val_task1_output_macro_f1_score: 0.5804 - val_task3_output_accuracy: 0.6822 - val_task3_output_loss: 0.6083 - val_task3_output_macro_f1_score: 0.6822\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step  \n",
      "\n",
      "Task 1 (Gendered Abuse) Validation Results:\n",
      "Precision: 0.6188\n",
      "Recall: 0.6069\n",
      "weighted F1 Score: 0.5509\n",
      "macro F1 Score: 0.5258\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not_hate       0.60      0.90      0.72       765\n",
      "        hate       0.64      0.22      0.33       591\n",
      "\n",
      "    accuracy                           0.61      1356\n",
      "   macro avg       0.62      0.56      0.53      1356\n",
      "weighted avg       0.62      0.61      0.55      1356\n",
      "\n",
      "\n",
      "Task 3 (Explicit Language) Validation Results:\n",
      "Precision: 0.6376\n",
      "Recall: 0.6637\n",
      "weighted F1 Score: 0.6067\n",
      "macro F1 Score: 0.5337\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "not_explicit       0.56      0.19      0.29       477\n",
      "    explicit       0.68      0.92      0.78       879\n",
      "\n",
      "    accuracy                           0.66      1356\n",
      "   macro avg       0.62      0.56      0.53      1356\n",
      "weighted avg       0.64      0.66      0.61      1356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, SpatialDropout1D, Conv1D,\n",
    "    Bidirectional, LSTM, Dense, Dropout,\n",
    "    GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Configure GPU for optimal performance\n",
    "def configure_gpu():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Enable memory growth for each GPU\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "            # Use mixed precision for better performance\n",
    "            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "            tf.keras.mixed_precision.set_global_policy(policy)\n",
    "            print('Mixed precision enabled')\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "configure_gpu()\n",
    "\n",
    "# Multi-Task Model Definition\n",
    "def create_multi_task_cnn_bilstm_model(max_len, max_features, embedding_matrix, embed_size=300):\n",
    "    \"\"\"\n",
    "    Creates a multi-task CNN-BiLSTM model architecture for joint prediction of\n",
    "    gendered abuse (task1) and explicit language (task3)\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(max_len,))\n",
    "    \n",
    "    # Embedding layer with pretrained weights (GloVe/FastText)\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=max_features,\n",
    "        output_dim=embed_size,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_len,\n",
    "        trainable=False  # As per paper, embeddings are non-trainable\n",
    "    )(input_layer)\n",
    "    \n",
    "    # Spatial Dropout to prevent overfitting\n",
    "    spatial_dropout = SpatialDropout1D(0.2)(embedding_layer)\n",
    "    \n",
    "    # CNN Layer - shared representation\n",
    "    conv_layer = Conv1D(\n",
    "        filters=64,\n",
    "        kernel_size=2,\n",
    "        activation='relu',\n",
    "        padding='same'\n",
    "    )(spatial_dropout)\n",
    "    \n",
    "    # Bidirectional LSTM Layer - shared representation\n",
    "    bilstm_layer = Bidirectional(\n",
    "        LSTM(\n",
    "            units=128,\n",
    "            return_sequences=True,\n",
    "            dropout=0.1,\n",
    "            recurrent_dropout=0.1\n",
    "        )\n",
    "    )(conv_layer)\n",
    "    \n",
    "    # Global Average Pooling - shared representation\n",
    "    gap_layer = GlobalAveragePooling1D()(bilstm_layer)\n",
    "    \n",
    "    # Task-specific layers for Task 1 (Gendered Abuse)\n",
    "    task1_dense = Dense(128, activation='relu', name='task1_dense')(gap_layer)\n",
    "    task1_dropout = Dropout(0.1)(task1_dense)\n",
    "    task1_output = Dense(2, activation='softmax', name='task1_output', dtype='float32')(task1_dropout)\n",
    "    \n",
    "    # Task-specific layers for Task 3 (Explicit Language)\n",
    "    task3_dense = Dense(128, activation='relu', name='task3_dense')(gap_layer)\n",
    "    task3_dropout = Dropout(0.1)(task3_dense)\n",
    "    task3_output = Dense(2, activation='softmax', name='task3_output', dtype='float32')(task3_dropout)\n",
    "    \n",
    "    # Create model with multiple outputs\n",
    "    model = Model(inputs=input_layer, outputs=[task1_output, task3_output])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Custom MacroF1Score Metric for multi-task learning\n",
    "class MacroF1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes=2, name='macro_f1_score', **kwargs):\n",
    "        super(MacroF1Score, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Convert probabilities to predicted class indices\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        \n",
    "        # Convert one-hot encoded y_true to class indices if needed\n",
    "        if len(y_true.shape) > 1 and y_true.shape[-1] > 1:\n",
    "            y_true = tf.argmax(y_true, axis=-1)\n",
    "        \n",
    "        # Initialize confusion matrix\n",
    "        conf_matrix = tf.math.confusion_matrix(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            num_classes=self.num_classes,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        # Calculate TP, FP, FN for each class\n",
    "        diag = tf.linalg.diag_part(conf_matrix)\n",
    "        row_sum = tf.reduce_sum(conf_matrix, axis=1)\n",
    "        col_sum = tf.reduce_sum(conf_matrix, axis=0)\n",
    "        \n",
    "        tp = diag\n",
    "        fp = col_sum - diag\n",
    "        fn = row_sum - diag\n",
    "        \n",
    "        # Update the state variables\n",
    "        self.tp.assign_add(tf.reduce_sum(tp))\n",
    "        self.fp.assign_add(tf.reduce_sum(fp))\n",
    "        self.fn.assign_add(tf.reduce_sum(fn))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        # Calculate precision and recall\n",
    "        precision = self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())\n",
    "        recall = self.tp / (self.tp + self.fn + tf.keras.backend.epsilon())\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "        \n",
    "        # Return macro F1 (average of per-class F1 scores)\n",
    "        return f1\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.tp.assign(0.)\n",
    "        self.fp.assign(0.)\n",
    "        self.fn.assign(0.)\n",
    "        self.count.assign(0.)\n",
    "\n",
    "# Model Training for multi-task learning\n",
    "def train_and_validate_multi_task_model(model, X_train, y_train_task1, y_train_task3, \n",
    "                                         X_val, y_val_task1, y_val_task3, \n",
    "                                         batch_size=32, epochs=15, model_dir='models_ta_task3_m1'):\n",
    "    \"\"\"\n",
    "    Trains the multi-task CNN-BiLSTM model with early stopping and model checkpointing\n",
    "    Returns the best model and training history\n",
    "    \"\"\"\n",
    "    # Create directory for saving models if it doesn't exist\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        os.path.join(model_dir, 'best_model_ta_task3_m1.h5'),\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Compile model with Adam optimizer\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss={\n",
    "            'task1_output': 'categorical_crossentropy',\n",
    "            'task3_output': 'categorical_crossentropy'\n",
    "        },\n",
    "        loss_weights={\n",
    "            'task1_output': 1.0,  # Weight for gendered abuse task\n",
    "            'task3_output': 1.0   # Weight for explicit language task\n",
    "        },\n",
    "        metrics={\n",
    "            'task1_output': ['accuracy', MacroF1Score(num_classes=2)],\n",
    "            'task3_output': ['accuracy', MacroF1Score(num_classes=2)]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, \n",
    "        {'task1_output': y_train_task1, 'task3_output': y_train_task3},\n",
    "        validation_data=(\n",
    "            X_val, \n",
    "            {'task1_output': y_val_task1, 'task3_output': y_val_task3}\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping, model_checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Load the best model found during training\n",
    "    best_model = load_model(\n",
    "        os.path.join(model_dir, 'best_model_ta_task3_m1.h5'), \n",
    "        custom_objects={'MacroF1Score': MacroF1Score}\n",
    "    )\n",
    "    \n",
    "    return history, best_model\n",
    "\n",
    "# Plot Training History for multi-task model\n",
    "def plot_multi_task_training_history(history, plot_dir='plots_ta_task3_m1'):\n",
    "    \"\"\"\n",
    "    Plots training history for both tasks (accuracy and loss curves)\n",
    "    Saves plots to specified directory\n",
    "    \"\"\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    # Task 1 (Gendered Abuse) plots\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot task1 accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history.history['task1_output_accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_task1_output_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Task 1 (Gendered Abuse) Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot task1 loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history.history['task1_output_loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_task1_output_loss'], label='Validation Loss')\n",
    "    plt.title('Task 1 (Gendered Abuse) Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Task 3 (Explicit Language) plots\n",
    "    # Plot task3 accuracy\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history.history['task3_output_accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_task3_output_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Task 3 (Explicit Language) Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot task3 loss\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(history.history['task3_output_loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_task3_output_loss'], label='Validation Loss')\n",
    "    plt.title('Task 3 (Explicit Language) Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, 'training_history_ta_task3_m1.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot combined loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Total Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Total Validation Loss')\n",
    "    plt.title('Multi-Task Model Combined Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, 'combined_loss_ta_task3_m1.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Evaluation function for multi-task model\n",
    "def evaluate_multi_task_validation(model, X_val, y_val_task1, y_val_task3, plot_dir='plots_ta_task3_m1'):\n",
    "    \"\"\"\n",
    "    Evaluates the multi-task model on validation data and saves metrics and plots\n",
    "    \"\"\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_pred_task1, y_pred_task3 = model.predict(X_val, batch_size=32)\n",
    "    \n",
    "    # Convert to class labels\n",
    "    y_pred_task1_labels = np.argmax(y_pred_task1, axis=1)\n",
    "    y_true_task1_labels = np.argmax(y_val_task1, axis=1)\n",
    "    \n",
    "    y_pred_task3_labels = np.argmax(y_pred_task3, axis=1)\n",
    "    y_true_task3_labels = np.argmax(y_val_task3, axis=1)\n",
    "    \n",
    "    # Task 1 (Gendered Abuse) metrics\n",
    "    task1_precision = precision_score(y_true_task1_labels, y_pred_task1_labels, average='weighted')\n",
    "    task1_recall = recall_score(y_true_task1_labels, y_pred_task1_labels, average='weighted')\n",
    "    task1_weighted_f1 = f1_score(y_true_task1_labels, y_pred_task1_labels, average='weighted')\n",
    "    task1_macro_f1 = f1_score(y_true_task1_labels, y_pred_task1_labels, average='macro')\n",
    "    \n",
    "    # Task 3 (Explicit Language) metrics\n",
    "    task3_precision = precision_score(y_true_task3_labels, y_pred_task3_labels, average='weighted')\n",
    "    task3_recall = recall_score(y_true_task3_labels, y_pred_task3_labels, average='weighted')\n",
    "    task3_weighted_f1 = f1_score(y_true_task3_labels, y_pred_task3_labels, average='weighted')\n",
    "    task3_macro_f1 = f1_score(y_true_task3_labels, y_pred_task3_labels, average='macro')\n",
    "    \n",
    "    # Classification reports\n",
    "    task1_report = classification_report(y_true_task1_labels, y_pred_task1_labels, \n",
    "                                        target_names=['not_hate', 'hate'])\n",
    "    \n",
    "    task3_report = classification_report(y_true_task3_labels, y_pred_task3_labels, \n",
    "                                        target_names=['not_explicit', 'explicit'])\n",
    "    \n",
    "    # Confusion matrices\n",
    "    task1_conf_matrix = confusion_matrix(y_true_task1_labels, y_pred_task1_labels)\n",
    "    task3_conf_matrix = confusion_matrix(y_true_task3_labels, y_pred_task3_labels)\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    # Task 1 confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(task1_conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Not Hate', 'Hate'],\n",
    "                yticklabels=['Not Hate', 'Hate'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Task 1 (Gendered Abuse) Confusion Matrix')\n",
    "    plt.savefig(os.path.join(plot_dir, 'task1_confusion_matrix_ta_task3_m1.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Task 3 confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(task3_conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Not Explicit', 'Explicit'],\n",
    "                yticklabels=['Not Explicit', 'Explicit'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Task 3 (Explicit Language) Confusion Matrix')\n",
    "    plt.savefig(os.path.join(plot_dir, 'task3_confusion_matrix_ta_task3_m1.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'task1': {\n",
    "            'precision': task1_precision,\n",
    "            'recall': task1_recall,\n",
    "            'f1_score_weighted': task1_weighted_f1,\n",
    "            'f1_score_macro': task1_macro_f1,\n",
    "            'classification_report': task1_report,\n",
    "            'confusion_matrix': task1_conf_matrix\n",
    "        },\n",
    "        'task3': {\n",
    "            'precision': task3_precision,\n",
    "            'recall': task3_recall,\n",
    "            'f1_score_weighted': task3_weighted_f1,\n",
    "            'f1_score_macro': task3_macro_f1,\n",
    "            'classification_report': task3_report,\n",
    "            'confusion_matrix': task3_conf_matrix\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Main Execution for Training and Validation\n",
    "if __name__ == \"__main__\":\n",
    "    # Split into train (80%) and validation (20%)\n",
    "    X_train, X_val, y_train_task1, y_val_task1, y_train_task3, y_val_task3 = train_test_split(\n",
    "        X, y_task1, y_task3, test_size=0.2, random_state=42, stratify=y_task1\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Validation samples: {len(X_val)}\")\n",
    "    \n",
    "    # Create multi-task model\n",
    "    embed_size = embedding_matrix.shape[1]\n",
    "    model = create_multi_task_cnn_bilstm_model(max_len, max_features, embedding_matrix, embed_size)\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Train multi-task model\n",
    "    history, trained_model = train_and_validate_multi_task_model(\n",
    "        model, X_train, y_train_task1, y_train_task3, \n",
    "        X_val, y_val_task1, y_val_task3,\n",
    "        batch_size=32,\n",
    "        epochs=15  \n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_multi_task_training_history(history)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_results = evaluate_multi_task_validation(\n",
    "        trained_model, X_val, y_val_task1, y_val_task3\n",
    "    )\n",
    "    \n",
    "    # Print Task 1 (Gendered Abuse) results\n",
    "    print(\"\\nTask 1 (Gendered Abuse) Validation Results:\")\n",
    "    print(f\"Precision: {val_results['task1']['precision']:.4f}\")\n",
    "    print(f\"Recall: {val_results['task1']['recall']:.4f}\")\n",
    "    print(f\"weighted F1 Score: {val_results['task1']['f1_score_weighted']:.4f}\")\n",
    "    print(f\"macro F1 Score: {val_results['task1']['f1_score_macro']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(val_results['task1']['classification_report'])\n",
    "    \n",
    "    # Print Task 3 (Explicit Language) results\n",
    "    print(\"\\nTask 3 (Explicit Language) Validation Results:\")\n",
    "    print(f\"Precision: {val_results['task3']['precision']:.4f}\")\n",
    "    print(f\"Recall: {val_results['task3']['recall']:.4f}\")\n",
    "    print(f\"weighted F1 Score: {val_results['task3']['f1_score_weighted']:.4f}\")\n",
    "    print(f\"macro F1 Score: {val_results['task3']['f1_score_macro']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(val_results['task3']['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "958d634a-2ae7-4c9e-87a6-d54fc9c76c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged dataset shape: (1135, 6)\n",
      "  unique_id                                               text binary_label_1  \\\n",
      "0      id_0     à®µà¯ˆà®°à®®à¯à®¤à¯à®¤à¯ à®’à®°à¯ à®•à®¾à®® à®®à®¿à®°à¯à®•à®®à¯ à®à®©à¯à®ªà®¤à¯ à®šà®¿à®©à®¿à®®à®¾ à®¤à¯à®±...       not_hate   \n",
      "1      id_1  #4YrsOfValiantVIVEGAM  #Valimai #AjithKumar   ...       not_hate   \n",
      "2      id_2  #AmbedkarBlueShirtRally  à®‡à®¨à¯à®¤ à®ªà¯‹à®°à®¾à®Ÿà¯à®Ÿà®¤à¯à®¤à¯à®•à¯à®•à¯ ...       not_hate   \n",
      "3      id_3  #BREAKING | à®¤à®¿à®°à¯à®šà¯à®šà®¿ à®®à®¾à®µà®Ÿà¯à®Ÿà®®à¯  à®®à®£à®ªà¯à®ªà®¾à®±à¯ˆà®¯à¯ˆ à®…à®Ÿà¯à®¤...       not_hate   \n",
      "4      id_4  #Bachelor ğŸ˜¤ğŸ˜¤ğŸ˜¤ğŸ˜¤ğŸ˜¤à®ªà®Ÿà®®à®¾à®Ÿà®¾ à®‡à®¤à¯ à®•à¯‹à®¤à¯à®¤à®¾ <handle repla...       not_hate   \n",
      "\n",
      "   label_1 binary_label_3  label_3  \n",
      "0        0   not_explicit        0  \n",
      "1        0       explicit        1  \n",
      "2        0       explicit        1  \n",
      "3        0   not_explicit        0  \n",
      "4        0       explicit        1  \n",
      "\n",
      "Label distribution:\n",
      "Label 1 (gendered abuse): {'not_hate': 596, 'hate': 539}\n",
      "Label 3 (explicit/aggressive): {'explicit': 765, 'not_explicit': 370}\n"
     ]
    }
   ],
   "source": [
    "# Process label 1 dataset\n",
    "df_l1 = pd.read_csv('test_ta_l1.csv')\n",
    "df_l1 = df_l1.rename(columns={'key': 'unique_id', 'sentence': 'text'})\n",
    "\n",
    "# Convert annotator columns to numeric without replacing NaNs\n",
    "df_l1[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5', 'ta_a6']] = df_l1[\n",
    "    ['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5', 'ta_a6']\n",
    "].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Compute 'label_1' based on majority voting while ignoring NaNs\n",
    "df_l1['label_1'] = (df_l1[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5', 'ta_a6']].mean(axis=1, skipna=True) >= 0.5).astype(int)\n",
    "\n",
    "# Create proper binary label for task 1 (gendered abuse)\n",
    "df_l1['binary_label_1'] = df_l1['label_1'].apply(lambda x: 'hate' if x == 1 else 'not_hate')\n",
    "\n",
    "# Process label 3 dataset\n",
    "df_l3 = pd.read_csv('test_ta_l3.csv')\n",
    "df_l3 = df_l3.rename(columns={'key': 'unique_id', 'sentence': 'text'})\n",
    "\n",
    "# Convert annotator columns to numeric without replacing NaNs\n",
    "df_l3[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5', 'ta_a6']] = df_l3[\n",
    "    ['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5', 'ta_a6']\n",
    "].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Compute 'label_3' based on majority voting while ignoring NaNs\n",
    "df_l3['label_3'] = (df_l3[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5', 'ta_a6']].mean(axis=1, skipna=True) >= 0.5).astype(int)\n",
    "\n",
    "# Create proper binary label for task 3 (explicit/aggressive)\n",
    "df_l3['binary_label_3'] = df_l3['label_3'].apply(lambda x: 'explicit' if x == 1 else 'not_explicit')\n",
    "\n",
    "# Select columns for merging\n",
    "df_l1_slim = df_l1[['text', 'label_1', 'binary_label_1']]\n",
    "df_l3_slim = df_l3[['text', 'label_3', 'binary_label_3']]\n",
    "\n",
    "# Merge the datasets based on text field\n",
    "merged_df = pd.merge(df_l1_slim, df_l3_slim, on='text', how='inner')\n",
    "\n",
    "# Add a unique_id column to the merged dataset\n",
    "merged_df['unique_id'] = [f'id_{i}' for i in range(len(merged_df))]\n",
    "\n",
    "# Reorder columns\n",
    "merged_df = merged_df[['unique_id', 'text', 'binary_label_1', 'label_1', 'binary_label_3', 'label_3']]\n",
    "\n",
    "# Save the merged dataset\n",
    "merged_df.to_csv('test_ta_task3.csv', index=False)\n",
    "\n",
    "# Display information\n",
    "print(f\"Final merged dataset shape: {merged_df.shape}\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# Check label distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(f\"Label 1 (gendered abuse): {merged_df['binary_label_1'].value_counts().to_dict()}\")\n",
    "print(f\"Label 3 (explicit/aggressive): {merged_df['binary_label_3'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df78ea4e-d4ba-4ff0-818a-00fab5167bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original vs Processed Text Samples:\n",
      "Original:    à®µà¯ˆà®°à®®à¯à®¤à¯à®¤à¯ à®’à®°à¯ à®•à®¾à®® à®®à®¿à®°à¯à®•à®®à¯ à®à®©à¯à®ªà®¤à¯ à®šà®¿à®©à®¿à®®à®¾ à®¤à¯à®±à¯ˆà®¯à®¿à®²à¯ à®‡à®°à¯à®•à¯à®•à¯à®®à¯ à®…à®©à¯ˆà®µà®°à¯à®•à¯à®•à¯à®®à¯ à®¤à¯†à®°à®¿à®¯à¯à®®à¯.  à®šà®®à¯à®¤à®¾à®¯à®¤à¯à®¤à¯à®•à¯à®•à¯ à®à®¤à¯à®¤à®©à¯ˆ à®ªà¯‡à®°à®µà®²à®®à¯ !  #SexualPredatorVairamuthu #à®ªà¯Šà®®à¯à®ªà®³_à®ªà¯Šà®°à¯à®•à¯à®•à®¿_à®µà¯ˆà®°à®®à¯à®¤à¯à®¤à¯ #SexualPredatorVairamuthu #à®ªà¯Šà®®à¯à®ªà®³_à®ªà¯Šà®°à¯à®•à¯à®•à®¿_à®µà¯ˆà®°à®®à¯à®¤à¯à®¤à¯ #SexualPredatorVairamuthu\n",
      "Processed:    à®µà¯ˆà®°à®®à¯à®¤à¯à®¤à¯ à®’à®°à¯ à®•à®¾à®® à®®à®¿à®°à¯à®•à®®à¯ à®à®©à¯à®ªà®¤à¯ à®šà®¿à®©à®¿à®®à®¾ à®¤à¯à®±à¯ˆà®¯à®¿à®²à¯ à®‡à®°à¯à®•à¯à®•à¯à®®à¯ à®…à®©à¯ˆà®µà®°à¯à®•à¯à®•à¯à®®à¯ à®¤à¯†à®°à®¿à®¯à¯à®®à¯   à®šà®®à¯à®¤à®¾à®¯à®¤à¯à®¤à¯à®•à¯à®•à¯ à®à®¤à¯à®¤à®©à¯ˆ à®ªà¯‡à®°à®µà®²à®®à¯     sexualpredatorvairamuthu  à®ªà¯Šà®®à¯à®ªà®³ à®ªà¯Šà®°à¯à®•à¯à®•à®¿ à®µà¯ˆà®°à®®à¯à®¤à¯à®¤à¯  sexualpredatorvairamuthu  à®ªà¯Šà®®à¯à®ªà®³ à®ªà¯Šà®°à¯à®•à¯à®•à®¿ à®µà¯ˆà®°à®®à¯à®¤à¯à®¤à¯  sexualpredatorvairamuthu\n",
      "--------------------------------------------------\n",
      "Original: #4YrsOfValiantVIVEGAM  #Valimai #AjithKumar   à®à®£à¯à®Ÿà®¾ à®•à¯à®³à¯à®³ à®ªà¯à®£à¯à®Ÿà¯ˆà®•à®²à®¾,à®¨à¯€à®™à¯à®•à®²à®¾à®®à¯ à®…à®œà¯€à®¤à¯,à®µà®¿à®œà®¯à¯ à®ªà¯‡à®©à¯à®¸à¯à®•à¯à®•à¯ à®ªà¯‹à®Ÿà¯à®Ÿà®¿à®¯à®¾,à®šà¯‚à®¤à¯à®¤à¯ à®…à®Ÿà®¿à®šà¯à®šà®¿ à®µà®¿à®Ÿà¯à®°à¯à®µà¯‹à®®à¯,à®ªà¯‹à®¯à®¿ à®¤à®©à¯à®¸à¯,à®šà®¿à®µà®¾ à®ªà¯‡à®©à¯à®¸à¯ à®•à¯‚à®Ÿ à®šà®£à¯à®Ÿ à®ªà¯‹à®Ÿà¯à®™à¯à®•à®Ÿà®¾ <handle replaced>ğŸ’¦à®à®šà¯à®š à®ªà¯‡à®©à¯à®¸à¯\n",
      "Processed:      valimai  ajithkumar   à®à®£à¯à®Ÿà®¾ à®•à¯à®³à¯à®³ à®ªà¯à®£à¯à®Ÿà¯ˆà®•à®²à®¾ à®¨à¯€à®™à¯à®•à®²à®¾à®®à¯ à®…à®œà¯€à®¤à¯ à®µà®¿à®œà®¯à¯ à®ªà¯‡à®©à¯à®¸à¯à®•à¯à®•à¯ à®ªà¯‹à®Ÿà¯à®Ÿà®¿à®¯à®¾ à®šà¯‚à®¤à¯à®¤à¯ à®…à®Ÿà®¿à®šà¯à®šà®¿ à®µà®¿à®Ÿà¯à®°à¯à®µà¯‹à®®à¯ à®ªà¯‹à®¯à®¿ à®¤à®©à¯à®¸à¯ à®šà®¿à®µà®¾ à®ªà¯‡à®©à¯à®¸à¯ à®•à¯‚à®Ÿ à®šà®£à¯à®Ÿ à®ªà¯‹à®Ÿà¯à®™à¯à®•à®Ÿà®¾  à®à®šà¯à®š à®ªà¯‡à®©à¯à®¸à¯\n",
      "--------------------------------------------------\n",
      "Original: #AmbedkarBlueShirtRally  à®‡à®¨à¯à®¤ à®ªà¯‹à®°à®¾à®Ÿà¯à®Ÿà®¤à¯à®¤à¯à®•à¯à®•à¯ à®µà®¨à¯à®¤ à®•à¯‚à®Ÿà¯à®Ÿà®®à¯ #à®¤à®°à¯à®ªà®¾à®°à¯ à®ªà®Ÿà®¤à¯à®¤à¯à®•à¯à®•à¯ à®µà®¨à¯à®¤ à®•à¯‚à®Ÿà¯à®Ÿà®¤à¯à®¤à¯Šà®Ÿà¯ à®…à®¤à®¿à®•à®®à¯ à®‡à®²à¯à®²à¯ˆ. à®‡à®¤à®¿à®²à¯ à®¯à®¾à®°à¯ à®¯à®¾à®°à¯ˆ à®µà®¿à®®à®°à¯à®šà®¿à®ªà¯à®ªà®¤à¯ ?   #à®ªà¯†à®°à®¿à®¯à®¾à®°à®¾à®µà®¤à¯_à®®à®¯à®¿à®°à®¾à®µà®¤à¯\n",
      "Processed:  ambedkarblueshirtrally  à®‡à®¨à¯à®¤ à®ªà¯‹à®°à®¾à®Ÿà¯à®Ÿà®¤à¯à®¤à¯à®•à¯à®•à¯ à®µà®¨à¯à®¤ à®•à¯‚à®Ÿà¯à®Ÿà®®à¯  à®¤à®°à¯à®ªà®¾à®°à¯ à®ªà®Ÿà®¤à¯à®¤à¯à®•à¯à®•à¯ à®µà®¨à¯à®¤ à®•à¯‚à®Ÿà¯à®Ÿà®¤à¯à®¤à¯Šà®Ÿà¯ à®…à®¤à®¿à®•à®®à¯ à®‡à®²à¯à®²à¯ˆ  à®‡à®¤à®¿à®²à¯ à®¯à®¾à®°à¯ à®¯à®¾à®°à¯ˆ à®µà®¿à®®à®°à¯à®šà®¿à®ªà¯à®ªà®¤à¯      à®ªà¯†à®°à®¿à®¯à®¾à®°à®¾à®µà®¤à¯ à®®à®¯à®¿à®°à®¾à®µà®¤à¯\n",
      "--------------------------------------------------\n",
      "\n",
      "Processed dataset shape: (1135, 7)\n",
      "  unique_id                                               text  \\\n",
      "0      id_0     à®µà¯ˆà®°à®®à¯à®¤à¯à®¤à¯ à®’à®°à¯ à®•à®¾à®® à®®à®¿à®°à¯à®•à®®à¯ à®à®©à¯à®ªà®¤à¯ à®šà®¿à®©à®¿à®®à®¾ à®¤à¯à®±...   \n",
      "1      id_1  #4YrsOfValiantVIVEGAM  #Valimai #AjithKumar   ...   \n",
      "2      id_2  #AmbedkarBlueShirtRally  à®‡à®¨à¯à®¤ à®ªà¯‹à®°à®¾à®Ÿà¯à®Ÿà®¤à¯à®¤à¯à®•à¯à®•à¯ ...   \n",
      "3      id_3  #BREAKING | à®¤à®¿à®°à¯à®šà¯à®šà®¿ à®®à®¾à®µà®Ÿà¯à®Ÿà®®à¯  à®®à®£à®ªà¯à®ªà®¾à®±à¯ˆà®¯à¯ˆ à®…à®Ÿà¯à®¤...   \n",
      "4      id_4  #Bachelor ğŸ˜¤ğŸ˜¤ğŸ˜¤ğŸ˜¤ğŸ˜¤à®ªà®Ÿà®®à®¾à®Ÿà®¾ à®‡à®¤à¯ à®•à¯‹à®¤à¯à®¤à®¾ <handle repla...   \n",
      "\n",
      "                                      processed_text binary_label_1  label_1  \\\n",
      "0     à®µà¯ˆà®°à®®à¯à®¤à¯à®¤à¯ à®’à®°à¯ à®•à®¾à®® à®®à®¿à®°à¯à®•à®®à¯ à®à®©à¯à®ªà®¤à¯ à®šà®¿à®©à®¿à®®à®¾ à®¤à¯à®±...       not_hate        0   \n",
      "1       valimai  ajithkumar   à®à®£à¯à®Ÿà®¾ à®•à¯à®³à¯à®³ à®ªà¯à®£à¯à®Ÿà¯ˆà®•...       not_hate        0   \n",
      "2   ambedkarblueshirtrally  à®‡à®¨à¯à®¤ à®ªà¯‹à®°à®¾à®Ÿà¯à®Ÿà®¤à¯à®¤à¯à®•à¯à®•à¯ ...       not_hate        0   \n",
      "3   breaking   à®¤à®¿à®°à¯à®šà¯à®šà®¿ à®®à®¾à®µà®Ÿà¯à®Ÿà®®à¯  à®®à®£à®ªà¯à®ªà®¾à®±à¯ˆà®¯à¯ˆ à®…à®Ÿà¯à®¤...       not_hate        0   \n",
      "4                       bachelor à®ªà®Ÿà®®à®¾à®Ÿà®¾ à®‡à®¤à¯ à®•à¯‹à®¤à¯à®¤à®¾         not_hate        0   \n",
      "\n",
      "  binary_label_3  label_3  \n",
      "0   not_explicit        0  \n",
      "1       explicit        1  \n",
      "2       explicit        1  \n",
      "3   not_explicit        0  \n",
      "4       explicit        1  \n"
     ]
    }
   ],
   "source": [
    "# Apply text normalization\n",
    "merged_df['processed_text'] = merged_df['text'].apply(lambda x: normalize_text(x))\n",
    "\n",
    "# Further processing to remove '...'\n",
    "merged_df['processed_text'] = merged_df['processed_text'].str.replace('...', '')\n",
    "\n",
    "# Display samples of processed text\n",
    "print(\"\\nOriginal vs Processed Text Samples:\")\n",
    "for i in range(3):\n",
    "    print(f\"Original: {merged_df['text'].iloc[i]}\")\n",
    "    print(f\"Processed: {merged_df['processed_text'].iloc[i]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Keep all columns but add processed text\n",
    "merged_df_final = merged_df[['unique_id', 'text', 'processed_text', 'binary_label_1', 'label_1', 'binary_label_3', 'label_3']]\n",
    "\n",
    "\n",
    "print(f\"\\nProcessed dataset shape: {merged_df_final.shape}\")\n",
    "print(merged_df_final.head())\n",
    "\n",
    "# Extract features (processed text)\n",
    "X = list(merged_df_final['processed_text'])\n",
    "\n",
    "# Extract labels for both tasks\n",
    "y_task1 = merged_df_final['label_1'].values\n",
    "y_task3 = merged_df_final['label_3'].values\n",
    "tokenizer.fit_on_texts(X)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding\n",
    "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "\n",
    "y_task1 = label_encoder.fit_transform(y_task1)\n",
    "y_task3 = label_encoder.fit_transform(y_task3)\n",
    "\n",
    "y_task1 = to_categorical(y_task1, num_classes=2)\n",
    "y_task3 = to_categorical(y_task3, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e2bda61-99ad-4da5-8a3f-2285ab82ba95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\n",
      "Task 1 (Gendered Abuse) Test Results:\n",
      "Precision: 0.5311\n",
      "Recall: 0.5330\n",
      "weighted F1 Score: 0.4632\n",
      "macro F1 Score: 0.4526\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not_hate       0.53      0.87      0.66       596\n",
      "        hate       0.53      0.16      0.24       539\n",
      "\n",
      "    accuracy                           0.53      1135\n",
      "   macro avg       0.53      0.52      0.45      1135\n",
      "weighted avg       0.53      0.53      0.46      1135\n",
      "\n",
      "\n",
      "Task 3 (Explicit Language) Test Results:\n",
      "Precision: 0.5782\n",
      "Recall: 0.6396\n",
      "weighted F1 Score: 0.5849\n",
      "macro F1 Score: 0.4878\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "not_explicit       0.37      0.15      0.21       370\n",
      "    explicit       0.68      0.88      0.77       765\n",
      "\n",
      "    accuracy                           0.64      1135\n",
      "   macro avg       0.52      0.51      0.49      1135\n",
      "weighted avg       0.58      0.64      0.58      1135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "test_results = evaluate_multi_task_validation(\n",
    "    trained_model, X, y_task1, y_task3\n",
    ")\n",
    "\n",
    "# Print Task 1 (Gendered Abuse) results\n",
    "print(\"\\nTask 1 (Gendered Abuse) Test Results:\")\n",
    "print(f\"Precision: {test_results['task1']['precision']:.4f}\")\n",
    "print(f\"Recall: {test_results['task1']['recall']:.4f}\")\n",
    "print(f\"weighted F1 Score: {test_results['task1']['f1_score_weighted']:.4f}\")\n",
    "print(f\"macro F1 Score: {test_results['task1']['f1_score_macro']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(test_results['task1']['classification_report'])\n",
    "\n",
    "# Print Task 3 (Explicit Language) results\n",
    "print(\"\\nTask 3 (Explicit Language) Test Results:\")\n",
    "print(f\"Precision: {test_results['task3']['precision']:.4f}\")\n",
    "print(f\"Recall: {test_results['task3']['recall']:.4f}\")\n",
    "print(f\"weighted F1 Score: {test_results['task3']['f1_score_weighted']:.4f}\")\n",
    "print(f\"macro F1 Score: {test_results['task3']['f1_score_macro']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(test_results['task3']['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f29e07-2e68-458d-b79b-2cf522606f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
