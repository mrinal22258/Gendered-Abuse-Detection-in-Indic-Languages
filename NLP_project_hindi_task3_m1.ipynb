{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c33c8c96-fbe5-4565-add1-e3532c30ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from skmultilearn.adapt import MLkNN\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import hamming_loss, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "845244c3-567f-437c-b736-0a320fe28cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged dataset shape: (6196, 6)\n",
      "  unique_id                                               text binary_label_1  \\\n",
      "0      id_0    भारत में तीन किसान कानून बिल वापस लेने पर भक...       not_hate   \n",
      "1      id_1    राजस्थान  अजीबोगरीब: गांव की खुशहाली के लिए ...       not_hate   \n",
      "2      id_2    सलमान ने की राखी के विरोधी की वकालत तो 'आइटम...       not_hate   \n",
      "3      id_3  !!हर शब्द अमॄतम!!  पुरानी एक कहावत है... टूटी ...       not_hate   \n",
      "4      id_4  \"PM मोदी जी \" की माँ के लिए अपशब्द कहने वाला द...           hate   \n",
      "\n",
      "   label_1 binary_label_3  label_3  \n",
      "0        0   not_explicit        0  \n",
      "1        0   not_explicit        0  \n",
      "2        0   not_explicit        0  \n",
      "3        0   not_explicit        0  \n",
      "4        1       explicit        1  \n",
      "\n",
      "Label distribution:\n",
      "Label 1 (gendered abuse): {'not_hate': 4426, 'hate': 1770}\n",
      "Label 3 (explicit/aggressive): {'not_explicit': 3320, 'explicit': 2876}\n"
     ]
    }
   ],
   "source": [
    "# Process label 1 dataset for Hindi\n",
    "df_l1 = pd.read_csv('train_hi_l1.csv')\n",
    "df_l1 = df_l1.rename(columns={'key': 'unique_id', 'sentence': 'text'})\n",
    "\n",
    "# Convert annotator columns to numeric without replacing NaNs (only 5 annotators)\n",
    "df_l1[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']] = df_l1[\n",
    "    ['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']\n",
    "].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Compute 'label_1' based on majority voting while ignoring NaNs\n",
    "df_l1['label_1'] = (df_l1[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']].mean(axis=1, skipna=True) >= 0.5).astype(int)\n",
    "\n",
    "# Create proper binary label for task 1 (gendered abuse)\n",
    "df_l1['binary_label_1'] = df_l1['label_1'].apply(lambda x: 'hate' if x == 1 else 'not_hate')\n",
    "\n",
    "# Process label 3 dataset for Hindi\n",
    "df_l3 = pd.read_csv('train_hi_l3.csv')\n",
    "df_l3 = df_l3.rename(columns={'key': 'unique_id', 'sentence': 'text'})\n",
    "\n",
    "# Convert annotator columns to numeric without replacing NaNs (only 5 annotators)\n",
    "df_l3[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']] = df_l3[\n",
    "    ['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']\n",
    "].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Compute 'label_3' based on majority voting while ignoring NaNs\n",
    "df_l3['label_3'] = (df_l3[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']].mean(axis=1, skipna=True) >= 0.5).astype(int)\n",
    "\n",
    "# Create proper binary label for task 3 (explicit/aggressive)\n",
    "df_l3['binary_label_3'] = df_l3['label_3'].apply(lambda x: 'explicit' if x == 1 else 'not_explicit')\n",
    "\n",
    "# Select columns for merging\n",
    "df_l1_slim = df_l1[['text', 'label_1', 'binary_label_1']]\n",
    "df_l3_slim = df_l3[['text', 'label_3', 'binary_label_3']]\n",
    "\n",
    "# Merge the datasets based on text field\n",
    "merged_df = pd.merge(df_l1_slim, df_l3_slim, on='text', how='inner')\n",
    "\n",
    "# Add a unique_id column to the merged dataset\n",
    "merged_df['unique_id'] = [f'id_{i}' for i in range(len(merged_df))]\n",
    "\n",
    "# Reorder columns\n",
    "merged_df = merged_df[['unique_id', 'text', 'binary_label_1', 'label_1', 'binary_label_3', 'label_3']]\n",
    "\n",
    "# Save the merged dataset\n",
    "merged_df.to_csv('train_hi_task3.csv', index=False)\n",
    "\n",
    "# Display information\n",
    "print(f\"Final merged dataset shape: {merged_df.shape}\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# Check label distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(f\"Label 1 (gendered abuse): {merged_df['binary_label_1'].value_counts().to_dict()}\")\n",
    "print(f\"Label 3 (explicit/aggressive): {merged_df['binary_label_3'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cc23090-2305-4cf7-aded-a545b394dab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original vs Processed Text Samples:\n",
      "Original:   भारत में तीन किसान कानून बिल वापस लेने पर भक्तों की जो हालत हुई अब यह किसी से छुपा नहीं है उसके इलावा जो टट्टी खोर पत्रकार थे उनकी भी हालत खराब हुई है किसी को मुंह दिखाने के लायक नहीं है अब यह लोग यहां तक कि अपने घर में भी मुंह दिखाने के लायक नहीं रहे।\n",
      "Processed:   भारत में तीन किसान कानून बिल वापस लेने पर भक्तों की जो हालत हुई अब यह किसी से छुपा नहीं है उसके इलावा जो टट्टी खोर पत्रकार थे उनकी भी हालत खराब हुई है किसी को मुंह दिखाने के लायक नहीं है अब यह लोग यहां तक कि अपने घर में भी मुंह दिखाने के लायक नहीं रहे।\n",
      "--------------------------------------------------\n",
      "Original:   राजस्थान  अजीबोगरीब: गांव की खुशहाली के लिए रात भर किन्नर करते है यह काम...\n",
      "Processed:   राजस्थान  अजीबोगरीब  गांव की खुशहाली के लिए रात भर किन्नर करते है यह काम   \n",
      "--------------------------------------------------\n",
      "Original:   सलमान ने की राखी के विरोधी की वकालत तो 'आइटम गर्ल' ने दी धमकी! #RakhiSawant #SalmanKhan\n",
      "Processed:   सलमान ने की राखी के विरोधी की वकालत तो  आइटम गर्ल  ने दी धमकी   rakhisawant  salmankhan\n",
      "--------------------------------------------------\n",
      "\n",
      "Processed dataset shape: (6196, 7)\n",
      "  unique_id                                               text  \\\n",
      "0      id_0    भारत में तीन किसान कानून बिल वापस लेने पर भक...   \n",
      "1      id_1    राजस्थान  अजीबोगरीब: गांव की खुशहाली के लिए ...   \n",
      "2      id_2    सलमान ने की राखी के विरोधी की वकालत तो 'आइटम...   \n",
      "3      id_3  !!हर शब्द अमॄतम!!  पुरानी एक कहावत है... टूटी ...   \n",
      "4      id_4  \"PM मोदी जी \" की माँ के लिए अपशब्द कहने वाला द...   \n",
      "\n",
      "                                      processed_text binary_label_1  label_1  \\\n",
      "0    भारत में तीन किसान कानून बिल वापस लेने पर भक...       not_hate        0   \n",
      "1    राजस्थान  अजीबोगरीब  गांव की खुशहाली के लिए ...       not_hate        0   \n",
      "2    सलमान ने की राखी के विरोधी की वकालत तो  आइटम...       not_hate        0   \n",
      "3    हर शब्द अमॄतम    पुरानी एक कहावत है    टूटी ...       not_hate        0   \n",
      "4   pm मोदी जी   की माँ के लिए अपशब्द कहने वाला द...           hate        1   \n",
      "\n",
      "  binary_label_3  label_3  \n",
      "0   not_explicit        0  \n",
      "1   not_explicit        0  \n",
      "2   not_explicit        0  \n",
      "3   not_explicit        0  \n",
      "4       explicit        1  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"\n",
    "                               u\"\\U0001F700-\\U0001F77F\"\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', ' ', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'<.*?>+', ' ', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', ' ', text)\n",
    "    text = re.sub(r'<handle replaced>', '', text)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "# Load the multi-task dataset\n",
    "merged_df = pd.read_csv('train_hi_task3.csv')\n",
    "\n",
    "# Apply text normalization\n",
    "merged_df['processed_text'] = merged_df['text'].apply(lambda x: normalize_text(x))\n",
    "\n",
    "# Further processing to remove '...'\n",
    "merged_df['processed_text'] = merged_df['processed_text'].str.replace('...', '')\n",
    "\n",
    "# Display samples of processed text\n",
    "print(\"\\nOriginal vs Processed Text Samples:\")\n",
    "for i in range(3):\n",
    "    print(f\"Original: {merged_df['text'].iloc[i]}\")\n",
    "    print(f\"Processed: {merged_df['processed_text'].iloc[i]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Keep all columns but add processed text\n",
    "merged_df_final = merged_df[['unique_id', 'text', 'processed_text', 'binary_label_1', 'label_1', 'binary_label_3', 'label_3']]\n",
    "\n",
    "\n",
    "print(f\"\\nProcessed dataset shape: {merged_df_final.shape}\")\n",
    "print(merged_df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0947217-9121-4c8b-81e6-433cb869aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (processed text)\n",
    "X = list(merged_df_final['processed_text'])\n",
    "\n",
    "# Extract labels for both tasks\n",
    "y_task1 = merged_df_final['label_1'].values\n",
    "y_task3 = merged_df_final['label_3'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e7ee2a-5980-4b56-9c61-2ed65c52e252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  63    5  896 ...    0    0    0]\n",
      " [ 629 1047    3 ...    0    0    0]\n",
      " [1783   21    3 ...    0    0    0]\n",
      " ...\n",
      " [   5  357 3995 ...    0    0    0]\n",
      " [   6  201 1216 ...    0    0    0]\n",
      " [ 205  205  688 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Activation, Dropout, Dense, Flatten,\n",
    "    Bidirectional, GRU, concatenate, SpatialDropout1D,\n",
    "    GlobalMaxPooling1D, GlobalAveragePooling1D, Conv1D,\n",
    "    Embedding, Input, Concatenate\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "######## Textual Features for Embedding ###################\n",
    "max_len = 100\n",
    "max_features = 4479\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding\n",
    "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "\n",
    "print(X)  # Check the processed sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62371454-8ffd-46f4-b990-f7dc52bc2539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_task1 = label_encoder.fit_transform(y_task1)\n",
    "y_task3 = label_encoder.fit_transform(y_task3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd970c0-4d50-4493-a752-9f80fa939166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_task1 = to_categorical(y_task1, num_classes=2)\n",
    "y_task3 = to_categorical(y_task3, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a885a1f6-39f6-41d9-8d19-eb3433dc0710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (4479, 50)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load GloVe embeddings from JSON\n",
    "with open('glove_embeddings.json', encoding=\"utf8\") as f:\n",
    "    embeddings_list = json.load(f)\n",
    "\n",
    "# Convert the list of vectors to a dictionary with word indices as keys\n",
    "embeddings_dictionary = {str(i): vector for i, vector in enumerate(embeddings_list)}\n",
    "\n",
    "# Define tokenizer \n",
    "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_features, vocab_size)  # Limit vocab to max_features\n",
    "\n",
    "# Get embedding dimension (from first vector in list)\n",
    "embed_size = len(embeddings_list[0]) if embeddings_list else 0\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "\n",
    "# Fill embedding matrix with corresponding word vectors\n",
    "for word, index in word_index.items():\n",
    "    if index >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_dictionary.get(word) or embeddings_dictionary.get(str(index))\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = np.asarray(embedding_vector, dtype=np.float32)\n",
    "\n",
    "print(\"Embedding matrix shape:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15197b05-b6b4-4d26-bfb2-d202ae8bca31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4956\n",
      "Validation samples: 1240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krmri\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">223,950</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ spatial_dropout1d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)            │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,464</span> │ spatial_dropout1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">197,632</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_average_pooling1d      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task1_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ global_average_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task3_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ global_average_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ task1_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ task3_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task1_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task3_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m50\u001b[0m)           │         \u001b[38;5;34m223,950\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ spatial_dropout1d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m50\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)            │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │           \u001b[38;5;34m6,464\u001b[0m │ spatial_dropout1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)          │         \u001b[38;5;34m197,632\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_average_pooling1d      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task1_dense (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │          \u001b[38;5;34m32,896\u001b[0m │ global_average_pooling1d[\u001b[38;5;34m…\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task3_dense (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │          \u001b[38;5;34m32,896\u001b[0m │ global_average_pooling1d[\u001b[38;5;34m…\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ task1_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ task3_dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task1_output (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                 │             \u001b[38;5;34m258\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task3_output (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                 │             \u001b[38;5;34m258\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">494,354</span> (1.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m494,354\u001b[0m (1.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">270,404</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m270,404\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">223,950</span> (874.80 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m223,950\u001b[0m (874.80 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1.3082 - task1_output_accuracy: 0.7122 - task1_output_loss: 0.6184 - task1_output_macro_f1_score: 0.7122 - task3_output_accuracy: 0.5306 - task3_output_loss: 0.6899 - task3_output_macro_f1_score: 0.5306  \n",
      "Epoch 1: val_loss improved from inf to 1.26564, saving model to models_hi_task3_m1\\best_model_hi_task3_m1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 109ms/step - loss: 1.3081 - task1_output_accuracy: 0.7122 - task1_output_loss: 0.6183 - task1_output_macro_f1_score: 0.7122 - task3_output_accuracy: 0.5307 - task3_output_loss: 0.6898 - task3_output_macro_f1_score: 0.5307 - val_loss: 1.2656 - val_task1_output_accuracy: 0.7145 - val_task1_output_loss: 0.5946 - val_task1_output_macro_f1_score: 0.7145 - val_task3_output_accuracy: 0.5734 - val_task3_output_loss: 0.6710 - val_task3_output_macro_f1_score: 0.5734\n",
      "Epoch 2/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 1.2729 - task1_output_accuracy: 0.7090 - task1_output_loss: 0.6008 - task1_output_macro_f1_score: 0.7090 - task3_output_accuracy: 0.5834 - task3_output_loss: 0.6721 - task3_output_macro_f1_score: 0.5834 \n",
      "Epoch 2: val_loss improved from 1.26564 to 1.26273, saving model to models_hi_task3_m1\\best_model_hi_task3_m1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 102ms/step - loss: 1.2729 - task1_output_accuracy: 0.7090 - task1_output_loss: 0.6008 - task1_output_macro_f1_score: 0.7090 - task3_output_accuracy: 0.5833 - task3_output_loss: 0.6721 - task3_output_macro_f1_score: 0.5833 - val_loss: 1.2627 - val_task1_output_accuracy: 0.7145 - val_task1_output_loss: 0.5876 - val_task1_output_macro_f1_score: 0.7145 - val_task3_output_accuracy: 0.5685 - val_task3_output_loss: 0.6751 - val_task3_output_macro_f1_score: 0.5685\n",
      "Epoch 3/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 1.2590 - task1_output_accuracy: 0.7172 - task1_output_loss: 0.5869 - task1_output_macro_f1_score: 0.7172 - task3_output_accuracy: 0.5828 - task3_output_loss: 0.6721 - task3_output_macro_f1_score: 0.5828  \n",
      "Epoch 3: val_loss did not improve from 1.26273\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 102ms/step - loss: 1.2590 - task1_output_accuracy: 0.7172 - task1_output_loss: 0.5869 - task1_output_macro_f1_score: 0.7172 - task3_output_accuracy: 0.5827 - task3_output_loss: 0.6721 - task3_output_macro_f1_score: 0.5827 - val_loss: 1.2747 - val_task1_output_accuracy: 0.7145 - val_task1_output_loss: 0.5957 - val_task1_output_macro_f1_score: 0.7145 - val_task3_output_accuracy: 0.5516 - val_task3_output_loss: 0.6788 - val_task3_output_macro_f1_score: 0.5516\n",
      "Epoch 4/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 1.2557 - task1_output_accuracy: 0.7178 - task1_output_loss: 0.5883 - task1_output_macro_f1_score: 0.7178 - task3_output_accuracy: 0.5975 - task3_output_loss: 0.6674 - task3_output_macro_f1_score: 0.5975 \n",
      "Epoch 4: val_loss improved from 1.26273 to 1.26015, saving model to models_hi_task3_m1\\best_model_hi_task3_m1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 102ms/step - loss: 1.2557 - task1_output_accuracy: 0.7178 - task1_output_loss: 0.5883 - task1_output_macro_f1_score: 0.7178 - task3_output_accuracy: 0.5974 - task3_output_loss: 0.6674 - task3_output_macro_f1_score: 0.5974 - val_loss: 1.2602 - val_task1_output_accuracy: 0.7145 - val_task1_output_loss: 0.5926 - val_task1_output_macro_f1_score: 0.7145 - val_task3_output_accuracy: 0.5960 - val_task3_output_loss: 0.6681 - val_task3_output_macro_f1_score: 0.5960\n",
      "Epoch 5/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1.2605 - task1_output_accuracy: 0.7159 - task1_output_loss: 0.5914 - task1_output_macro_f1_score: 0.7159 - task3_output_accuracy: 0.5913 - task3_output_loss: 0.6690 - task3_output_macro_f1_score: 0.5913 \n",
      "Epoch 5: val_loss improved from 1.26015 to 1.25378, saving model to models_hi_task3_m1\\best_model_hi_task3_m1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 100ms/step - loss: 1.2604 - task1_output_accuracy: 0.7159 - task1_output_loss: 0.5914 - task1_output_macro_f1_score: 0.7159 - task3_output_accuracy: 0.5913 - task3_output_loss: 0.6690 - task3_output_macro_f1_score: 0.5913 - val_loss: 1.2538 - val_task1_output_accuracy: 0.7145 - val_task1_output_loss: 0.5870 - val_task1_output_macro_f1_score: 0.7145 - val_task3_output_accuracy: 0.6016 - val_task3_output_loss: 0.6668 - val_task3_output_macro_f1_score: 0.6016\n",
      "Epoch 6/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1.2586 - task1_output_accuracy: 0.7053 - task1_output_loss: 0.5927 - task1_output_macro_f1_score: 0.7053 - task3_output_accuracy: 0.5930 - task3_output_loss: 0.6659 - task3_output_macro_f1_score: 0.5930 \n",
      "Epoch 6: val_loss did not improve from 1.25378\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 101ms/step - loss: 1.2586 - task1_output_accuracy: 0.7054 - task1_output_loss: 0.5927 - task1_output_macro_f1_score: 0.7054 - task3_output_accuracy: 0.5930 - task3_output_loss: 0.6659 - task3_output_macro_f1_score: 0.5930 - val_loss: 1.2597 - val_task1_output_accuracy: 0.7145 - val_task1_output_loss: 0.5907 - val_task1_output_macro_f1_score: 0.7145 - val_task3_output_accuracy: 0.5871 - val_task3_output_loss: 0.6689 - val_task3_output_macro_f1_score: 0.5871\n",
      "Epoch 7/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 1.2326 - task1_output_accuracy: 0.7134 - task1_output_loss: 0.5763 - task1_output_macro_f1_score: 0.7134 - task3_output_accuracy: 0.6035 - task3_output_loss: 0.6563 - task3_output_macro_f1_score: 0.6035  \n",
      "Epoch 7: val_loss improved from 1.25378 to 1.24843, saving model to models_hi_task3_m1\\best_model_hi_task3_m1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 105ms/step - loss: 1.2327 - task1_output_accuracy: 0.7134 - task1_output_loss: 0.5764 - task1_output_macro_f1_score: 0.7134 - task3_output_accuracy: 0.6035 - task3_output_loss: 0.6564 - task3_output_macro_f1_score: 0.6035 - val_loss: 1.2484 - val_task1_output_accuracy: 0.7145 - val_task1_output_loss: 0.5850 - val_task1_output_macro_f1_score: 0.7145 - val_task3_output_accuracy: 0.6040 - val_task3_output_loss: 0.6634 - val_task3_output_macro_f1_score: 0.6040\n",
      "Epoch 8/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 1.2398 - task1_output_accuracy: 0.7103 - task1_output_loss: 0.5845 - task1_output_macro_f1_score: 0.7103 - task3_output_accuracy: 0.6238 - task3_output_loss: 0.6554 - task3_output_macro_f1_score: 0.6238  \n",
      "Epoch 8: val_loss did not improve from 1.24843\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 104ms/step - loss: 1.2398 - task1_output_accuracy: 0.7103 - task1_output_loss: 0.5845 - task1_output_macro_f1_score: 0.7103 - task3_output_accuracy: 0.6237 - task3_output_loss: 0.6554 - task3_output_macro_f1_score: 0.6237 - val_loss: 1.2534 - val_task1_output_accuracy: 0.7145 - val_task1_output_loss: 0.5869 - val_task1_output_macro_f1_score: 0.7145 - val_task3_output_accuracy: 0.5863 - val_task3_output_loss: 0.6663 - val_task3_output_macro_f1_score: 0.5863\n",
      "Epoch 9/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 1.2438 - task1_output_accuracy: 0.7101 - task1_output_loss: 0.5855 - task1_output_macro_f1_score: 0.7101 - task3_output_accuracy: 0.6131 - task3_output_loss: 0.6583 - task3_output_macro_f1_score: 0.6131\n",
      "Epoch 9: val_loss did not improve from 1.24843\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 108ms/step - loss: 1.2438 - task1_output_accuracy: 0.7101 - task1_output_loss: 0.5855 - task1_output_macro_f1_score: 0.7101 - task3_output_accuracy: 0.6131 - task3_output_loss: 0.6583 - task3_output_macro_f1_score: 0.6131 - val_loss: 1.2566 - val_task1_output_accuracy: 0.7145 - val_task1_output_loss: 0.5888 - val_task1_output_macro_f1_score: 0.7145 - val_task3_output_accuracy: 0.5952 - val_task3_output_loss: 0.6679 - val_task3_output_macro_f1_score: 0.5952\n",
      "Epoch 10/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 1.2335 - task1_output_accuracy: 0.7195 - task1_output_loss: 0.5807 - task1_output_macro_f1_score: 0.7195 - task3_output_accuracy: 0.6164 - task3_output_loss: 0.6528 - task3_output_macro_f1_score: 0.6164 \n",
      "Epoch 10: val_loss did not improve from 1.24843\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 112ms/step - loss: 1.2335 - task1_output_accuracy: 0.7195 - task1_output_loss: 0.5807 - task1_output_macro_f1_score: 0.7195 - task3_output_accuracy: 0.6164 - task3_output_loss: 0.6528 - task3_output_macro_f1_score: 0.6164 - val_loss: 1.2545 - val_task1_output_accuracy: 0.7145 - val_task1_output_loss: 0.5886 - val_task1_output_macro_f1_score: 0.7145 - val_task3_output_accuracy: 0.6081 - val_task3_output_loss: 0.6659 - val_task3_output_macro_f1_score: 0.6081\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krmri\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krmri\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krmri\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krmri\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task 1 (Gendered Abuse) Validation Results:\n",
      "Precision: 0.5105\n",
      "Recall: 0.7145\n",
      "weighted F1 Score: 0.5955\n",
      "macro F1 Score: 0.4167\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not_hate       0.71      1.00      0.83       886\n",
      "        hate       0.00      0.00      0.00       354\n",
      "\n",
      "    accuracy                           0.71      1240\n",
      "   macro avg       0.36      0.50      0.42      1240\n",
      "weighted avg       0.51      0.71      0.60      1240\n",
      "\n",
      "\n",
      "Task 3 (Explicit Language) Validation Results:\n",
      "Precision: 0.6092\n",
      "Recall: 0.6040\n",
      "weighted F1 Score: 0.6035\n",
      "macro F1 Score: 0.6039\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "not_explicit       0.64      0.56      0.60       652\n",
      "    explicit       0.57      0.66      0.61       588\n",
      "\n",
      "    accuracy                           0.60      1240\n",
      "   macro avg       0.61      0.61      0.60      1240\n",
      "weighted avg       0.61      0.60      0.60      1240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, SpatialDropout1D, Conv1D,\n",
    "    Bidirectional, LSTM, Dense, Dropout,\n",
    "    GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Configure GPU for optimal performance\n",
    "def configure_gpu():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Enable memory growth for each GPU\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "            # Use mixed precision for better performance\n",
    "            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "            tf.keras.mixed_precision.set_global_policy(policy)\n",
    "            print('Mixed precision enabled')\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "configure_gpu()\n",
    "\n",
    "# Multi-Task Model Definition\n",
    "def create_multi_task_cnn_bilstm_model(max_len, max_features, embedding_matrix, embed_size=300):\n",
    "    \"\"\"\n",
    "    Creates a multi-task CNN-BiLSTM model architecture for joint prediction of\n",
    "    gendered abuse (task1) and explicit language (task3)\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(max_len,))\n",
    "    \n",
    "    # Embedding layer with pretrained weights (GloVe/FastText)\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=max_features,\n",
    "        output_dim=embed_size,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_len,\n",
    "        trainable=False  # As per paper, embeddings are non-trainable\n",
    "    )(input_layer)\n",
    "    \n",
    "    # Spatial Dropout to prevent overfitting\n",
    "    spatial_dropout = SpatialDropout1D(0.2)(embedding_layer)\n",
    "    \n",
    "    # CNN Layer - shared representation\n",
    "    conv_layer = Conv1D(\n",
    "        filters=64,\n",
    "        kernel_size=2,\n",
    "        activation='relu',\n",
    "        padding='same'\n",
    "    )(spatial_dropout)\n",
    "    \n",
    "    # Bidirectional LSTM Layer - shared representation\n",
    "    bilstm_layer = Bidirectional(\n",
    "        LSTM(\n",
    "            units=128,\n",
    "            return_sequences=True,\n",
    "            dropout=0.1,\n",
    "            recurrent_dropout=0.1\n",
    "        )\n",
    "    )(conv_layer)\n",
    "    \n",
    "    # Global Average Pooling - shared representation\n",
    "    gap_layer = GlobalAveragePooling1D()(bilstm_layer)\n",
    "    \n",
    "    # Task-specific layers for Task 1 (Gendered Abuse)\n",
    "    task1_dense = Dense(128, activation='relu', name='task1_dense')(gap_layer)\n",
    "    task1_dropout = Dropout(0.1)(task1_dense)\n",
    "    task1_output = Dense(2, activation='softmax', name='task1_output', dtype='float32')(task1_dropout)\n",
    "    \n",
    "    # Task-specific layers for Task 3 (Explicit Language)\n",
    "    task3_dense = Dense(128, activation='relu', name='task3_dense')(gap_layer)\n",
    "    task3_dropout = Dropout(0.1)(task3_dense)\n",
    "    task3_output = Dense(2, activation='softmax', name='task3_output', dtype='float32')(task3_dropout)\n",
    "    \n",
    "    # Create model with multiple outputs\n",
    "    model = Model(inputs=input_layer, outputs=[task1_output, task3_output])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Custom MacroF1Score Metric for multi-task learning\n",
    "class MacroF1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes=2, name='macro_f1_score', **kwargs):\n",
    "        super(MacroF1Score, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Convert probabilities to predicted class indices\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        \n",
    "        # Convert one-hot encoded y_true to class indices if needed\n",
    "        if len(y_true.shape) > 1 and y_true.shape[-1] > 1:\n",
    "            y_true = tf.argmax(y_true, axis=-1)\n",
    "        \n",
    "        # Initialize confusion matrix\n",
    "        conf_matrix = tf.math.confusion_matrix(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            num_classes=self.num_classes,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        # Calculate TP, FP, FN for each class\n",
    "        diag = tf.linalg.diag_part(conf_matrix)\n",
    "        row_sum = tf.reduce_sum(conf_matrix, axis=1)\n",
    "        col_sum = tf.reduce_sum(conf_matrix, axis=0)\n",
    "        \n",
    "        tp = diag\n",
    "        fp = col_sum - diag\n",
    "        fn = row_sum - diag\n",
    "        \n",
    "        # Update the state variables\n",
    "        self.tp.assign_add(tf.reduce_sum(tp))\n",
    "        self.fp.assign_add(tf.reduce_sum(fp))\n",
    "        self.fn.assign_add(tf.reduce_sum(fn))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        # Calculate precision and recall\n",
    "        precision = self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())\n",
    "        recall = self.tp / (self.tp + self.fn + tf.keras.backend.epsilon())\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "        \n",
    "        # Return macro F1 (average of per-class F1 scores)\n",
    "        return f1\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.tp.assign(0.)\n",
    "        self.fp.assign(0.)\n",
    "        self.fn.assign(0.)\n",
    "        self.count.assign(0.)\n",
    "\n",
    "# Model Training for multi-task learning\n",
    "def train_and_validate_multi_task_model(model, X_train, y_train_task1, y_train_task3, \n",
    "                                         X_val, y_val_task1, y_val_task3, \n",
    "                                         batch_size=32, epochs=15, model_dir='models_hi_task3_m1'):\n",
    "    \"\"\"\n",
    "    Trains the multi-task CNN-BiLSTM model with early stopping and model checkpointing\n",
    "    Returns the best model and training history\n",
    "    \"\"\"\n",
    "    # Create directory for saving models if it doesn't exist\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        os.path.join(model_dir, 'best_model_hi_task3_m1.h5'),\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Compile model with Adam optimizer\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss={\n",
    "            'task1_output': 'categorical_crossentropy',\n",
    "            'task3_output': 'categorical_crossentropy'\n",
    "        },\n",
    "        loss_weights={\n",
    "            'task1_output': 1.0,  # Weight for gendered abuse task\n",
    "            'task3_output': 1.0   # Weight for explicit language task\n",
    "        },\n",
    "        metrics={\n",
    "            'task1_output': ['accuracy', MacroF1Score(num_classes=2)],\n",
    "            'task3_output': ['accuracy', MacroF1Score(num_classes=2)]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, \n",
    "        {'task1_output': y_train_task1, 'task3_output': y_train_task3},\n",
    "        validation_data=(\n",
    "            X_val, \n",
    "            {'task1_output': y_val_task1, 'task3_output': y_val_task3}\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping, model_checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Load the best model found during training\n",
    "    best_model = load_model(\n",
    "        os.path.join(model_dir, 'best_model_hi_task3_m1.h5'), \n",
    "        custom_objects={'MacroF1Score': MacroF1Score}\n",
    "    )\n",
    "    \n",
    "    return history, best_model\n",
    "\n",
    "# Plot Training History for multi-task model\n",
    "def plot_multi_task_training_history(history, plot_dir='plots_hi_task3_m1'):\n",
    "    \"\"\"\n",
    "    Plots training history for both tasks (accuracy and loss curves)\n",
    "    Saves plots to specified directory\n",
    "    \"\"\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    # Task 1 (Gendered Abuse) plots\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot task1 accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history.history['task1_output_accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_task1_output_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Task 1 (Gendered Abuse) Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot task1 loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history.history['task1_output_loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_task1_output_loss'], label='Validation Loss')\n",
    "    plt.title('Task 1 (Gendered Abuse) Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Task 3 (Explicit Language) plots\n",
    "    # Plot task3 accuracy\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history.history['task3_output_accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_task3_output_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Task 3 (Explicit Language) Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot task3 loss\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(history.history['task3_output_loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_task3_output_loss'], label='Validation Loss')\n",
    "    plt.title('Task 3 (Explicit Language) Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, 'training_history_hi_task3_m1.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot combined loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Total Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Total Validation Loss')\n",
    "    plt.title('Multi-Task Model Combined Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, 'combined_loss_hi_task3_m1.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Evaluation function for multi-task model\n",
    "def evaluate_multi_task_validation(model, X_val, y_val_task1, y_val_task3, plot_dir='plots_hi_task3_m1'):\n",
    "    \"\"\"\n",
    "    Evaluates the multi-task model on validation data and saves metrics and plots\n",
    "    \"\"\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_pred_task1, y_pred_task3 = model.predict(X_val, batch_size=32)\n",
    "    \n",
    "    # Convert to class labels\n",
    "    y_pred_task1_labels = np.argmax(y_pred_task1, axis=1)\n",
    "    y_true_task1_labels = np.argmax(y_val_task1, axis=1)\n",
    "    \n",
    "    y_pred_task3_labels = np.argmax(y_pred_task3, axis=1)\n",
    "    y_true_task3_labels = np.argmax(y_val_task3, axis=1)\n",
    "    \n",
    "    # Task 1 (Gendered Abuse) metrics\n",
    "    task1_precision = precision_score(y_true_task1_labels, y_pred_task1_labels, average='weighted')\n",
    "    task1_recall = recall_score(y_true_task1_labels, y_pred_task1_labels, average='weighted')\n",
    "    task1_weighted_f1 = f1_score(y_true_task1_labels, y_pred_task1_labels, average='weighted')\n",
    "    task1_macro_f1 = f1_score(y_true_task1_labels, y_pred_task1_labels, average='macro')\n",
    "    \n",
    "    # Task 3 (Explicit Language) metrics\n",
    "    task3_precision = precision_score(y_true_task3_labels, y_pred_task3_labels, average='weighted')\n",
    "    task3_recall = recall_score(y_true_task3_labels, y_pred_task3_labels, average='weighted')\n",
    "    task3_weighted_f1 = f1_score(y_true_task3_labels, y_pred_task3_labels, average='weighted')\n",
    "    task3_macro_f1 = f1_score(y_true_task3_labels, y_pred_task3_labels, average='macro')\n",
    "    \n",
    "    # Classification reports\n",
    "    task1_report = classification_report(y_true_task1_labels, y_pred_task1_labels, \n",
    "                                        target_names=['not_hate', 'hate'])\n",
    "    \n",
    "    task3_report = classification_report(y_true_task3_labels, y_pred_task3_labels, \n",
    "                                        target_names=['not_explicit', 'explicit'])\n",
    "    \n",
    "    # Confusion matrices\n",
    "    task1_conf_matrix = confusion_matrix(y_true_task1_labels, y_pred_task1_labels)\n",
    "    task3_conf_matrix = confusion_matrix(y_true_task3_labels, y_pred_task3_labels)\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    # Task 1 confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(task1_conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Not Hate', 'Hate'],\n",
    "                yticklabels=['Not Hate', 'Hate'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Task 1 (Gendered Abuse) Confusion Matrix')\n",
    "    plt.savefig(os.path.join(plot_dir, 'task1_confusion_matrix_hi_task3_m1.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Task 3 confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(task3_conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Not Explicit', 'Explicit'],\n",
    "                yticklabels=['Not Explicit', 'Explicit'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Task 3 (Explicit Language) Confusion Matrix')\n",
    "    plt.savefig(os.path.join(plot_dir, 'task3_confusion_matrix_hi_task3_m1.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'task1': {\n",
    "            'precision': task1_precision,\n",
    "            'recall': task1_recall,\n",
    "            'f1_score_weighted': task1_weighted_f1,\n",
    "            'f1_score_macro': task1_macro_f1,\n",
    "            'classification_report': task1_report,\n",
    "            'confusion_matrix': task1_conf_matrix\n",
    "        },\n",
    "        'task3': {\n",
    "            'precision': task3_precision,\n",
    "            'recall': task3_recall,\n",
    "            'f1_score_weighted': task3_weighted_f1,\n",
    "            'f1_score_macro': task3_macro_f1,\n",
    "            'classification_report': task3_report,\n",
    "            'confusion_matrix': task3_conf_matrix\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Main Execution for Training and Validation\n",
    "if __name__ == \"__main__\":\n",
    "    # Split into train (80%) and validation (20%)\n",
    "    X_train, X_val, y_train_task1, y_val_task1, y_train_task3, y_val_task3 = train_test_split(\n",
    "        X, y_task1, y_task3, test_size=0.2, random_state=42, stratify=y_task1\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Validation samples: {len(X_val)}\")\n",
    "    \n",
    "    # Create multi-task model\n",
    "    embed_size = embedding_matrix.shape[1]\n",
    "    model = create_multi_task_cnn_bilstm_model(max_len, max_features, embedding_matrix, embed_size)\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Train multi-task model\n",
    "    history, trained_model = train_and_validate_multi_task_model(\n",
    "        model, X_train, y_train_task1, y_train_task3, \n",
    "        X_val, y_val_task1, y_val_task3,\n",
    "        batch_size=32,\n",
    "        epochs=15  \n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_multi_task_training_history(history)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_results = evaluate_multi_task_validation(\n",
    "        trained_model, X_val, y_val_task1, y_val_task3\n",
    "    )\n",
    "    \n",
    "    # Print Task 1 (Gendered Abuse) results\n",
    "    print(\"\\nTask 1 (Gendered Abuse) Validation Results:\")\n",
    "    print(f\"Precision: {val_results['task1']['precision']:.4f}\")\n",
    "    print(f\"Recall: {val_results['task1']['recall']:.4f}\")\n",
    "    print(f\"weighted F1 Score: {val_results['task1']['f1_score_weighted']:.4f}\")\n",
    "    print(f\"macro F1 Score: {val_results['task1']['f1_score_macro']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(val_results['task1']['classification_report'])\n",
    "    \n",
    "    # Print Task 3 (Explicit Language) results\n",
    "    print(\"\\nTask 3 (Explicit Language) Validation Results:\")\n",
    "    print(f\"Precision: {val_results['task3']['precision']:.4f}\")\n",
    "    print(f\"Recall: {val_results['task3']['recall']:.4f}\")\n",
    "    print(f\"weighted F1 Score: {val_results['task3']['f1_score_weighted']:.4f}\")\n",
    "    print(f\"macro F1 Score: {val_results['task3']['f1_score_macro']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(val_results['task3']['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93b3c135-ca56-4279-b9af-bf0056b36317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged dataset shape: (1517, 6)\n",
      "  unique_id                                               text binary_label_1  \\\n",
      "0      id_0  #BandraStation #SharadPawar #Muradabad  अगर अभ...       not_hate   \n",
      "1      id_1  #ConspiracyAgainstIndia  सुन लो रे देश के गद्द...       not_hate   \n",
      "2      id_2  #MarathaReservation : महाराष्ट्र में जश्न का म...       not_hate   \n",
      "3      id_3  #RheaChakraborty aap Mahesh bhatt se madad kiy...           hate   \n",
      "4      id_4  #SecularMaskOfd हमे पता नहि क्या कहते है ,इतना...       not_hate   \n",
      "\n",
      "   label_1 binary_label_3  label_3  \n",
      "0        0   not_explicit        0  \n",
      "1        0   not_explicit        0  \n",
      "2        0   not_explicit        0  \n",
      "3        1       explicit        1  \n",
      "4        0   not_explicit        0  \n",
      "\n",
      "Label distribution:\n",
      "Label 1 (gendered abuse): {'not_hate': 1159, 'hate': 358}\n",
      "Label 3 (explicit/aggressive): {'not_explicit': 1159, 'explicit': 358}\n"
     ]
    }
   ],
   "source": [
    "# Process label 1 dataset for Hindi\n",
    "df_l1 = pd.read_csv('test_hi_l1.csv', engine='python', on_bad_lines='skip')\n",
    "df_l1 = df_l1.rename(columns={'key': 'unique_id', 'sentence': 'text'})\n",
    "\n",
    "# Convert annotator columns to numeric without replacing NaNs (only 5 annotators)\n",
    "df_l1[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']] = df_l1[\n",
    "    ['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']\n",
    "].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Compute 'label_1' based on majority voting while ignoring NaNs\n",
    "df_l1['label_1'] = (df_l1[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']].mean(axis=1, skipna=True) >= 0.5).astype(int)\n",
    "\n",
    "# Create proper binary label for task 1 (gendered abuse)\n",
    "df_l1['binary_label_1'] = df_l1['label_1'].apply(lambda x: 'hate' if x == 1 else 'not_hate')\n",
    "\n",
    "# Process label 3 dataset for Hindi\n",
    "df_l3 = pd.read_csv('test_hi_l1.csv', engine='python', on_bad_lines='skip')\n",
    "df_l3 = df_l3.rename(columns={'key': 'unique_id', 'sentence': 'text'})\n",
    "\n",
    "# Convert annotator columns to numeric without replacing NaNs (only 5 annotators)\n",
    "df_l3[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']] = df_l3[\n",
    "    ['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']\n",
    "].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Compute 'label_3' based on majority voting while ignoring NaNs\n",
    "df_l3['label_3'] = (df_l3[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']].mean(axis=1, skipna=True) >= 0.5).astype(int)\n",
    "\n",
    "# Create proper binary label for task 3 (explicit/aggressive)\n",
    "df_l3['binary_label_3'] = df_l3['label_3'].apply(lambda x: 'explicit' if x == 1 else 'not_explicit')\n",
    "\n",
    "# Select columns for merging\n",
    "df_l1_slim = df_l1[['text', 'label_1', 'binary_label_1']]\n",
    "df_l3_slim = df_l3[['text', 'label_3', 'binary_label_3']]\n",
    "\n",
    "# Merge the datasets based on text field\n",
    "merged_df = pd.merge(df_l1_slim, df_l3_slim, on='text', how='inner')\n",
    "\n",
    "# Add a unique_id column to the merged dataset\n",
    "merged_df['unique_id'] = [f'id_{i}' for i in range(len(merged_df))]\n",
    "\n",
    "# Reorder columns\n",
    "merged_df = merged_df[['unique_id', 'text', 'binary_label_1', 'label_1', 'binary_label_3', 'label_3']]\n",
    "\n",
    "# Save the merged dataset\n",
    "merged_df.to_csv('test_hi_task3.csv', index=False)\n",
    "\n",
    "# Display information\n",
    "print(f\"Final merged dataset shape: {merged_df.shape}\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# Check label distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(f\"Label 1 (gendered abuse): {merged_df['binary_label_1'].value_counts().to_dict()}\")\n",
    "print(f\"Label 3 (explicit/aggressive): {merged_df['binary_label_3'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3703f78a-5f67-4b16-94a1-659b30ab8801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original vs Processed Text Samples:\n",
      "Original: #BandraStation #SharadPawar #Muradabad  अगर अभी आपको होश आ गया हो तो ये मान लीजिये की अब आप भारत के नागरिक नही है , अब आप आने वाले समय के शरिया कानून का पालन करते नजर आएंगे जहां काफिरों के लिए अलग क़ानून होगा और मुसलमानों के लिए अलग।  #गजवायेHind की शुरूआत हो चुकी है #10ईयर और।\n",
      "Processed:  bandrastation  sharadpawar  muradabad  अगर अभी आपको होश आ गया हो तो ये मान लीजिये की अब आप भारत के नागरिक नही है   अब आप आने वाले समय के शरिया कानून का पालन करते नजर आएंगे जहां काफिरों के लिए अलग क़ानून होगा और मुसलमानों के लिए अलग।   गजवायेhind की शुरूआत हो चुकी है    और।\n",
      "--------------------------------------------------\n",
      "Original: #ConspiracyAgainstIndia  सुन लो रे देश के गद्दारों घर के और बाहर वाले भी यदि तुम अभी भी नहीं सुधरे ना तो तुम्हारी गांड में छाता डालकर मोर बना दूंगा\n",
      "Processed:  conspiracyagainstindia  सुन लो रे देश के गद्दारों घर के और बाहर वाले भी यदि तुम अभी भी नहीं सुधरे ना तो तुम्हारी गांड में छाता डालकर मोर बना दूंगा\n",
      "--------------------------------------------------\n",
      "Original: #MarathaReservation : महाराष्ट्र में जश्न का माहौल  #Devendrafadnavis #Marathas #Maratha #Reservation <handle replaced><handle replaced><handle replaced><handle replaced><handle replaced><handle replaced><handle replaced><handle replaced><handle replaced>  \n",
      "Processed:  marathareservation   महाराष्ट्र में जश्न का माहौल   devendrafadnavis  marathas  maratha  reservation            \n",
      "--------------------------------------------------\n",
      "\n",
      "Processed dataset shape: (1517, 7)\n",
      "  unique_id                                               text  \\\n",
      "0      id_0  #BandraStation #SharadPawar #Muradabad  अगर अभ...   \n",
      "1      id_1  #ConspiracyAgainstIndia  सुन लो रे देश के गद्द...   \n",
      "2      id_2  #MarathaReservation : महाराष्ट्र में जश्न का म...   \n",
      "3      id_3  #RheaChakraborty aap Mahesh bhatt se madad kiy...   \n",
      "4      id_4  #SecularMaskOfd हमे पता नहि क्या कहते है ,इतना...   \n",
      "\n",
      "                                      processed_text binary_label_1  label_1  \\\n",
      "0   bandrastation  sharadpawar  muradabad  अगर अभ...       not_hate        0   \n",
      "1   conspiracyagainstindia  सुन लो रे देश के गद्द...       not_hate        0   \n",
      "2   marathareservation   महाराष्ट्र में जश्न का म...       not_hate        0   \n",
      "3   rheachakraborty aap mahesh bhatt se madad kiy...           hate        1   \n",
      "4   secularmaskofd हमे पता नहि क्या कहते है  इतना...       not_hate        0   \n",
      "\n",
      "  binary_label_3  label_3  \n",
      "0   not_explicit        0  \n",
      "1   not_explicit        0  \n",
      "2   not_explicit        0  \n",
      "3       explicit        1  \n",
      "4   not_explicit        0  \n"
     ]
    }
   ],
   "source": [
    "# Apply text normalization\n",
    "merged_df['processed_text'] = merged_df['text'].apply(lambda x: normalize_text(x))\n",
    "\n",
    "# Further processing to remove '...'\n",
    "merged_df['processed_text'] = merged_df['processed_text'].str.replace('...', '')\n",
    "\n",
    "# Display samples of processed text\n",
    "print(\"\\nOriginal vs Processed Text Samples:\")\n",
    "for i in range(3):\n",
    "    print(f\"Original: {merged_df['text'].iloc[i]}\")\n",
    "    print(f\"Processed: {merged_df['processed_text'].iloc[i]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Keep all columns but add processed text\n",
    "merged_df_final = merged_df[['unique_id', 'text', 'processed_text', 'binary_label_1', 'label_1', 'binary_label_3', 'label_3']]\n",
    "\n",
    "\n",
    "print(f\"\\nProcessed dataset shape: {merged_df_final.shape}\")\n",
    "print(merged_df_final.head())\n",
    "\n",
    "# Extract features (processed text)\n",
    "X = list(merged_df_final['processed_text'])\n",
    "\n",
    "# Extract labels for both tasks\n",
    "y_task1 = merged_df_final['label_1'].values\n",
    "y_task3 = merged_df_final['label_3'].values\n",
    "tokenizer.fit_on_texts(X)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding\n",
    "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "\n",
    "y_task1 = label_encoder.fit_transform(y_task1)\n",
    "y_task3 = label_encoder.fit_transform(y_task3)\n",
    "\n",
    "y_task1 = to_categorical(y_task1, num_classes=2)\n",
    "y_task3 = to_categorical(y_task3, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce712717-a2b9-42c0-b301-22dda3ccce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krmri\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krmri\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krmri\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krmri\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task 1 (Gendered Abuse) Test Results:\n",
      "Precision: 0.5837\n",
      "Recall: 0.7640\n",
      "weighted F1 Score: 0.6618\n",
      "macro F1 Score: 0.4331\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not_hate       0.76      1.00      0.87      1159\n",
      "        hate       0.00      0.00      0.00       358\n",
      "\n",
      "    accuracy                           0.76      1517\n",
      "   macro avg       0.38      0.50      0.43      1517\n",
      "weighted avg       0.58      0.76      0.66      1517\n",
      "\n",
      "\n",
      "Task 3 (Explicit Language) Test Results:\n",
      "Precision: 0.6854\n",
      "Recall: 0.5049\n",
      "weighted F1 Score: 0.5383\n",
      "macro F1 Score: 0.4856\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "not_explicit       0.81      0.46      0.59      1159\n",
      "    explicit       0.27      0.66      0.39       358\n",
      "\n",
      "    accuracy                           0.50      1517\n",
      "   macro avg       0.54      0.56      0.49      1517\n",
      "weighted avg       0.69      0.50      0.54      1517\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "test_results = evaluate_multi_task_validation(\n",
    "    trained_model, X, y_task1, y_task3\n",
    ")\n",
    "\n",
    "# Print Task 1 (Gendered Abuse) results\n",
    "print(\"\\nTask 1 (Gendered Abuse) Test Results:\")\n",
    "print(f\"Precision: {test_results['task1']['precision']:.4f}\")\n",
    "print(f\"Recall: {test_results['task1']['recall']:.4f}\")\n",
    "print(f\"weighted F1 Score: {test_results['task1']['f1_score_weighted']:.4f}\")\n",
    "print(f\"macro F1 Score: {test_results['task1']['f1_score_macro']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(test_results['task1']['classification_report'])\n",
    "\n",
    "# Print Task 3 (Explicit Language) results\n",
    "print(\"\\nTask 3 (Explicit Language) Test Results:\")\n",
    "print(f\"Precision: {test_results['task3']['precision']:.4f}\")\n",
    "print(f\"Recall: {test_results['task3']['recall']:.4f}\")\n",
    "print(f\"weighted F1 Score: {test_results['task3']['f1_score_weighted']:.4f}\")\n",
    "print(f\"macro F1 Score: {test_results['task3']['f1_score_macro']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(test_results['task3']['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11eed36-b9c0-4f6c-b3a9-6aa1b84c3e26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
