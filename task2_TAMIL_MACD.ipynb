{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoHpsjKef9Yx",
        "outputId": "f789d2e1-ec57-4217-c751-0f6d4e5c255a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-multilearn==0.2.0\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/89.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-multilearn==0.2.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from skmultilearn.adapt import MLkNN\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import hamming_loss, accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "import seaborn as sns\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "8-9B399IqrCM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from torch.optim import AdamW\n",
        "import json\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "wpt = nltk.WordPunctTokenizer()\n",
        "\n",
        "d2= pd.read_csv('/content/drive/MyDrive/Nlp/MACD-main/dataset_80_10_10/tamil_train.csv')\n",
        "d2\n",
        "d2 = d2.rename(columns={'key' : 'label', 'sentence' : 'text'})\n",
        "d2.to_csv('updated_train_ta.csv', index=False)\n",
        "# d2\n",
        "\n",
        "# Convert annotator columns to numeric without replacing NaNs\n",
        "# d2[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']] = d2[\n",
        "#     ['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']\n",
        "# ].apply(pd.to_numeric, errors='coerce')  # NaNs are retained\n",
        "\n",
        "# Compute 'label' based on majority voting while ignoring NaNs\n",
        "# d2['label'] = (d2[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']].mean(axis=1, skipna=True) >= 0.5).astype(int)\n",
        "# d2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8gYI-jkqrLi",
        "outputId": "93c79c50-de43-470c-9212-17ae61a072cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create binary label ('hate' or 'not_hate')\n",
        "def determine_binary_label(label):\n",
        "    return 'hate' if label == 1 else 'not_hate'\n",
        "\n",
        "d2['binary_label'] = d2['label'].apply(determine_binary_label)\n",
        "\n",
        "# # Reorder columns\n",
        "d2 = d2[['text', 'binary_label', 'label']]"
      ],
      "metadata": {
        "id": "UvXUc_cAqrRs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"\n",
        "                               u\"\\U0001F700-\\U0001F77F\"\n",
        "                               u\"\\U0001F780-\\U0001F7FF\"\n",
        "                               u\"\\U0001F800-\\U0001F8FF\"\n",
        "                               u\"\\U0001F900-\\U0001F9FF\"\n",
        "                               u\"\\U0001FA00-\\U0001FA6F\"\n",
        "                               u\"\\U0001FA70-\\U0001FAFF\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\[.*?\\]', ' ', text)\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "    text = re.sub(r'<.*?>+', ' ', text)\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r'\\w*\\d\\w*', ' ', text)\n",
        "    text = re.sub(r'<handle replaced>', '', text)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "    return text\n",
        "\n",
        "## Apply the written function ##\n",
        "d2.loc[:, 'text'] = d2['text'].apply(lambda x: normalize_text(x))\n",
        "processed_list = []\n",
        "for j in d2['text']:\n",
        "    process = j.replace('...','')\n",
        "    processed_list.append(process)\n",
        "\n",
        "df_processed = pd.DataFrame(processed_list)\n",
        "df_processed.columns = ['text']\n",
        "df_processed.head(n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "YOpcp6FMqrWQ",
        "outputId": "baf45d1a-05d4-4fc3-834c-85bae78e0690"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text\n",
              "0  சலோமியா             சுண்ட கஞ்சி   சோறு டா குழம...\n",
              "1  டேய் பொட்ட பாடு  உன் வாழுல  ஊரா ன்   சாமானை வை...\n",
              "2                      நான்கு கிளிகள் மூன்று பூக்கள்\n",
              "3                               நல்ல மூடு போல உனக்கு\n",
              "4            இந்த கேலட்டு புண்டை தொல்லை தாங்க முடியல"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9f093b03-cc54-4a0e-868c-75fe28a8b063\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>சலோமியா             சுண்ட கஞ்சி   சோறு டா குழம...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>டேய் பொட்ட பாடு  உன் வாழுல  ஊரா ன்   சாமானை வை...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>நான்கு கிளிகள் மூன்று பூக்கள்</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>நல்ல மூடு போல உனக்கு</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>இந்த கேலட்டு புண்டை தொல்லை தாங்க முடியல</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f093b03-cc54-4a0e-868c-75fe28a8b063')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9f093b03-cc54-4a0e-868c-75fe28a8b063 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9f093b03-cc54-4a0e-868c-75fe28a8b063');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0289c925-86da-4c58-a88a-35b736233cf6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0289c925-86da-4c58-a88a-35b736233cf6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0289c925-86da-4c58-a88a-35b736233cf6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_processed",
              "summary": "{\n  \"name\": \"df_processed\",\n  \"rows\": 24000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 23838,\n        \"samples\": [\n          \"\\u0ba4\\u0bc2 \\u0ba4\\u0bc2 \\u0ba4\\u0bc2 \\u0ba4\\u0bc2 \\u0ba4\\u0bc2 \\u0ba4\\u0bc2 \\u0ba4\\u0bc2 \\u0ba4\\u0bc2 \\u0ba4\\u0bc2 \\u0ba4\\u0bc2 \\u0ba4\\u0bc2 \\u0ba4\\u0bc2 \\u0ba4\\u0bc2 \\u0ba4\\u0bc2\",\n          \"\\u0b9a\\u0bbf\\u0bb0\\u0bbf\\u0baa\\u0bcd\\u0baa\\u0bc1 \\u0bb0\\u0bca\\u0bae\\u0bcd\\u0baa \\u0b85\\u0bb4\\u0b95\\u0bbe \\u0ba4\\u0bbe\\u0ba9\\u0bcd \\u0b87\\u0bb0\\u0bc1\\u0b95\\u0bcd\\u0b95\\u0bc1\\u0ba4\\u0bc1 \\u0bb5\\u0bc6\\u0bb0\\u0bbf \\u0ba8\\u0bc8\\u0bb8\\u0bcd \\u0b9a\\u0bc2\\u0baa\\u0bcd\\u0baa\\u0bb0\\u0bcd \\u0b85\\u0bb0\\u0bc1\\u0bae\\u0bc8 \\u0b9a\\u0bc7\\u0bb2\\u0bae\\u0bcd\",\n          \"\\u0ba8\\u0bbe\\u0ba4\\u0bcd\\u0ba4\\u0bae\\u0bcd \\u0baa\\u0bc1\\u0b9f\\u0bbf\\u0b9a\\u0bcd\\u0b9a \\u0baa\\u0bc1             \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = list(df_processed['text'])\n",
        "y = d2[['label']].values\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXdlCA1UqrbO",
        "outputId": "e69afe6f-e34d-421b-a1f4-f867f0b43fdd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [0],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    LSTM, Activation, Dropout, Dense, Flatten,\n",
        "    Bidirectional, GRU, concatenate, SpatialDropout1D,\n",
        "    GlobalMaxPooling1D, GlobalAveragePooling1D, Conv1D,\n",
        "    Embedding, Input, Concatenate\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "######## Textual Features for Embedding ###################\n",
        "max_len = 100\n",
        "max_features = 4479\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(X)\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# Padding\n",
        "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjjbadEgqrgB",
        "outputId": "ec3ab504-4c8f-4ad7-de32-ec4acd1655ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 335  354   25 ...    0    0    0]\n",
            " [  30   65   95 ...    0    0    0]\n",
            " [3490 1556 3491 ...    0    0    0]\n",
            " ...\n",
            " [  70  126 1655 ...    0    0    0]\n",
            " [ 362 1740  355 ...    0    0    0]\n",
            " [ 209  341 1818 ...    0    0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y.ravel())\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrmSdWu5qrk_",
        "outputId": "cd2fe78a-51ce-4ffb-f665-a6901bfc131f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, ..., 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "y = to_categorical(y, num_classes=2)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75sqHAvqqrpf",
        "outputId": "dc200dda-f1c2-4a97-c002-8f30a73f4d51"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       ...,\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Nlp/uli_dataset-main/glove_embeddings.json', encoding=\"utf8\") as f:\n",
        "    embeddings_list = json.load(f)\n",
        "\n",
        "# Convert the list of vectors to a dictionary with word indices as keys\n",
        "embeddings_dictionary = {str(i): vector for i, vector in enumerate(embeddings_list)}\n",
        "\n",
        "# Define tokenizer\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size\n",
        "word_index = tokenizer.word_index\n",
        "num_words = min(max_features, vocab_size)  # Limit vocab to max_features\n",
        "\n",
        "# Get embedding dimension (from first vector in list)\n",
        "embed_size = len(embeddings_list[0]) if embeddings_list else 0\n",
        "\n",
        "# Initialize embedding matrix\n",
        "embedding_matrix = np.zeros((num_words, embed_size))\n",
        "\n",
        "# Fill embedding matrix with corresponding word vectors\n",
        "for word, index in word_index.items():\n",
        "    if index >= max_features:\n",
        "        continue\n",
        "    embedding_vector = embeddings_dictionary.get(word) or embeddings_dictionary.get(str(index))\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = np.asarray(embedding_vector, dtype=np.float32)\n",
        "\n",
        "print(\"Embedding matrix shape:\", embedding_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7cqifmWqrue",
        "outputId": "3b67f0d6-6204-4e79-a2c7-e34107d7bdbd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding matrix shape: (4479, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Embedding, SpatialDropout1D, Conv1D,\n",
        "    Bidirectional, LSTM, Dense, Dropout,\n",
        "    GlobalAveragePooling1D\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "def create_cnn_bilstm_model(max_len, max_features, embedding_matrix, embed_size=300):\n",
        "    \"\"\"\n",
        "    Creates the CNN-BiLSTM model architecture as described in the paper\n",
        "    \"\"\"\n",
        "    # Input layer\n",
        "    input_layer = Input(shape=(max_len,))\n",
        "\n",
        "    # Embedding layer with pretrained weights (GloVe/FastText)\n",
        "    embedding_layer = Embedding(\n",
        "        input_dim=max_features,\n",
        "        output_dim=embed_size,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=max_len,\n",
        "        trainable=False  # As per paper, embeddings are non-trainable\n",
        "    )(input_layer)\n",
        "\n",
        "    # Spatial Dropout to prevent overfitting (as mentioned in paper)\n",
        "    spatial_dropout = SpatialDropout1D(0.2)(embedding_layer)\n",
        "    # CNN Layer (as described in paper)\n",
        "    conv_layer = Conv1D(\n",
        "        filters=64,  # As per paper\n",
        "        kernel_size=2,  # As per paper\n",
        "        activation='tanh',\n",
        "        padding='same'\n",
        "    )(spatial_dropout)\n",
        "\n",
        "    # Bidirectional LSTM Layer (as described in paper)\n",
        "    bilstm_layer = Bidirectional(\n",
        "        LSTM(\n",
        "            units=128,  # As per paper\n",
        "            return_sequences=True,\n",
        "            dropout=0.1,  # As per paper\n",
        "            recurrent_dropout=0.5  # As per paper\n",
        "        )\n",
        "    )(conv_layer)\n",
        "\n",
        "    # Global Average Pooling (as per paper)\n",
        "    gap_layer = GlobalAveragePooling1D()(bilstm_layer)\n",
        "\n",
        "    # Dense layer (as per paper)\n",
        "    dense_layer = Dense(128, activation='tanh')(gap_layer)\n",
        "    dropout_layer = Dropout(0.2)(dense_layer)  # Additional dropout as per paper\n",
        "\n",
        "    # Output layer (use float32 for softmax for numerical stability)\n",
        "    output_layer = Dense(2, activation='softmax', dtype='float32')(dropout_layer)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "XI1_Do9gqrzW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MacroF1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, num_classes = 2, name='macro_f1_score', **kwargs):\n",
        "        super(MacroF1Score, self).__init__(name=name, **kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
        "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
        "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
        "        self.count = self.add_weight(name='count', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        # Convert probabilities to predicted class indices\n",
        "        y_pred = tf.argmax(y_pred, axis=-1)\n",
        "\n",
        "        # Convert one-hot encoded y_true to class indices if needed\n",
        "        if len(y_true.shape) > 1 and y_true.shape[-1] > 1:\n",
        "            y_true = tf.argmax(y_true, axis=-1)\n",
        "\n",
        "        # Initialize confusion matrix\n",
        "        conf_matrix = tf.math.confusion_matrix(\n",
        "            y_true,\n",
        "            y_pred,\n",
        "            num_classes=self.num_classes,\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "\n",
        "        # Calculate TP, FP, FN for each class\n",
        "        diag = tf.linalg.diag_part(conf_matrix)\n",
        "        row_sum = tf.reduce_sum(conf_matrix, axis=1)\n",
        "        col_sum = tf.reduce_sum(conf_matrix, axis=0)\n",
        "\n",
        "        tp = diag\n",
        "        fp = col_sum - diag\n",
        "        fn = row_sum - diag\n",
        "\n",
        "        # Update the state variables\n",
        "        self.tp.assign_add(tf.reduce_sum(tp))\n",
        "        self.fp.assign_add(tf.reduce_sum(fp))\n",
        "        self.fn.assign_add(tf.reduce_sum(fn))\n",
        "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        # Calculate precision and recall\n",
        "        precision = self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())\n",
        "        recall = self.tp / (self.tp + self.fn + tf.keras.backend.epsilon())\n",
        "\n",
        "        # Calculate F1 score\n",
        "        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
        "\n",
        "        # Return macro F1 (average of per-class F1 scores)\n",
        "        return f1\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.tp.assign(0.)\n",
        "        self.fp.assign(0.)\n",
        "        self.fn.assign(0.)\n",
        "        self.count.assign(0.)"
      ],
      "metadata": {
        "id": "cTOgvCFAqr30"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_MODEL_DIR = '/content/drive/MyDrive/Nlp/models_cnn_bilstm'\n",
        "CNN_PLOTS_DIR = '/content/drive/MyDrive/Nlp/plots_cnn_bilstm'\n",
        "CNN_EVAL_DIR = '/content/drive/MyDrive/Nlp/eval_cnn_bilstm'\n"
      ],
      "metadata": {
        "id": "3lBylk0ctoD9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate_model(model, X_train, y_train, X_val, y_val, batch_size=82, epochs=5, model_dir='models_ta_task2_m1'):\n",
        "    \"\"\"\n",
        "    Trains the CNN-BiLSTM model with early stopping and model checkpointing\n",
        "    Returns the best model and training history\n",
        "    \"\"\"\n",
        "    # Create directory for saving models if it doesn't exist\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='macro_f1_score',\n",
        "        patience=2,\n",
        "        restore_best_weights=True,\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    model_checkpoint = ModelCheckpoint(\n",
        "        os.path.join(model_dir, 'best_model_ta_task2_m1.h5'),  # Save entire model\n",
        "        monitor='macro_f1_score',\n",
        "        mode='max',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Compile model with Adam optimizer (as per paper)\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy', MacroF1Score(num_classes=2)]\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        callbacks=[early_stopping, model_checkpoint],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Load the best model found during training\n",
        "    best_model = load_model(os.path.join(model_dir, 'best_model_ta_task2_m1.h5'),\n",
        "                          custom_objects={'MacroF1Score': MacroF1Score})\n",
        "\n",
        "    return history, best_model\n",
        "\n"
      ],
      "metadata": {
        "id": "grQRdYU8qr7s"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history, plot_dir='plots_nlp_project_ta_task2_m1'):\n",
        "    \"\"\"\n",
        "    Plots training history (accuracy and loss curves)\n",
        "    Saves plots to specified directory\n",
        "    \"\"\"\n",
        "    os.makedirs(plot_dir, exist_ok=True)\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(plot_dir, 'training_history_hi_task1_m1.png'))\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "Ui-SNaBYqsAs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_validation(model, X_val, y_val, plot_dir='best_model_ta_task2_m1.h5'):\n",
        "    \"\"\"\n",
        "    Evaluates the model on validation data and saves metrics and plots\n",
        "    \"\"\"\n",
        "    os.makedirs(plot_dir, exist_ok=True)\n",
        "\n",
        "    # Predict probabilities\n",
        "    y_pred_proba = model.predict(X_val, batch_size=64)\n",
        "\n",
        "    # Convert to class labels\n",
        "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "    y_true = np.argmax(y_val, axis=1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "\n",
        "    # Classification report\n",
        "    report = classification_report(y_true, y_pred, target_names=['not_hate', 'hate'])\n",
        "\n",
        "    # Confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Not Hate', 'Hate'],\n",
        "                yticklabels=['Not Hate', 'Hate'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix (Validation)')\n",
        "    plt.savefig(os.path.join(plot_dir, 'confusion_matrix_val_ta_task2_m1.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score_weighted': weighted_f1,\n",
        "        'f1_score_macro': macro_f1,\n",
        "        'classification_report': report,\n",
        "        'confusion_matrix': conf_matrix\n",
        "    }"
      ],
      "metadata": {
        "id": "esAU1FXPqsFd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression_baseline(X_train_text, X_val_text, y_train_lr, y_val_lr, save_dir='/content/drive/MyDrive/Nlp/lr_baseline'):\n",
        "    import pickle\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Create TF-IDF features\n",
        "    tfidf = TfidfVectorizer(max_features=5000)\n",
        "    X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
        "    X_val_tfidf = tfidf.transform(X_val_text)\n",
        "\n",
        "    # Train logistic regression\n",
        "    logreg = LogisticRegression(max_iter=1000)\n",
        "    logreg.fit(X_train_tfidf, y_train_lr.ravel())\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    y_pred_lr = logreg.predict(X_val_tfidf)\n",
        "    precision = precision_score(y_val_lr, y_pred_lr, average='weighted')\n",
        "    recall = recall_score(y_val_lr, y_pred_lr, average='weighted')\n",
        "    weighted_f1 = f1_score(y_val_lr, y_pred_lr, average='weighted')\n",
        "    macro_f1 = f1_score(y_val_lr, y_pred_lr, average='macro')\n",
        "    report = metrics.classification_report(y_val_lr, y_pred_lr, target_names=['not_hate', 'hate'])\n",
        "    conf_matrix_lr = confusion_matrix(y_val_lr, y_pred_lr)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix_lr, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Not Hate', 'Hate'],\n",
        "                yticklabels=['Not Hate', 'Hate'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix (Logistic Regression)')\n",
        "    plt.savefig(os.path.join(save_dir, 'confusion_matrix_lr.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Save the LR model and TF-IDF vectorizer\n",
        "    with open(os.path.join(save_dir, 'logistic_regression_model.pkl'), 'wb') as f:\n",
        "        pickle.dump({'model': logreg, 'tfidf': tfidf}, f)\n",
        "\n",
        "    print(\"\\nLogistic Regression Baseline Evaluation (Validation):\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "    print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    return logreg, tfidf\n",
        "\n",
        "def evaluate_logistic_regression_on_test(logreg, tfidf, save_dir='/content/drive/MyDrive/Nlp/lr_baseline'):\n",
        "    # Load and preprocess test data\n",
        "    d2_test = pd.read_csv('/content/drive/MyDrive/Nlp/MACD-main/dataset_80_10_10/tamil_test.csv', engine='python', on_bad_lines='skip')\n",
        "    d2_test = d2_test.rename(columns={'key': 'label', 'sentence': 'text'})\n",
        "    d2_test['binary_label'] = d2_test['label'].apply(determine_binary_label)\n",
        "    d2_test = d2_test[['text', 'binary_label', 'label']]\n",
        "    d2_test['text'] = d2_test['text'].apply(lambda x: normalize_text(x))\n",
        "    processed_list_test = [j.replace('...', '') for j in d2_test['text']]\n",
        "    df_processed_test = pd.DataFrame(processed_list_test, columns=['text'])\n",
        "    X_test_text = list(df_processed_test['text'])\n",
        "    y_test = d2_test[['label']].values\n",
        "\n",
        "    X_test_tfidf = tfidf.transform(X_test_text)\n",
        "    y_pred_test = logreg.predict(X_test_tfidf)\n",
        "    precision = precision_score(y_test, y_pred_test, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred_test, average='weighted')\n",
        "    weighted_f1 = f1_score(y_test, y_pred_test, average='weighted')\n",
        "    macro_f1 = f1_score(y_test, y_pred_test, average='macro')\n",
        "    report = metrics.classification_report(y_test, y_pred_test, target_names=['not_hate', 'hate'])\n",
        "    conf_matrix_test = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Not Hate', 'Hate'],\n",
        "                yticklabels=['Not Hate', 'Hate'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix (Logistic Regression Test)')\n",
        "    plt.savefig(os.path.join(save_dir, 'confusion_matrix_lr_test.png'))\n",
        "    plt.close()\n",
        "\n",
        "    print(\"\\nLogistic Regression Baseline Evaluation (Test):\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "    print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(report)"
      ],
      "metadata": {
        "id": "n1m5BttRtPDS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # ------------------ CNN-BiLSTM Baseline ------------------ #\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "    print(f\"Training samples (CNN): {len(X_train)}\")\n",
        "    print(f\"Validation samples (CNN): {len(X_val)}\")\n",
        "    embed_size = embedding_matrix.shape[1]\n",
        "    cnn_model = create_cnn_bilstm_model(max_len, max_features, embedding_matrix, embed_size)\n",
        "    cnn_model.summary()\n",
        "    history, trained_cnn_model = train_and_validate_model(\n",
        "        cnn_model, X_train, y_train, X_val, y_val,\n",
        "        batch_size=64,\n",
        "        epochs=5,\n",
        "        model_dir=CNN_MODEL_DIR\n",
        "    )\n",
        "    plot_training_history(history, plot_dir=CNN_PLOTS_DIR)\n",
        "    val_results = evaluate_validation(trained_cnn_model, X_val, y_val, plot_dir=CNN_EVAL_DIR)\n",
        "\n",
        "    print(\"\\nCNN-BiLSTM Validation Results:\")\n",
        "    print(f\"Precision: {val_results['precision']:.4f}\")\n",
        "    print(f\"Recall: {val_results['recall']:.4f}\")\n",
        "    print(f\"Weighted F1 Score: {val_results['f1_score_weighted']:.4f}\")\n",
        "    print(f\"Macro F1 Score: {val_results['f1_score_macro']:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(val_results['classification_report'])\n",
        "\n",
        "    d2_test = pd.read_csv('/content/drive/MyDrive/Nlp/MACD-main/dataset_80_10_10/tamil_test.csv', engine='python', on_bad_lines='skip')\n",
        "    d2_test = d2_test.rename(columns={'key': 'label', 'sentence': 'text'})\n",
        "    d2_test['binary_label'] = d2_test['label'].apply(determine_binary_label)\n",
        "    d2_test = d2_test[['text', 'binary_label', 'label']]\n",
        "    d2_test['text'] = d2_test['text'].apply(lambda x: normalize_text(x))\n",
        "    processed_list_test = [j.replace('...', '') for j in d2_test['text']]\n",
        "    df_processed_test = pd.DataFrame(processed_list_test, columns=['text'])\n",
        "    X_test_text_cnn = list(df_processed_test['text'])\n",
        "    # Use the same tokenizer for CNN baseline\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test_text_cnn)\n",
        "    X_test_padded = pad_sequences(X_test_seq, padding='post', maxlen=max_len)\n",
        "\n",
        "    test_results = evaluate_validation(trained_cnn_model, X_test_padded, to_categorical(label_encoder.fit_transform(d2_test[['label']].values.ravel()), num_classes=2), plot_dir=CNN_EVAL_DIR)\n",
        "\n",
        "    print(\"\\nCNN-BiLSTM Test Results:\")\n",
        "    print(f\"Precision: {test_results['precision']:.4f}\")\n",
        "    print(f\"Recall: {test_results['recall']:.4f}\")\n",
        "    print(f\"Weighted F1 Score: {test_results['f1_score_weighted']:.4f}\")\n",
        "    print(f\"Macro F1 Score: {test_results['f1_score_macro']:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(test_results['classification_report'])\n",
        "\n",
        "    # ------------------ Logistic Regression Baseline ------------------ #\n",
        "    # Split the original cleaned text for LR baseline\n",
        "    X_text = list(df_processed['text']) # df_processed contains the cleaned text data\n",
        "    X_train_text, X_val_text, y_train_lr, y_val_lr = train_test_split(\n",
        "        X_text, d2['label'], test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    lr_model, tfidf_vectorizer = logistic_regression_baseline(X_train_text, X_val_text, y_train_lr, y_val_lr, save_dir='/content/drive/MyDrive/Nlp/lr_baseline')\n",
        "\n",
        "    # Evaluate LR baseline on test set\n",
        "    evaluate_logistic_regression_on_test(lr_model, tfidf_vectorizer, save_dir='/content/drive/MyDrive/Nlp/lr_baseline')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bv2xnbW1qsJ_",
        "outputId": "965fb516-454d-49d1-fc59-06dde143119d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples (CNN): 19200\n",
            "Validation samples (CNN): 4800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │       \u001b[38;5;34m223,950\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ spatial_dropout1d               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │         \u001b[38;5;34m6,464\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m197,632\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m258\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">223,950</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ spatial_dropout1d               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,464</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">197,632</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m461,200\u001b[0m (1.76 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">461,200</span> (1.76 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m237,250\u001b[0m (926.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">237,250</span> (926.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m223,950\u001b[0m (874.80 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">223,950</span> (874.80 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682ms/step - accuracy: 0.5406 - loss: 0.6780 - macro_f1_score: 0.5406\n",
            "Epoch 1: macro_f1_score improved from -inf to 0.54693, saving model to /content/drive/MyDrive/Nlp/models_cnn_bilstm/best_model_ta_task2_m1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 725ms/step - accuracy: 0.5406 - loss: 0.6779 - macro_f1_score: 0.5406 - val_accuracy: 0.5527 - val_loss: 0.6547 - val_macro_f1_score: 0.5527\n",
            "Epoch 2/5\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634ms/step - accuracy: 0.5662 - loss: 0.6573 - macro_f1_score: 0.5662\n",
            "Epoch 2: macro_f1_score improved from 0.54693 to 0.56854, saving model to /content/drive/MyDrive/Nlp/models_cnn_bilstm/best_model_ta_task2_m1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 674ms/step - accuracy: 0.5662 - loss: 0.6573 - macro_f1_score: 0.5662 - val_accuracy: 0.5615 - val_loss: 0.6434 - val_macro_f1_score: 0.5615\n",
            "Epoch 3/5\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 623ms/step - accuracy: 0.5687 - loss: 0.6481 - macro_f1_score: 0.5687\n",
            "Epoch 3: macro_f1_score improved from 0.56854 to 0.57214, saving model to /content/drive/MyDrive/Nlp/models_cnn_bilstm/best_model_ta_task2_m1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 659ms/step - accuracy: 0.5687 - loss: 0.6481 - macro_f1_score: 0.5687 - val_accuracy: 0.5713 - val_loss: 0.6487 - val_macro_f1_score: 0.5712\n",
            "Epoch 4/5\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631ms/step - accuracy: 0.5830 - loss: 0.6454 - macro_f1_score: 0.5830\n",
            "Epoch 4: macro_f1_score improved from 0.57214 to 0.57849, saving model to /content/drive/MyDrive/Nlp/models_cnn_bilstm/best_model_ta_task2_m1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 667ms/step - accuracy: 0.5830 - loss: 0.6454 - macro_f1_score: 0.5830 - val_accuracy: 0.5790 - val_loss: 0.6391 - val_macro_f1_score: 0.5790\n",
            "Epoch 5/5\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621ms/step - accuracy: 0.5803 - loss: 0.6401 - macro_f1_score: 0.5803\n",
            "Epoch 5: macro_f1_score improved from 0.57849 to 0.57984, saving model to /content/drive/MyDrive/Nlp/models_cnn_bilstm/best_model_ta_task2_m1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 691ms/step - accuracy: 0.5803 - loss: 0.6401 - macro_f1_score: 0.5803 - val_accuracy: 0.5748 - val_loss: 0.6358 - val_macro_f1_score: 0.5748\n",
            "Restoring model weights from the end of the best epoch: 5.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 125ms/step\n",
            "\n",
            "CNN-BiLSTM Validation Results:\n",
            "Precision: 0.6311\n",
            "Recall: 0.5748\n",
            "Weighted F1 Score: 0.5450\n",
            "Macro F1 Score: 0.5514\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    not_hate       0.53      0.86      0.65      2248\n",
            "        hate       0.72      0.33      0.45      2552\n",
            "\n",
            "    accuracy                           0.57      4800\n",
            "   macro avg       0.62      0.59      0.55      4800\n",
            "weighted avg       0.63      0.57      0.54      4800\n",
            "\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step\n",
            "\n",
            "CNN-BiLSTM Test Results:\n",
            "Precision: 0.6248\n",
            "Recall: 0.5757\n",
            "Weighted F1 Score: 0.5491\n",
            "Macro F1 Score: 0.5548\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    not_hate       0.53      0.84      0.65      1410\n",
            "        hate       0.71      0.34      0.46      1590\n",
            "\n",
            "    accuracy                           0.58      3000\n",
            "   macro avg       0.62      0.59      0.55      3000\n",
            "weighted avg       0.62      0.58      0.55      3000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-6989b6300dda>:14: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
            "  logreg.fit(X_train_tfidf, y_train_lr.ravel())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Logistic Regression Baseline Evaluation (Validation):\n",
            "Precision: 0.7411\n",
            "Recall: 0.7406\n",
            "Weighted F1 Score: 0.7408\n",
            "Macro F1 Score: 0.7399\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    not_hate       0.72      0.73      0.73      2248\n",
            "        hate       0.76      0.75      0.75      2552\n",
            "\n",
            "    accuracy                           0.74      4800\n",
            "   macro avg       0.74      0.74      0.74      4800\n",
            "weighted avg       0.74      0.74      0.74      4800\n",
            "\n",
            "\n",
            "Logistic Regression Baseline Evaluation (Test):\n",
            "Precision: 0.7375\n",
            "Recall: 0.7377\n",
            "Weighted F1 Score: 0.7375\n",
            "Macro F1 Score: 0.7365\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    not_hate       0.72      0.71      0.72      1410\n",
            "        hate       0.75      0.76      0.75      1590\n",
            "\n",
            "    accuracy                           0.74      3000\n",
            "   macro avg       0.74      0.74      0.74      3000\n",
            "weighted avg       0.74      0.74      0.74      3000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##TESTING###\n",
        "d2= pd.read_csv('/content/drive/MyDrive/Nlp/MACD-main/dataset_80_10_10/tamil_test.csv', engine='python', on_bad_lines='skip')\n",
        "d2\n",
        "d2 = d2.rename(columns={'key' : 'label', 'sentence' : 'text'})\n",
        "d2.to_csv('updated_test_ta.csv', index=False)\n",
        "# d2\n",
        "\n",
        "# Convert annotator columns to numeric without replacing NaNs\n",
        "# d2[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']] = d2[\n",
        "#     ['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']\n",
        "# ].apply(pd.to_numeric, errors='coerce')  # NaNs are retained\n",
        "\n",
        "# Compute 'label' based on majority voting while ignoring NaNs\n",
        "# d2['label'] = (d2[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']].mean(axis=1, skipna=True) >= 0.5).astype(int)\n",
        "# d2"
      ],
      "metadata": {
        "id": "eTiMWaj_qsOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2.loc[:, 'binary_label'] = d2['label'].apply(determine_binary_label)\n",
        "\n",
        "# # Reorder columns\n",
        "d2 = d2[['text', 'binary_label', 'label']]\n",
        "\n",
        "d2.to_csv('updated_test_ta.csv', index=False)\n",
        "\n",
        "d2.loc[:, 'text'] = d2['text'].apply(lambda x: normalize_text(x))\n",
        "processed_list = []\n",
        "for j in d2['text']:\n",
        "    process = j.replace('...','')\n",
        "    processed_list.append(process)\n",
        "\n",
        "df_processed = pd.DataFrame(processed_list)\n",
        "df_processed.columns = ['text']\n",
        "df_processed.head(n=5)\n",
        "\n",
        "X = list(df_processed['text'])\n",
        "y = d2[['label']].values\n",
        "\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# Padding\n",
        "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
        "\n",
        "y = label_encoder.fit_transform(y.ravel())\n",
        "\n",
        "y = to_categorical(y, num_classes=2)"
      ],
      "metadata": {
        "id": "0ASpazs_qsSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = evaluate_validation(trained_model, X, y)\n",
        "\n",
        "print(r\"\\Test Results:\")\n",
        "print(f\"Precision: {val_results['precision']:.4f}\")\n",
        "print(f\"Recall: {val_results['recall']:.4f}\")\n",
        "print(f\"weighted F1 Score: {val_results['f1_score_weighted']:.4f}\")\n",
        "print(f\"macro F1 Score: {val_results['f1_score_macro']:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(val_results['classification_report'])"
      ],
      "metadata": {
        "id": "MHjA2JI5qsXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LxFofz05qscs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DrVBpgVAqshj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}