{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb2c6517-f420-45be-871c-fab3425a2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from skmultilearn.adapt import MLkNN\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import hamming_loss, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c83c24eb-6e45-4aff-a959-16b7a97b3425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged dataset shape: (6779, 6)\n",
      "  unique_id                                               text binary_label_1  \\\n",
      "0      id_0     *1. à®®à¯à®°à®šà¯Šà®²à®¿ à®…à®²à¯à®µà®²à®•à®®à¯ à®…à®®à¯ˆà®¨à¯à®¤à¯à®³à¯à®³ à®‡à®Ÿà®®à¯ à®ªà®à¯à®šà®®à®¿...       not_hate   \n",
      "1      id_1     à®šà¯‹à®¤à¯à®¤à¯à®•à¯à®•à¯ à®ªà®¿à®šà¯à®šà¯ˆ à®à®Ÿà¯à®•à¯à®•à®¿à®± à®•à®Ÿà®™à¯à®•à®¾à®° à®¨à®¾à®¯à¯à®•à®³à¯à®•...       not_hate   \n",
      "2      id_2           à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®©à¯à®©à¯ à®à®¤à®¾à®µà®¤à¯ à®ªà¯à®°à®¿à®¯à¯à®¤à®¾       not_hate   \n",
      "3      id_3      à®ªà®šà¯à®šà¯ˆ à®®à¯Šà®³à®•à®¾ à®•à®¾à®°à®®à¯ vicky à®…à®®à¯à®®à®¾ à®ªà¯à®£à¯à®Ÿà¯ˆ à®¨à®¾à®±à¯à®®à¯ ğŸ˜†           hate   \n",
      "4      id_4    à®à®©à¯à®© à®‰à®Ÿà®®à¯à®ªà¯ à®Ÿà®¾ à®šà®¾à®®à®¿- à®šà¯à®®à¯à®®à®¾ à®µà®³à¯à®µà®³à¯à®©à¯.. à®®à¯à®²à¯ˆ ...           hate   \n",
      "\n",
      "   label_1 binary_label_3  label_3  \n",
      "0        0   not_explicit        0  \n",
      "1        0       explicit        1  \n",
      "2        0   not_explicit        0  \n",
      "3        1       explicit        1  \n",
      "4        1       explicit        1  \n",
      "\n",
      "Label distribution:\n",
      "Label 1 (gendered abuse): {'not_hate': 3834, 'hate': 2945}\n",
      "Label 3 (explicit/aggressive): {'explicit': 4393, 'not_explicit': 2386}\n"
     ]
    }
   ],
   "source": [
    "# Process label 1 dataset for Tamil\n",
    "df_l1 = pd.read_csv('train_ta_l1.csv')\n",
    "df_l1 = df_l1.rename(columns={'key': 'unique_id', 'sentence': 'text'})\n",
    "\n",
    "# Convert annotator columns to numeric without replacing NaNs (only 5 annotators)\n",
    "df_l1[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5']] = df_l1[\n",
    "    ['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5']\n",
    "].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Compute 'label_1' based on majority voting while ignoring NaNs\n",
    "df_l1['label_1'] = (df_l1[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5']].mean(axis=1, skipna=True) >= 0.5).astype(int)\n",
    "\n",
    "# Create proper binary label for task 1 (gendered abuse)\n",
    "df_l1['binary_label_1'] = df_l1['label_1'].apply(lambda x: 'hate' if x == 1 else 'not_hate')\n",
    "\n",
    "# Process label 3 dataset for Tamil\n",
    "df_l3 = pd.read_csv('train_ta_l3.csv')\n",
    "df_l3 = df_l3.rename(columns={'key': 'unique_id', 'sentence': 'text'})\n",
    "\n",
    "# Convert annotator columns to numeric without replacing NaNs (only 5 annotators)\n",
    "df_l3[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5']] = df_l3[\n",
    "    ['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5']\n",
    "].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Compute 'label_3' based on majority voting while ignoring NaNs\n",
    "df_l3['label_3'] = (df_l3[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5']].mean(axis=1, skipna=True) >= 0.5).astype(int)\n",
    "\n",
    "# Create proper binary label for task 3 (explicit/aggressive)\n",
    "df_l3['binary_label_3'] = df_l3['label_3'].apply(lambda x: 'explicit' if x == 1 else 'not_explicit')\n",
    "\n",
    "# Select columns for merging\n",
    "df_l1_slim = df_l1[['text', 'label_1', 'binary_label_1']]\n",
    "df_l3_slim = df_l3[['text', 'label_3', 'binary_label_3']]\n",
    "\n",
    "# Merge the datasets based on text field\n",
    "merged_df = pd.merge(df_l1_slim, df_l3_slim, on='text', how='inner')\n",
    "\n",
    "# Add a unique_id column to the merged dataset\n",
    "merged_df['unique_id'] = [f'id_{i}' for i in range(len(merged_df))]\n",
    "\n",
    "# Reorder columns\n",
    "merged_df = merged_df[['unique_id', 'text', 'binary_label_1', 'label_1', 'binary_label_3', 'label_3']]\n",
    "\n",
    "# Save the merged dataset\n",
    "merged_df.to_csv('train_ta_task3.csv', index=False)\n",
    "\n",
    "# Display information\n",
    "print(f\"Final merged dataset shape: {merged_df.shape}\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# Check label distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(f\"Label 1 (gendered abuse): {merged_df['binary_label_1'].value_counts().to_dict()}\")\n",
    "print(f\"Label 3 (explicit/aggressive): {merged_df['binary_label_3'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdff0933-cd4e-49d3-8f37-b9681b181646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original vs Processed Text Samples:\n",
      "Original:    *1. à®®à¯à®°à®šà¯Šà®²à®¿ à®…à®²à¯à®µà®²à®•à®®à¯ à®…à®®à¯ˆà®¨à¯à®¤à¯à®³à¯à®³ à®‡à®Ÿà®®à¯ à®ªà®à¯à®šà®®à®¿ à®¨à®¿à®²à®®à¯ à®‡à®²à¯à®²à¯ˆ à®à®©à¯à®ªà®¤à¯ˆ à®¨à®¿à®°à¯‚à®ªà®¿à®•à¯à®• 1985-à®†à®®à¯ à®†à®£à¯à®Ÿà¯ à®µà®¾à®™à¯à®•à®ªà¯à®ªà®Ÿà¯à®Ÿ à®ªà®Ÿà¯à®Ÿà®¾à®µà¯ˆ à®†à®¤à®¾à®°à®®à®¾à®•à®•à¯ à®•à®¾à®Ÿà¯à®Ÿà®¿à®¯à®¿à®°à¯à®•à¯à®•à®¿à®±à®¾à®°à¯ à®®à¯.à®•.à®¸à¯à®Ÿà®¾à®²à®¿à®©à¯.  à®‡à®¤à®±à¯à®•à¯ à®•à®¾à®Ÿà¯à®Ÿ à®µà¯‡à®£à¯à®Ÿà®¿à®¯...  \n",
      "Processed:        à®®à¯à®°à®šà¯Šà®²à®¿ à®…à®²à¯à®µà®²à®•à®®à¯ à®…à®®à¯ˆà®¨à¯à®¤à¯à®³à¯à®³ à®‡à®Ÿà®®à¯ à®ªà®à¯à®šà®®à®¿ à®¨à®¿à®²à®®à¯ à®‡à®²à¯à®²à¯ˆ à®à®©à¯à®ªà®¤à¯ˆ à®¨à®¿à®°à¯‚à®ªà®¿à®•à¯à®•   à®†à®®à¯ à®†à®£à¯à®Ÿà¯ à®µà®¾à®™à¯à®•à®ªà¯à®ªà®Ÿà¯à®Ÿ à®ªà®Ÿà¯à®Ÿà®¾à®µà¯ˆ à®†à®¤à®¾à®°à®®à®¾à®•à®•à¯ à®•à®¾à®Ÿà¯à®Ÿà®¿à®¯à®¿à®°à¯à®•à¯à®•à®¿à®±à®¾à®°à¯ à®®à¯ à®• à®¸à¯à®Ÿà®¾à®²à®¿à®©à¯   à®‡à®¤à®±à¯à®•à¯ à®•à®¾à®Ÿà¯à®Ÿ à®µà¯‡à®£à¯à®Ÿà®¿à®¯     \n",
      "--------------------------------------------------\n",
      "Original:    à®šà¯‹à®¤à¯à®¤à¯à®•à¯à®•à¯ à®ªà®¿à®šà¯à®šà¯ˆ à®à®Ÿà¯à®•à¯à®•à®¿à®± à®•à®Ÿà®™à¯à®•à®¾à®° à®¨à®¾à®¯à¯à®•à®³à¯à®•à¯à®•à¯ à®ªà¯‡à®šà¯à®šà¯ à®ªà¯à®£à¯à®Ÿà¯ˆà®¯à¯ˆ  à®ªà®¾à®°à¯..   à®ªà¯‹à®¯à¯ à®šà¯€à®©à®¾ à®•à®¾à®°à®©à¯à®•à¯à®•à¯ à®šà¯‚à®¤à¯à®¤à¯ à®•à¯à®Ÿà¯ à®ªà¯‹\n",
      "Processed:    à®šà¯‹à®¤à¯à®¤à¯à®•à¯à®•à¯ à®ªà®¿à®šà¯à®šà¯ˆ à®à®Ÿà¯à®•à¯à®•à®¿à®± à®•à®Ÿà®™à¯à®•à®¾à®° à®¨à®¾à®¯à¯à®•à®³à¯à®•à¯à®•à¯ à®ªà¯‡à®šà¯à®šà¯ à®ªà¯à®£à¯à®Ÿà¯ˆà®¯à¯ˆ  à®ªà®¾à®°à¯     à®ªà¯‹à®¯à¯ à®šà¯€à®©à®¾ à®•à®¾à®°à®©à¯à®•à¯à®•à¯ à®šà¯‚à®¤à¯à®¤à¯ à®•à¯à®Ÿà¯ à®ªà¯‹\n",
      "--------------------------------------------------\n",
      "Original:    à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®©à¯à®©à¯ à®à®¤à®¾à®µà®¤à¯ à®ªà¯à®°à®¿à®¯à¯à®¤à®¾\n",
      "Processed:    à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®©à¯à®©à¯ à®à®¤à®¾à®µà®¤à¯ à®ªà¯à®°à®¿à®¯à¯à®¤à®¾\n",
      "--------------------------------------------------\n",
      "\n",
      "Processed dataset shape: (6779, 7)\n",
      "  unique_id                                               text  \\\n",
      "0      id_0     *1. à®®à¯à®°à®šà¯Šà®²à®¿ à®…à®²à¯à®µà®²à®•à®®à¯ à®…à®®à¯ˆà®¨à¯à®¤à¯à®³à¯à®³ à®‡à®Ÿà®®à¯ à®ªà®à¯à®šà®®à®¿...   \n",
      "1      id_1     à®šà¯‹à®¤à¯à®¤à¯à®•à¯à®•à¯ à®ªà®¿à®šà¯à®šà¯ˆ à®à®Ÿà¯à®•à¯à®•à®¿à®± à®•à®Ÿà®™à¯à®•à®¾à®° à®¨à®¾à®¯à¯à®•à®³à¯à®•...   \n",
      "2      id_2           à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®©à¯à®©à¯ à®à®¤à®¾à®µà®¤à¯ à®ªà¯à®°à®¿à®¯à¯à®¤à®¾   \n",
      "3      id_3      à®ªà®šà¯à®šà¯ˆ à®®à¯Šà®³à®•à®¾ à®•à®¾à®°à®®à¯ vicky à®…à®®à¯à®®à®¾ à®ªà¯à®£à¯à®Ÿà¯ˆ à®¨à®¾à®±à¯à®®à¯ ğŸ˜†   \n",
      "4      id_4    à®à®©à¯à®© à®‰à®Ÿà®®à¯à®ªà¯ à®Ÿà®¾ à®šà®¾à®®à®¿- à®šà¯à®®à¯à®®à®¾ à®µà®³à¯à®µà®³à¯à®©à¯.. à®®à¯à®²à¯ˆ ...   \n",
      "\n",
      "                                      processed_text binary_label_1  label_1  \\\n",
      "0         à®®à¯à®°à®šà¯Šà®²à®¿ à®…à®²à¯à®µà®²à®•à®®à¯ à®…à®®à¯ˆà®¨à¯à®¤à¯à®³à¯à®³ à®‡à®Ÿà®®à¯ à®ªà®à¯à®šà®®à®¿...       not_hate        0   \n",
      "1     à®šà¯‹à®¤à¯à®¤à¯à®•à¯à®•à¯ à®ªà®¿à®šà¯à®šà¯ˆ à®à®Ÿà¯à®•à¯à®•à®¿à®± à®•à®Ÿà®™à¯à®•à®¾à®° à®¨à®¾à®¯à¯à®•à®³à¯à®•...       not_hate        0   \n",
      "2           à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®¤à®¤à¯à®¤à®ªà¯à®¤à¯à®¤ à®©à¯à®©à¯ à®à®¤à®¾à®µà®¤à¯ à®ªà¯à®°à®¿à®¯à¯à®¤à®¾       not_hate        0   \n",
      "3       à®ªà®šà¯à®šà¯ˆ à®®à¯Šà®³à®•à®¾ à®•à®¾à®°à®®à¯ vicky à®…à®®à¯à®®à®¾ à®ªà¯à®£à¯à®Ÿà¯ˆ à®¨à®¾à®±à¯à®®à¯            hate        1   \n",
      "4    à®à®©à¯à®© à®‰à®Ÿà®®à¯à®ªà¯ à®Ÿà®¾ à®šà®¾à®®à®¿  à®šà¯à®®à¯à®®à®¾ à®µà®³à¯à®µà®³à¯à®©à¯   à®®à¯à®²à¯ˆ ...           hate        1   \n",
      "\n",
      "  binary_label_3  label_3  \n",
      "0   not_explicit        0  \n",
      "1       explicit        1  \n",
      "2   not_explicit        0  \n",
      "3       explicit        1  \n",
      "4       explicit        1  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"\n",
    "                               u\"\\U0001F700-\\U0001F77F\"\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', ' ', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'<.*?>+', ' ', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', ' ', text)\n",
    "    text = re.sub(r'<handle replaced>', '', text)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "# Load the multi-task dataset\n",
    "merged_df = pd.read_csv('train_ta_task3.csv')\n",
    "\n",
    "# Apply text normalization\n",
    "merged_df['processed_text'] = merged_df['text'].apply(lambda x: normalize_text(x))\n",
    "\n",
    "# Further processing to remove '...'\n",
    "merged_df['processed_text'] = merged_df['processed_text'].str.replace('...', '')\n",
    "\n",
    "# Display samples of processed text\n",
    "print(\"\\nOriginal vs Processed Text Samples:\")\n",
    "for i in range(3):\n",
    "    print(f\"Original: {merged_df['text'].iloc[i]}\")\n",
    "    print(f\"Processed: {merged_df['processed_text'].iloc[i]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Keep all columns but add processed text\n",
    "merged_df_final = merged_df[['unique_id', 'text', 'processed_text', 'binary_label_1', 'label_1', 'binary_label_3', 'label_3']]\n",
    "\n",
    "\n",
    "print(f\"\\nProcessed dataset shape: {merged_df_final.shape}\")\n",
    "print(merged_df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae6eb7f-b57a-4dfb-9a93-cb7057f1bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (processed text)\n",
    "X = list(merged_df_final['processed_text'])\n",
    "\n",
    "# Extract labels for both tasks\n",
    "y_task1 = merged_df_final['label_1'].values\n",
    "y_task3 = merged_df_final['label_3'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a63c71-3782-4532-9363-c8a6fdc7c50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1557 2466  444 ...    0    0    0]\n",
      " [1396  323 2468 ...    0    0    0]\n",
      " [ 135  520 2470 ...    0    0    0]\n",
      " ...\n",
      " [3879 2142 1100 ...    0    0    0]\n",
      " [ 406  430  602 ...    0    0    0]\n",
      " [   4  850   13 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Activation, Dropout, Dense, Flatten,\n",
    "    Bidirectional, GRU, concatenate, SpatialDropout1D,\n",
    "    GlobalMaxPooling1D, GlobalAveragePooling1D, Conv1D,\n",
    "    Embedding, Input, Concatenate\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "######## Textual Features for Embedding ###################\n",
    "max_len = 100\n",
    "max_features = 4479\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding\n",
    "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "\n",
    "print(X)  # Check the processed sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49defb62-78c4-42b5-968d-d3b96ee08bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_task1 = label_encoder.fit_transform(y_task1)\n",
    "y_task3 = label_encoder.fit_transform(y_task3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8556a66b-55dc-4380-9efd-32a18b1186d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_task1 = to_categorical(y_task1, num_classes=2)\n",
    "y_task3 = to_categorical(y_task3, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fe60a2c-8a6d-4e87-a617-6f96da90f061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (4479, 50)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load GloVe embeddings from JSON\n",
    "with open('glove_embeddings.json', encoding=\"utf8\") as f:\n",
    "    embeddings_list = json.load(f)\n",
    "\n",
    "# Convert the list of vectors to a dictionary with word indices as keys\n",
    "embeddings_dictionary = {str(i): vector for i, vector in enumerate(embeddings_list)}\n",
    "\n",
    "# Define tokenizer \n",
    "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_features, vocab_size)  # Limit vocab to max_features\n",
    "\n",
    "# Get embedding dimension (from first vector in list)\n",
    "embed_size = len(embeddings_list[0]) if embeddings_list else 0\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "\n",
    "# Fill embedding matrix with corresponding word vectors\n",
    "for word, index in word_index.items():\n",
    "    if index >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_dictionary.get(word) or embeddings_dictionary.get(str(index))\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = np.asarray(embedding_vector, dtype=np.float32)\n",
    "\n",
    "print(\"Embedding matrix shape:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2758cfe8-837c-4935-969c-57baba5671d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5423\n",
      "Validation samples: 1356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krmri\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)           â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">223,950</span> â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ spatial_dropout1d             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)           â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)            â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">138,240</span> â”‚ spatial_dropout1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_1               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">123,648</span> â”‚ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)               â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_head_attention          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> â”‚ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)          â”‚                           â”‚                 â”‚ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ multi_head_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_max_pooling1d          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ global_average_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ global_max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task1_dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> â”‚ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task3_dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> â”‚ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task1_bn1                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚ task1_dense1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task3_bn1                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚ task3_dense1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ task1_bn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ task3_bn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task1_dense2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task3_dense2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task1_bn2                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚ task1_dense2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task3_bn2                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚ task3_dense2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ task1_bn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ task3_bn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task1_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                 â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> â”‚ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task3_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                 â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> â”‚ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m50\u001b[0m)           â”‚         \u001b[38;5;34m223,950\u001b[0m â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ spatial_dropout1d             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m50\u001b[0m)           â”‚               \u001b[38;5;34m0\u001b[0m â”‚ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”‚ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)            â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚         \u001b[38;5;34m138,240\u001b[0m â”‚ spatial_dropout1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_1               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)          â”‚         \u001b[38;5;34m123,648\u001b[0m â”‚ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”‚ (\u001b[38;5;33mBidirectional\u001b[0m)               â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_head_attention          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)          â”‚          \u001b[38;5;34m66,048\u001b[0m â”‚ bidirectional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)          â”‚                           â”‚                 â”‚ bidirectional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate (\u001b[38;5;33mConcatenate\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚               \u001b[38;5;34m0\u001b[0m â”‚ bidirectional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ multi_head_attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_max_pooling1d          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate_1 (\u001b[38;5;33mConcatenate\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ global_average_pooling1d[\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ global_max_pooling1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task1_dense1 (\u001b[38;5;33mDense\u001b[0m)          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               â”‚         \u001b[38;5;34m131,328\u001b[0m â”‚ concatenate_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task3_dense1 (\u001b[38;5;33mDense\u001b[0m)          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               â”‚         \u001b[38;5;34m131,328\u001b[0m â”‚ concatenate_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task1_bn1                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               â”‚           \u001b[38;5;34m1,024\u001b[0m â”‚ task1_dense1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task3_bn1                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               â”‚           \u001b[38;5;34m1,024\u001b[0m â”‚ task3_dense1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ task1_bn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ task3_bn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task1_dense2 (\u001b[38;5;33mDense\u001b[0m)          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               â”‚          \u001b[38;5;34m32,896\u001b[0m â”‚ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task3_dense2 (\u001b[38;5;33mDense\u001b[0m)          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               â”‚          \u001b[38;5;34m32,896\u001b[0m â”‚ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task1_bn2                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               â”‚             \u001b[38;5;34m512\u001b[0m â”‚ task1_dense2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task3_bn2                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               â”‚             \u001b[38;5;34m512\u001b[0m â”‚ task3_dense2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ task1_bn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ task3_bn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task1_output (\u001b[38;5;33mDense\u001b[0m)          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                 â”‚             \u001b[38;5;34m258\u001b[0m â”‚ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ task3_output (\u001b[38;5;33mDense\u001b[0m)          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                 â”‚             \u001b[38;5;34m258\u001b[0m â”‚ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">883,922</span> (3.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m883,922\u001b[0m (3.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">882,386</span> (3.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m882,386\u001b[0m (3.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - loss: 1.8833 - task1_output_accuracy: 0.5210 - task1_output_loss: 0.9654 - task1_output_macro_f1_score: 0.5210 - task3_output_accuracy: 0.5368 - task3_output_loss: 0.9161 - task3_output_macro_f1_score: 0.5368 \n",
      "Epoch 1: val_loss improved from inf to 1.31818, saving model to models_ta_multi_task_gru\\best_model_ta_multi_task_gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 228ms/step - loss: 1.8824 - task1_output_accuracy: 0.5209 - task1_output_loss: 0.9650 - task1_output_macro_f1_score: 0.5209 - task3_output_accuracy: 0.5370 - task3_output_loss: 0.9156 - task3_output_macro_f1_score: 0.5370 - val_loss: 1.3182 - val_task1_output_accuracy: 0.5656 - val_task1_output_loss: 0.6817 - val_task1_output_macro_f1_score: 0.5656 - val_task3_output_accuracy: 0.6689 - val_task3_output_loss: 0.6344 - val_task3_output_macro_f1_score: 0.6689\n",
      "Epoch 2/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 1.4897 - task1_output_accuracy: 0.5494 - task1_output_loss: 0.7586 - task1_output_macro_f1_score: 0.5494 - task3_output_accuracy: 0.5948 - task3_output_loss: 0.7292 - task3_output_macro_f1_score: 0.5948 \n",
      "Epoch 2: val_loss improved from 1.31818 to 1.25145, saving model to models_ta_multi_task_gru\\best_model_ta_multi_task_gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 212ms/step - loss: 1.4894 - task1_output_accuracy: 0.5494 - task1_output_loss: 0.7586 - task1_output_macro_f1_score: 0.5494 - task3_output_accuracy: 0.5949 - task3_output_loss: 0.7291 - task3_output_macro_f1_score: 0.5949 - val_loss: 1.2515 - val_task1_output_accuracy: 0.6173 - val_task1_output_loss: 0.6609 - val_task1_output_macro_f1_score: 0.6173 - val_task3_output_accuracy: 0.6844 - val_task3_output_loss: 0.5893 - val_task3_output_macro_f1_score: 0.6844\n",
      "Epoch 3/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - loss: 1.2081 - task1_output_accuracy: 0.6477 - task1_output_loss: 0.6447 - task1_output_macro_f1_score: 0.6477 - task3_output_accuracy: 0.7190 - task3_output_loss: 0.5616 - task3_output_macro_f1_score: 0.7190 \n",
      "Epoch 3: val_loss improved from 1.25145 to 1.00369, saving model to models_ta_multi_task_gru\\best_model_ta_multi_task_gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 286ms/step - loss: 1.2076 - task1_output_accuracy: 0.6479 - task1_output_loss: 0.6445 - task1_output_macro_f1_score: 0.6479 - task3_output_accuracy: 0.7192 - task3_output_loss: 0.5613 - task3_output_macro_f1_score: 0.7192 - val_loss: 1.0037 - val_task1_output_accuracy: 0.7242 - val_task1_output_loss: 0.5570 - val_task1_output_macro_f1_score: 0.7242 - val_task3_output_accuracy: 0.7957 - val_task3_output_loss: 0.4430 - val_task3_output_macro_f1_score: 0.7957\n",
      "Epoch 4/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297ms/step - loss: 0.9049 - task1_output_accuracy: 0.7395 - task1_output_loss: 0.5149 - task1_output_macro_f1_score: 0.7395 - task3_output_accuracy: 0.8463 - task3_output_loss: 0.3882 - task3_output_macro_f1_score: 0.8463 \n",
      "Epoch 4: val_loss improved from 1.00369 to 0.92152, saving model to models_ta_multi_task_gru\\best_model_ta_multi_task_gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 317ms/step - loss: 0.9049 - task1_output_accuracy: 0.7396 - task1_output_loss: 0.5149 - task1_output_macro_f1_score: 0.7396 - task3_output_accuracy: 0.8463 - task3_output_loss: 0.3883 - task3_output_macro_f1_score: 0.8463 - val_loss: 0.9215 - val_task1_output_accuracy: 0.7448 - val_task1_output_loss: 0.5239 - val_task1_output_macro_f1_score: 0.7448 - val_task3_output_accuracy: 0.8355 - val_task3_output_loss: 0.3947 - val_task3_output_macro_f1_score: 0.8355\n",
      "Epoch 5/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step - loss: 0.8094 - task1_output_accuracy: 0.7761 - task1_output_loss: 0.4648 - task1_output_macro_f1_score: 0.7761 - task3_output_accuracy: 0.8723 - task3_output_loss: 0.3427 - task3_output_macro_f1_score: 0.8723 \n",
      "Epoch 5: val_loss did not improve from 0.92152\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 332ms/step - loss: 0.8093 - task1_output_accuracy: 0.7761 - task1_output_loss: 0.4648 - task1_output_macro_f1_score: 0.7761 - task3_output_accuracy: 0.8723 - task3_output_loss: 0.3427 - task3_output_macro_f1_score: 0.8723 - val_loss: 1.0286 - val_task1_output_accuracy: 0.7412 - val_task1_output_loss: 0.5569 - val_task1_output_macro_f1_score: 0.7412 - val_task3_output_accuracy: 0.8333 - val_task3_output_loss: 0.4682 - val_task3_output_macro_f1_score: 0.8333\n",
      "Epoch 6/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - loss: 0.7554 - task1_output_accuracy: 0.7877 - task1_output_loss: 0.4467 - task1_output_macro_f1_score: 0.7877 - task3_output_accuracy: 0.8858 - task3_output_loss: 0.3069 - task3_output_macro_f1_score: 0.8858 \n",
      "Epoch 6: val_loss improved from 0.92152 to 0.88302, saving model to models_ta_multi_task_gru\\best_model_ta_multi_task_gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - loss: 0.7554 - task1_output_accuracy: 0.7877 - task1_output_loss: 0.4467 - task1_output_macro_f1_score: 0.7877 - task3_output_accuracy: 0.8858 - task3_output_loss: 0.3069 - task3_output_macro_f1_score: 0.8858 - val_loss: 0.8830 - val_task1_output_accuracy: 0.7581 - val_task1_output_loss: 0.5004 - val_task1_output_macro_f1_score: 0.7581 - val_task3_output_accuracy: 0.8466 - val_task3_output_loss: 0.3799 - val_task3_output_macro_f1_score: 0.8466\n",
      "Epoch 7/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330ms/step - loss: 0.7046 - task1_output_accuracy: 0.8056 - task1_output_loss: 0.4263 - task1_output_macro_f1_score: 0.8056 - task3_output_accuracy: 0.8976 - task3_output_loss: 0.2764 - task3_output_macro_f1_score: 0.8976 \n",
      "Epoch 7: val_loss did not improve from 0.88302\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 348ms/step - loss: 0.7045 - task1_output_accuracy: 0.8056 - task1_output_loss: 0.4263 - task1_output_macro_f1_score: 0.8056 - task3_output_accuracy: 0.8976 - task3_output_loss: 0.2764 - task3_output_macro_f1_score: 0.8976 - val_loss: 0.9752 - val_task1_output_accuracy: 0.7478 - val_task1_output_loss: 0.5149 - val_task1_output_macro_f1_score: 0.7478 - val_task3_output_accuracy: 0.8400 - val_task3_output_loss: 0.4541 - val_task3_output_macro_f1_score: 0.8400\n",
      "Epoch 8/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - loss: 0.6756 - task1_output_accuracy: 0.8168 - task1_output_loss: 0.4118 - task1_output_macro_f1_score: 0.8168 - task3_output_accuracy: 0.9027 - task3_output_loss: 0.2619 - task3_output_macro_f1_score: 0.9027 \n",
      "Epoch 8: val_loss did not improve from 0.88302\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 343ms/step - loss: 0.6755 - task1_output_accuracy: 0.8168 - task1_output_loss: 0.4117 - task1_output_macro_f1_score: 0.8168 - task3_output_accuracy: 0.9027 - task3_output_loss: 0.2619 - task3_output_macro_f1_score: 0.9027 - val_loss: 0.9604 - val_task1_output_accuracy: 0.7566 - val_task1_output_loss: 0.5282 - val_task1_output_macro_f1_score: 0.7566 - val_task3_output_accuracy: 0.8451 - val_task3_output_loss: 0.4282 - val_task3_output_macro_f1_score: 0.8451\n",
      "Epoch 9/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step - loss: 0.6302 - task1_output_accuracy: 0.8196 - task1_output_loss: 0.3926 - task1_output_macro_f1_score: 0.8196 - task3_output_accuracy: 0.9124 - task3_output_loss: 0.2357 - task3_output_macro_f1_score: 0.9124 \n",
      "Epoch 9: val_loss did not improve from 0.88302\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 344ms/step - loss: 0.6302 - task1_output_accuracy: 0.8196 - task1_output_loss: 0.3925 - task1_output_macro_f1_score: 0.8196 - task3_output_accuracy: 0.9124 - task3_output_loss: 0.2357 - task3_output_macro_f1_score: 0.9124 - val_loss: 1.0394 - val_task1_output_accuracy: 0.7485 - val_task1_output_loss: 0.5580 - val_task1_output_macro_f1_score: 0.7485 - val_task3_output_accuracy: 0.8252 - val_task3_output_loss: 0.4782 - val_task3_output_macro_f1_score: 0.8252\n",
      "Epoch 9: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43/43\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 143ms/step\n",
      "\n",
      "Task 1 (Gendered Abuse) Validation Results:\n",
      "Precision: 0.7822\n",
      "Recall: 0.7581\n",
      "weighted F1 Score: 0.7583\n",
      "macro F1 Score: 0.7581\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not_hate       0.87      0.68      0.76       767\n",
      "        hate       0.67      0.86      0.76       589\n",
      "\n",
      "    accuracy                           0.76      1356\n",
      "   macro avg       0.77      0.77      0.76      1356\n",
      "weighted avg       0.78      0.76      0.76      1356\n",
      "\n",
      "\n",
      "Task 3 (Explicit Language) Validation Results:\n",
      "Precision: 0.8453\n",
      "Recall: 0.8466\n",
      "weighted F1 Score: 0.8458\n",
      "macro F1 Score: 0.8250\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "not_explicit       0.78      0.75      0.76       449\n",
      "    explicit       0.88      0.90      0.89       907\n",
      "\n",
      "    accuracy                           0.85      1356\n",
      "   macro avg       0.83      0.82      0.83      1356\n",
      "weighted avg       0.85      0.85      0.85      1356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, SpatialDropout1D, Conv1D,\n",
    "    Bidirectional, LSTM, GRU, Dense, Dropout,\n",
    "    GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Configure GPU for optimal performance\n",
    "def configure_gpu():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Enable memory growth for each GPU\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "            # Use mixed precision for better performance\n",
    "            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "            tf.keras.mixed_precision.set_global_policy(policy)\n",
    "            print('Mixed precision enabled')\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "configure_gpu()\n",
    "\n",
    "# Multi-Task Model Definition with GRU and Attention\n",
    "def create_multi_task_gru_attention_model(max_len, max_features, embedding_matrix, embed_size=300):\n",
    "    \"\"\"\n",
    "    Creates a multi-task GRU model with hierarchical attention mechanism for joint prediction of\n",
    "    gendered abuse (task1) and explicit language (task3)\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(max_len,))\n",
    "    \n",
    "    # Embedding layer with pretrained weights\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=max_features,\n",
    "        output_dim=embed_size,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_len,\n",
    "        trainable=True  # Make embeddings trainable for fine-tuning\n",
    "    )(input_layer)\n",
    "    \n",
    "    # Spatial Dropout with higher rate\n",
    "    spatial_dropout = SpatialDropout1D(0.3)(embedding_layer)\n",
    "    \n",
    "    # Multiple GRU layers with different window sizes\n",
    "    gru_layer1 = Bidirectional(\n",
    "        GRU(\n",
    "            units=128,\n",
    "            return_sequences=True,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(1e-5)\n",
    "        )\n",
    "    )(spatial_dropout)\n",
    "    \n",
    "    gru_layer2 = Bidirectional(\n",
    "        GRU(\n",
    "            units=64,\n",
    "            return_sequences=True,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2\n",
    "        )\n",
    "    )(gru_layer1)\n",
    "    \n",
    "    # Multi-head self-attention (simplified version)\n",
    "    attention_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=8,\n",
    "        key_dim=16\n",
    "    )(gru_layer2, gru_layer2)\n",
    "    \n",
    "    # Skip connection\n",
    "    concat_layer = tf.keras.layers.Concatenate()([gru_layer2, attention_layer])\n",
    "    \n",
    "    # Feature extraction with pooling operations\n",
    "    avg_pool = GlobalAveragePooling1D()(concat_layer)\n",
    "    max_pool = GlobalMaxPooling1D()(concat_layer)\n",
    "    \n",
    "    # Combine pooled features\n",
    "    shared_features = tf.keras.layers.Concatenate()([avg_pool, max_pool])\n",
    "    \n",
    "    # Task-specific layers for Task 1 (Gendered Abuse)\n",
    "    task1_dense1 = Dense(256, activation='relu', name='task1_dense1')(shared_features)\n",
    "    task1_bn1 = tf.keras.layers.BatchNormalization(name='task1_bn1')(task1_dense1)\n",
    "    task1_dropout1 = Dropout(0.3)(task1_bn1)\n",
    "    \n",
    "    task1_dense2 = Dense(128, activation='relu', name='task1_dense2')(task1_dropout1)\n",
    "    task1_bn2 = tf.keras.layers.BatchNormalization(name='task1_bn2')(task1_dense2)\n",
    "    task1_dropout2 = Dropout(0.2)(task1_bn2)\n",
    "    \n",
    "    task1_output = Dense(2, activation='softmax', name='task1_output', dtype='float32')(task1_dropout2)\n",
    "    \n",
    "    # Task-specific layers for Task 3 (Explicit Language)\n",
    "    task3_dense1 = Dense(256, activation='relu', name='task3_dense1')(shared_features)\n",
    "    task3_bn1 = tf.keras.layers.BatchNormalization(name='task3_bn1')(task3_dense1)\n",
    "    task3_dropout1 = Dropout(0.3)(task3_bn1)\n",
    "    \n",
    "    task3_dense2 = Dense(128, activation='relu', name='task3_dense2')(task3_dropout1)\n",
    "    task3_bn2 = tf.keras.layers.BatchNormalization(name='task3_bn2')(task3_dense2)\n",
    "    task3_dropout2 = Dropout(0.2)(task3_bn2)\n",
    "    \n",
    "    task3_output = Dense(2, activation='softmax', name='task3_output', dtype='float32')(task3_dropout2)\n",
    "    \n",
    "    # Create model with multiple outputs\n",
    "    model = Model(inputs=input_layer, outputs=[task1_output, task3_output])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Custom MacroF1Score Metric for multi-task learning\n",
    "class MacroF1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes=2, name='macro_f1_score', **kwargs):\n",
    "        super(MacroF1Score, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Convert probabilities to predicted class indices\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        \n",
    "        # Convert one-hot encoded y_true to class indices if needed\n",
    "        if len(y_true.shape) > 1 and y_true.shape[-1] > 1:\n",
    "            y_true = tf.argmax(y_true, axis=-1)\n",
    "        \n",
    "        # Initialize confusion matrix\n",
    "        conf_matrix = tf.math.confusion_matrix(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            num_classes=self.num_classes,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        # Calculate TP, FP, FN for each class\n",
    "        diag = tf.linalg.diag_part(conf_matrix)\n",
    "        row_sum = tf.reduce_sum(conf_matrix, axis=1)\n",
    "        col_sum = tf.reduce_sum(conf_matrix, axis=0)\n",
    "        \n",
    "        tp = diag\n",
    "        fp = col_sum - diag\n",
    "        fn = row_sum - diag\n",
    "        \n",
    "        # Update the state variables\n",
    "        self.tp.assign_add(tf.reduce_sum(tp))\n",
    "        self.fp.assign_add(tf.reduce_sum(fp))\n",
    "        self.fn.assign_add(tf.reduce_sum(fn))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        # Calculate precision and recall\n",
    "        precision = self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())\n",
    "        recall = self.tp / (self.tp + self.fn + tf.keras.backend.epsilon())\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "        \n",
    "        # Return macro F1 (average of per-class F1 scores)\n",
    "        return f1\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.tp.assign(0.)\n",
    "        self.fp.assign(0.)\n",
    "        self.fn.assign(0.)\n",
    "        self.count.assign(0.)\n",
    "\n",
    "# Model Training for multi-task learning\n",
    "def train_and_validate_multi_task_model(model, X_train, y_train_task1, y_train_task3, \n",
    "                                         X_val, y_val_task1, y_val_task3, \n",
    "                                         batch_size=32, epochs=15, model_dir='models_ta_multi_task_gru'):\n",
    "    \"\"\"\n",
    "    Trains the multi-task GRU-Attention model with early stopping and model checkpointing\n",
    "    Returns the best model and training history\n",
    "    \"\"\"\n",
    "    # Create directory for saving models if it doesn't exist\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        os.path.join(model_dir, 'best_model_ta_multi_task_gru.h5'),\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Compile model with Adam optimizer\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss={\n",
    "            'task1_output': 'categorical_crossentropy',\n",
    "            'task3_output': 'categorical_crossentropy'\n",
    "        },\n",
    "        loss_weights={\n",
    "            'task1_output': 1.0,  # Weight for gendered abuse task\n",
    "            'task3_output': 1.0   # Weight for explicit language task\n",
    "        },\n",
    "        metrics={\n",
    "            'task1_output': ['accuracy', MacroF1Score(num_classes=2)],\n",
    "            'task3_output': ['accuracy', MacroF1Score(num_classes=2)]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, \n",
    "        {'task1_output': y_train_task1, 'task3_output': y_train_task3},\n",
    "        validation_data=(\n",
    "            X_val, \n",
    "            {'task1_output': y_val_task1, 'task3_output': y_val_task3}\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping, model_checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Load the best model found during training\n",
    "    best_model = load_model(\n",
    "        os.path.join(model_dir, 'best_model_ta_multi_task_gru.h5'), \n",
    "        custom_objects={'MacroF1Score': MacroF1Score}\n",
    "    )\n",
    "    \n",
    "    return history, best_model\n",
    "\n",
    "# Plot Training History for multi-task model\n",
    "def plot_multi_task_training_history(history, plot_dir='plots_ta_multi_task_gru'):\n",
    "    \"\"\"\n",
    "    Plots training history for both tasks (accuracy and loss curves)\n",
    "    Saves plots to specified directory\n",
    "    \"\"\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    # Task 1 (Gendered Abuse) plots\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot task1 accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history.history['task1_output_accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_task1_output_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Task 1 (Gendered Abuse) Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot task1 loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history.history['task1_output_loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_task1_output_loss'], label='Validation Loss')\n",
    "    plt.title('Task 1 (Gendered Abuse) Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Task 3 (Explicit Language) plots\n",
    "    # Plot task3 accuracy\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history.history['task3_output_accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_task3_output_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Task 3 (Explicit Language) Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot task3 loss\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(history.history['task3_output_loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_task3_output_loss'], label='Validation Loss')\n",
    "    plt.title('Task 3 (Explicit Language) Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, 'training_history_ta_multi_task_gru.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot combined loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Total Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Total Validation Loss')\n",
    "    plt.title('Multi-Task Model Combined Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, 'combined_loss_ta_multi_task_gru.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot macro F1 scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['task1_output_macro_f1_score'], label='Task1 Train F1')\n",
    "    plt.plot(history.history['val_task1_output_macro_f1_score'], label='Task1 Val F1')\n",
    "    plt.plot(history.history['task3_output_macro_f1_score'], label='Task3 Train F1')\n",
    "    plt.plot(history.history['val_task3_output_macro_f1_score'], label='Task3 Val F1')\n",
    "    plt.title('Multi-Task Model Macro F1 Scores')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, 'f1_scores_ta_multi_task_gru.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Evaluation function for multi-task model\n",
    "def evaluate_multi_task_validation(model, X_val, y_val_task1, y_val_task3, plot_dir='plots_ta_multi_task_gru'):\n",
    "    \"\"\"\n",
    "    Evaluates the multi-task model on validation data and saves metrics and plots\n",
    "    \"\"\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_pred_task1, y_pred_task3 = model.predict(X_val, batch_size=32)\n",
    "    \n",
    "    # Convert to class labels\n",
    "    y_pred_task1_labels = np.argmax(y_pred_task1, axis=1)\n",
    "    y_true_task1_labels = np.argmax(y_val_task1, axis=1)\n",
    "    \n",
    "    y_pred_task3_labels = np.argmax(y_pred_task3, axis=1)\n",
    "    y_true_task3_labels = np.argmax(y_val_task3, axis=1)\n",
    "    \n",
    "    # Task 1 (Gendered Abuse) metrics\n",
    "    task1_precision = precision_score(y_true_task1_labels, y_pred_task1_labels, average='weighted')\n",
    "    task1_recall = recall_score(y_true_task1_labels, y_pred_task1_labels, average='weighted')\n",
    "    task1_weighted_f1 = f1_score(y_true_task1_labels, y_pred_task1_labels, average='weighted')\n",
    "    task1_macro_f1 = f1_score(y_true_task1_labels, y_pred_task1_labels, average='macro')\n",
    "    \n",
    "    # Task 3 (Explicit Language) metrics\n",
    "    task3_precision = precision_score(y_true_task3_labels, y_pred_task3_labels, average='weighted')\n",
    "    task3_recall = recall_score(y_true_task3_labels, y_pred_task3_labels, average='weighted')\n",
    "    task3_weighted_f1 = f1_score(y_true_task3_labels, y_pred_task3_labels, average='weighted')\n",
    "    task3_macro_f1 = f1_score(y_true_task3_labels, y_pred_task3_labels, average='macro')\n",
    "    \n",
    "    # Classification reports\n",
    "    task1_report = classification_report(y_true_task1_labels, y_pred_task1_labels, \n",
    "                                        target_names=['not_hate', 'hate'])\n",
    "    \n",
    "    task3_report = classification_report(y_true_task3_labels, y_pred_task3_labels, \n",
    "                                        target_names=['not_explicit', 'explicit'])\n",
    "    \n",
    "    # Confusion matrices\n",
    "    task1_conf_matrix = confusion_matrix(y_true_task1_labels, y_pred_task1_labels)\n",
    "    task3_conf_matrix = confusion_matrix(y_true_task3_labels, y_pred_task3_labels)\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    # Task 1 confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(task1_conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Not Hate', 'Hate'],\n",
    "                yticklabels=['Not Hate', 'Hate'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Task 1 (Gendered Abuse) Confusion Matrix')\n",
    "    plt.savefig(os.path.join(plot_dir, 'task1_confusion_matrix_ta_multi_task_gru.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Task 3 confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(task3_conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Not Explicit', 'Explicit'],\n",
    "                yticklabels=['Not Explicit', 'Explicit'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Task 3 (Explicit Language) Confusion Matrix')\n",
    "    plt.savefig(os.path.join(plot_dir, 'task3_confusion_matrix_ta_multi_task_gru.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'task1': {\n",
    "            'precision': task1_precision,\n",
    "            'recall': task1_recall,\n",
    "            'f1_score_weighted': task1_weighted_f1,\n",
    "            'f1_score_macro': task1_macro_f1,\n",
    "            'classification_report': task1_report,\n",
    "            'confusion_matrix': task1_conf_matrix\n",
    "        },\n",
    "        'task3': {\n",
    "            'precision': task3_precision,\n",
    "            'recall': task3_recall,\n",
    "            'f1_score_weighted': task3_weighted_f1,\n",
    "            'f1_score_macro': task3_macro_f1,\n",
    "            'classification_report': task3_report,\n",
    "            'confusion_matrix': task3_conf_matrix\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Main Execution for Training and Validation\n",
    "if __name__ == \"__main__\":\n",
    "    # Split into train (80%) and validation (20%)\n",
    "    X_train, X_val, y_train_task1, y_val_task1, y_train_task3, y_val_task3 = train_test_split(\n",
    "        X, y_task1, y_task3, test_size=0.2, random_state=42, stratify=y_task1\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Validation samples: {len(X_val)}\")\n",
    "    \n",
    "    # Create multi-task GRU-Attention model\n",
    "    embed_size = embedding_matrix.shape[1]\n",
    "    model = create_multi_task_gru_attention_model(max_len, max_features, embedding_matrix, embed_size)\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Train multi-task model\n",
    "    history, trained_model = train_and_validate_multi_task_model(\n",
    "        model, X_train, y_train_task1, y_train_task3, \n",
    "        X_val, y_val_task1, y_val_task3,\n",
    "        batch_size=32,\n",
    "        epochs=15  \n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_multi_task_training_history(history)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_results = evaluate_multi_task_validation(\n",
    "        trained_model, X_val, y_val_task1, y_val_task3\n",
    "    )\n",
    "    \n",
    "    # Print Task 1 (Gendered Abuse) results\n",
    "    print(\"\\nTask 1 (Gendered Abuse) Validation Results:\")\n",
    "    print(f\"Precision: {val_results['task1']['precision']:.4f}\")\n",
    "    print(f\"Recall: {val_results['task1']['recall']:.4f}\")\n",
    "    print(f\"weighted F1 Score: {val_results['task1']['f1_score_weighted']:.4f}\")\n",
    "    print(f\"macro F1 Score: {val_results['task1']['f1_score_macro']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(val_results['task1']['classification_report'])\n",
    "    \n",
    "    # Print Task 3 (Explicit Language) results\n",
    "    print(\"\\nTask 3 (Explicit Language) Validation Results:\")\n",
    "    print(f\"Precision: {val_results['task3']['precision']:.4f}\")\n",
    "    print(f\"Recall: {val_results['task3']['recall']:.4f}\")\n",
    "    print(f\"weighted F1 Score: {val_results['task3']['f1_score_weighted']:.4f}\")\n",
    "    print(f\"macro F1 Score: {val_results['task3']['f1_score_macro']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(val_results['task3']['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "256c9c72-1909-45b0-bbbd-b3d949d6394c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged dataset shape: (1135, 6)\n",
      "  unique_id                                               text binary_label_1  \\\n",
      "0      id_0     à®µà¯ˆà®°à®®à¯à®¤à¯à®¤à¯ à®’à®°à¯ à®•à®¾à®® à®®à®¿à®°à¯à®•à®®à¯ à®à®©à¯à®ªà®¤à¯ à®šà®¿à®©à®¿à®®à®¾ à®¤à¯à®±...       not_hate   \n",
      "1      id_1  #4YrsOfValiantVIVEGAM  #Valimai #AjithKumar   ...       not_hate   \n",
      "2      id_2  #AmbedkarBlueShirtRally  à®‡à®¨à¯à®¤ à®ªà¯‹à®°à®¾à®Ÿà¯à®Ÿà®¤à¯à®¤à¯à®•à¯à®•à¯ ...       not_hate   \n",
      "3      id_3  #BREAKING | à®¤à®¿à®°à¯à®šà¯à®šà®¿ à®®à®¾à®µà®Ÿà¯à®Ÿà®®à¯  à®®à®£à®ªà¯à®ªà®¾à®±à¯ˆà®¯à¯ˆ à®…à®Ÿà¯à®¤...       not_hate   \n",
      "4      id_4  #Bachelor ğŸ˜¤ğŸ˜¤ğŸ˜¤ğŸ˜¤ğŸ˜¤à®ªà®Ÿà®®à®¾à®Ÿà®¾ à®‡à®¤à¯ à®•à¯‹à®¤à¯à®¤à®¾ <handle repla...       not_hate   \n",
      "\n",
      "   label_1 binary_label_3  label_3  \n",
      "0        0   not_explicit        0  \n",
      "1        0       explicit        1  \n",
      "2        0       explicit        1  \n",
      "3        0   not_explicit        0  \n",
      "4        0       explicit        1  \n",
      "\n",
      "Label distribution:\n",
      "Label 1 (gendered abuse): {'hate': 587, 'not_hate': 548}\n",
      "Label 3 (explicit/aggressive): {'explicit': 776, 'not_explicit': 359}\n"
     ]
    }
   ],
   "source": [
    "# Process label 1 dataset for Tamil\n",
    "df_l1 = pd.read_csv('test_ta_l1.csv')\n",
    "df_l1 = df_l1.rename(columns={'key': 'unique_id', 'sentence': 'text'})\n",
    "\n",
    "# Convert annotator columns to numeric without replacing NaNs (only 5 annotators)\n",
    "df_l1[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5']] = df_l1[\n",
    "    ['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5']\n",
    "].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Compute 'label_1' based on majority voting while ignoring NaNs\n",
    "df_l1['label_1'] = (df_l1[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5']].mean(axis=1, skipna=True) >= 0.5).astype(int)\n",
    "\n",
    "# Create proper binary label for task 1 (gendered abuse)\n",
    "df_l1['binary_label_1'] = df_l1['label_1'].apply(lambda x: 'hate' if x == 1 else 'not_hate')\n",
    "\n",
    "# Process label 3 dataset for Tamil\n",
    "df_l3 = pd.read_csv('test_ta_l3.csv')\n",
    "df_l3 = df_l3.rename(columns={'key': 'unique_id', 'sentence': 'text'})\n",
    "\n",
    "# Convert annotator columns to numeric without replacing NaNs (only 5 annotators)\n",
    "df_l3[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5']] = df_l3[\n",
    "    ['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5']\n",
    "].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Compute 'label_3' based on majority voting while ignoring NaNs\n",
    "df_l3['label_3'] = (df_l3[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5']].mean(axis=1, skipna=True) >= 0.5).astype(int)\n",
    "\n",
    "# Create proper binary label for task 3 (explicit/aggressive)\n",
    "df_l3['binary_label_3'] = df_l3['label_3'].apply(lambda x: 'explicit' if x == 1 else 'not_explicit')\n",
    "\n",
    "# Select columns for merging\n",
    "df_l1_slim = df_l1[['text', 'label_1', 'binary_label_1']]\n",
    "df_l3_slim = df_l3[['text', 'label_3', 'binary_label_3']]\n",
    "\n",
    "# Merge the datasets based on text field\n",
    "merged_df = pd.merge(df_l1_slim, df_l3_slim, on='text', how='inner')\n",
    "\n",
    "# Add a unique_id column to the merged dataset\n",
    "merged_df['unique_id'] = [f'id_{i}' for i in range(len(merged_df))]\n",
    "\n",
    "# Reorder columns\n",
    "merged_df = merged_df[['unique_id', 'text', 'binary_label_1', 'label_1', 'binary_label_3', 'label_3']]\n",
    "\n",
    "# Save the merged dataset\n",
    "merged_df.to_csv('test_ta_task3.csv', index=False)\n",
    "\n",
    "# Display information\n",
    "print(f\"Final merged dataset shape: {merged_df.shape}\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# Check label distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(f\"Label 1 (gendered abuse): {merged_df['binary_label_1'].value_counts().to_dict()}\")\n",
    "print(f\"Label 3 (explicit/aggressive): {merged_df['binary_label_3'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bdcff83-fefa-429e-a792-ce1c1af2c202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original vs Processed Text Samples:\n",
      "Original:    à®µà¯ˆà®°à®®à¯à®¤à¯à®¤à¯ à®’à®°à¯ à®•à®¾à®® à®®à®¿à®°à¯à®•à®®à¯ à®à®©à¯à®ªà®¤à¯ à®šà®¿à®©à®¿à®®à®¾ à®¤à¯à®±à¯ˆà®¯à®¿à®²à¯ à®‡à®°à¯à®•à¯à®•à¯à®®à¯ à®…à®©à¯ˆà®µà®°à¯à®•à¯à®•à¯à®®à¯ à®¤à¯†à®°à®¿à®¯à¯à®®à¯.  à®šà®®à¯à®¤à®¾à®¯à®¤à¯à®¤à¯à®•à¯à®•à¯ à®à®¤à¯à®¤à®©à¯ˆ à®ªà¯‡à®°à®µà®²à®®à¯ !  #SexualPredatorVairamuthu #à®ªà¯Šà®®à¯à®ªà®³_à®ªà¯Šà®°à¯à®•à¯à®•à®¿_à®µà¯ˆà®°à®®à¯à®¤à¯à®¤à¯ #SexualPredatorVairamuthu #à®ªà¯Šà®®à¯à®ªà®³_à®ªà¯Šà®°à¯à®•à¯à®•à®¿_à®µà¯ˆà®°à®®à¯à®¤à¯à®¤à¯ #SexualPredatorVairamuthu\n",
      "Processed:    à®µà¯ˆà®°à®®à¯à®¤à¯à®¤à¯ à®’à®°à¯ à®•à®¾à®® à®®à®¿à®°à¯à®•à®®à¯ à®à®©à¯à®ªà®¤à¯ à®šà®¿à®©à®¿à®®à®¾ à®¤à¯à®±à¯ˆà®¯à®¿à®²à¯ à®‡à®°à¯à®•à¯à®•à¯à®®à¯ à®…à®©à¯ˆà®µà®°à¯à®•à¯à®•à¯à®®à¯ à®¤à¯†à®°à®¿à®¯à¯à®®à¯   à®šà®®à¯à®¤à®¾à®¯à®¤à¯à®¤à¯à®•à¯à®•à¯ à®à®¤à¯à®¤à®©à¯ˆ à®ªà¯‡à®°à®µà®²à®®à¯     sexualpredatorvairamuthu  à®ªà¯Šà®®à¯à®ªà®³ à®ªà¯Šà®°à¯à®•à¯à®•à®¿ à®µà¯ˆà®°à®®à¯à®¤à¯à®¤à¯  sexualpredatorvairamuthu  à®ªà¯Šà®®à¯à®ªà®³ à®ªà¯Šà®°à¯à®•à¯à®•à®¿ à®µà¯ˆà®°à®®à¯à®¤à¯à®¤à¯  sexualpredatorvairamuthu\n",
      "--------------------------------------------------\n",
      "Original: #4YrsOfValiantVIVEGAM  #Valimai #AjithKumar   à®à®£à¯à®Ÿà®¾ à®•à¯à®³à¯à®³ à®ªà¯à®£à¯à®Ÿà¯ˆà®•à®²à®¾,à®¨à¯€à®™à¯à®•à®²à®¾à®®à¯ à®…à®œà¯€à®¤à¯,à®µà®¿à®œà®¯à¯ à®ªà¯‡à®©à¯à®¸à¯à®•à¯à®•à¯ à®ªà¯‹à®Ÿà¯à®Ÿà®¿à®¯à®¾,à®šà¯‚à®¤à¯à®¤à¯ à®…à®Ÿà®¿à®šà¯à®šà®¿ à®µà®¿à®Ÿà¯à®°à¯à®µà¯‹à®®à¯,à®ªà¯‹à®¯à®¿ à®¤à®©à¯à®¸à¯,à®šà®¿à®µà®¾ à®ªà¯‡à®©à¯à®¸à¯ à®•à¯‚à®Ÿ à®šà®£à¯à®Ÿ à®ªà¯‹à®Ÿà¯à®™à¯à®•à®Ÿà®¾ <handle replaced>ğŸ’¦à®à®šà¯à®š à®ªà¯‡à®©à¯à®¸à¯\n",
      "Processed:      valimai  ajithkumar   à®à®£à¯à®Ÿà®¾ à®•à¯à®³à¯à®³ à®ªà¯à®£à¯à®Ÿà¯ˆà®•à®²à®¾ à®¨à¯€à®™à¯à®•à®²à®¾à®®à¯ à®…à®œà¯€à®¤à¯ à®µà®¿à®œà®¯à¯ à®ªà¯‡à®©à¯à®¸à¯à®•à¯à®•à¯ à®ªà¯‹à®Ÿà¯à®Ÿà®¿à®¯à®¾ à®šà¯‚à®¤à¯à®¤à¯ à®…à®Ÿà®¿à®šà¯à®šà®¿ à®µà®¿à®Ÿà¯à®°à¯à®µà¯‹à®®à¯ à®ªà¯‹à®¯à®¿ à®¤à®©à¯à®¸à¯ à®šà®¿à®µà®¾ à®ªà¯‡à®©à¯à®¸à¯ à®•à¯‚à®Ÿ à®šà®£à¯à®Ÿ à®ªà¯‹à®Ÿà¯à®™à¯à®•à®Ÿà®¾  à®à®šà¯à®š à®ªà¯‡à®©à¯à®¸à¯\n",
      "--------------------------------------------------\n",
      "Original: #AmbedkarBlueShirtRally  à®‡à®¨à¯à®¤ à®ªà¯‹à®°à®¾à®Ÿà¯à®Ÿà®¤à¯à®¤à¯à®•à¯à®•à¯ à®µà®¨à¯à®¤ à®•à¯‚à®Ÿà¯à®Ÿà®®à¯ #à®¤à®°à¯à®ªà®¾à®°à¯ à®ªà®Ÿà®¤à¯à®¤à¯à®•à¯à®•à¯ à®µà®¨à¯à®¤ à®•à¯‚à®Ÿà¯à®Ÿà®¤à¯à®¤à¯Šà®Ÿà¯ à®…à®¤à®¿à®•à®®à¯ à®‡à®²à¯à®²à¯ˆ. à®‡à®¤à®¿à®²à¯ à®¯à®¾à®°à¯ à®¯à®¾à®°à¯ˆ à®µà®¿à®®à®°à¯à®šà®¿à®ªà¯à®ªà®¤à¯ ?   #à®ªà¯†à®°à®¿à®¯à®¾à®°à®¾à®µà®¤à¯_à®®à®¯à®¿à®°à®¾à®µà®¤à¯\n",
      "Processed:  ambedkarblueshirtrally  à®‡à®¨à¯à®¤ à®ªà¯‹à®°à®¾à®Ÿà¯à®Ÿà®¤à¯à®¤à¯à®•à¯à®•à¯ à®µà®¨à¯à®¤ à®•à¯‚à®Ÿà¯à®Ÿà®®à¯  à®¤à®°à¯à®ªà®¾à®°à¯ à®ªà®Ÿà®¤à¯à®¤à¯à®•à¯à®•à¯ à®µà®¨à¯à®¤ à®•à¯‚à®Ÿà¯à®Ÿà®¤à¯à®¤à¯Šà®Ÿà¯ à®…à®¤à®¿à®•à®®à¯ à®‡à®²à¯à®²à¯ˆ  à®‡à®¤à®¿à®²à¯ à®¯à®¾à®°à¯ à®¯à®¾à®°à¯ˆ à®µà®¿à®®à®°à¯à®šà®¿à®ªà¯à®ªà®¤à¯      à®ªà¯†à®°à®¿à®¯à®¾à®°à®¾à®µà®¤à¯ à®®à®¯à®¿à®°à®¾à®µà®¤à¯\n",
      "--------------------------------------------------\n",
      "\n",
      "Processed dataset shape: (1135, 7)\n",
      "  unique_id                                               text  \\\n",
      "0      id_0     à®µà¯ˆà®°à®®à¯à®¤à¯à®¤à¯ à®’à®°à¯ à®•à®¾à®® à®®à®¿à®°à¯à®•à®®à¯ à®à®©à¯à®ªà®¤à¯ à®šà®¿à®©à®¿à®®à®¾ à®¤à¯à®±...   \n",
      "1      id_1  #4YrsOfValiantVIVEGAM  #Valimai #AjithKumar   ...   \n",
      "2      id_2  #AmbedkarBlueShirtRally  à®‡à®¨à¯à®¤ à®ªà¯‹à®°à®¾à®Ÿà¯à®Ÿà®¤à¯à®¤à¯à®•à¯à®•à¯ ...   \n",
      "3      id_3  #BREAKING | à®¤à®¿à®°à¯à®šà¯à®šà®¿ à®®à®¾à®µà®Ÿà¯à®Ÿà®®à¯  à®®à®£à®ªà¯à®ªà®¾à®±à¯ˆà®¯à¯ˆ à®…à®Ÿà¯à®¤...   \n",
      "4      id_4  #Bachelor ğŸ˜¤ğŸ˜¤ğŸ˜¤ğŸ˜¤ğŸ˜¤à®ªà®Ÿà®®à®¾à®Ÿà®¾ à®‡à®¤à¯ à®•à¯‹à®¤à¯à®¤à®¾ <handle repla...   \n",
      "\n",
      "                                      processed_text binary_label_1  label_1  \\\n",
      "0     à®µà¯ˆà®°à®®à¯à®¤à¯à®¤à¯ à®’à®°à¯ à®•à®¾à®® à®®à®¿à®°à¯à®•à®®à¯ à®à®©à¯à®ªà®¤à¯ à®šà®¿à®©à®¿à®®à®¾ à®¤à¯à®±...       not_hate        0   \n",
      "1       valimai  ajithkumar   à®à®£à¯à®Ÿà®¾ à®•à¯à®³à¯à®³ à®ªà¯à®£à¯à®Ÿà¯ˆà®•...       not_hate        0   \n",
      "2   ambedkarblueshirtrally  à®‡à®¨à¯à®¤ à®ªà¯‹à®°à®¾à®Ÿà¯à®Ÿà®¤à¯à®¤à¯à®•à¯à®•à¯ ...       not_hate        0   \n",
      "3   breaking   à®¤à®¿à®°à¯à®šà¯à®šà®¿ à®®à®¾à®µà®Ÿà¯à®Ÿà®®à¯  à®®à®£à®ªà¯à®ªà®¾à®±à¯ˆà®¯à¯ˆ à®…à®Ÿà¯à®¤...       not_hate        0   \n",
      "4                       bachelor à®ªà®Ÿà®®à®¾à®Ÿà®¾ à®‡à®¤à¯ à®•à¯‹à®¤à¯à®¤à®¾         not_hate        0   \n",
      "\n",
      "  binary_label_3  label_3  \n",
      "0   not_explicit        0  \n",
      "1       explicit        1  \n",
      "2       explicit        1  \n",
      "3   not_explicit        0  \n",
      "4       explicit        1  \n"
     ]
    }
   ],
   "source": [
    "# Apply text normalization\n",
    "merged_df['processed_text'] = merged_df['text'].apply(lambda x: normalize_text(x))\n",
    "\n",
    "# Further processing to remove '...'\n",
    "merged_df['processed_text'] = merged_df['processed_text'].str.replace('...', '')\n",
    "\n",
    "# Display samples of processed text\n",
    "print(\"\\nOriginal vs Processed Text Samples:\")\n",
    "for i in range(3):\n",
    "    print(f\"Original: {merged_df['text'].iloc[i]}\")\n",
    "    print(f\"Processed: {merged_df['processed_text'].iloc[i]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Keep all columns but add processed text\n",
    "merged_df_final = merged_df[['unique_id', 'text', 'processed_text', 'binary_label_1', 'label_1', 'binary_label_3', 'label_3']]\n",
    "\n",
    "\n",
    "print(f\"\\nProcessed dataset shape: {merged_df_final.shape}\")\n",
    "print(merged_df_final.head())\n",
    "\n",
    "# Extract features (processed text)\n",
    "X = list(merged_df_final['processed_text'])\n",
    "\n",
    "# Extract labels for both tasks\n",
    "y_task1 = merged_df_final['label_1'].values\n",
    "y_task3 = merged_df_final['label_3'].values\n",
    "tokenizer.fit_on_texts(X)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding\n",
    "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "\n",
    "y_task1 = label_encoder.fit_transform(y_task1)\n",
    "y_task3 = label_encoder.fit_transform(y_task3)\n",
    "\n",
    "y_task1 = to_categorical(y_task1, num_classes=2)\n",
    "y_task3 = to_categorical(y_task3, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c75750d-89e6-46d5-b28b-f48d804bc4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step \n",
      "\n",
      "Task 1 (Gendered Abuse) Test Results:\n",
      "Precision: 0.6175\n",
      "Recall: 0.6167\n",
      "weighted F1 Score: 0.6168\n",
      "macro F1 Score: 0.6167\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not_hate       0.60      0.62      0.61       548\n",
      "        hate       0.63      0.61      0.62       587\n",
      "\n",
      "    accuracy                           0.62      1135\n",
      "   macro avg       0.62      0.62      0.62      1135\n",
      "weighted avg       0.62      0.62      0.62      1135\n",
      "\n",
      "\n",
      "Task 3 (Explicit Language) Test Results:\n",
      "Precision: 0.6886\n",
      "Recall: 0.6952\n",
      "weighted F1 Score: 0.6914\n",
      "macro F1 Score: 0.6392\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "not_explicit       0.52      0.48      0.50       359\n",
      "    explicit       0.77      0.80      0.78       776\n",
      "\n",
      "    accuracy                           0.70      1135\n",
      "   macro avg       0.64      0.64      0.64      1135\n",
      "weighted avg       0.69      0.70      0.69      1135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "test_results = evaluate_multi_task_validation(\n",
    "    trained_model, X, y_task1, y_task3\n",
    ")\n",
    "\n",
    "# Print Task 1 (Gendered Abuse) results\n",
    "print(\"\\nTask 1 (Gendered Abuse) Test Results:\")\n",
    "print(f\"Precision: {test_results['task1']['precision']:.4f}\")\n",
    "print(f\"Recall: {test_results['task1']['recall']:.4f}\")\n",
    "print(f\"weighted F1 Score: {test_results['task1']['f1_score_weighted']:.4f}\")\n",
    "print(f\"macro F1 Score: {test_results['task1']['f1_score_macro']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(test_results['task1']['classification_report'])\n",
    "\n",
    "# Print Task 3 (Explicit Language) results\n",
    "print(\"\\nTask 3 (Explicit Language) Test Results:\")\n",
    "print(f\"Precision: {test_results['task3']['precision']:.4f}\")\n",
    "print(f\"Recall: {test_results['task3']['recall']:.4f}\")\n",
    "print(f\"weighted F1 Score: {test_results['task3']['f1_score_weighted']:.4f}\")\n",
    "print(f\"macro F1 Score: {test_results['task3']['f1_score_macro']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(test_results['task3']['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db15968-eada-4c16-842f-5b36565d3094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
