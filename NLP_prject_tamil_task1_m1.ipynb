{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13850ab6-0b22-42c1-b0d7-b26093d6bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from skmultilearn.adapt import MLkNN\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import hamming_loss, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a9b98f-4673-449b-b50a-eb77805444ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ta_a1</th>\n",
       "      <th>ta_a2</th>\n",
       "      <th>ta_a3</th>\n",
       "      <th>ta_a4</th>\n",
       "      <th>ta_a5</th>\n",
       "      <th>ta_a6</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*1. роорпБро░роЪрпКро▓ро┐ роЕро▓рпБро╡ро▓роХроорпН роЕроорпИроирпНродрпБро│рпНро│ роЗроЯроорпН рокроЮрпНроЪрооро┐...</td>\n",
       "      <td>question_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>роЪрпЛродрпНродрпБроХрпНроХрпБ рокро┐роЪрпНроЪрпИ роОроЯрпБроХрпНроХро┐ро▒ роХроЯроЩрпНроХро╛ро░ роиро╛ропрпНроХро│рпБроХ...</td>\n",
       "      <td>question_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>родродрпНродрокрпБродрпНрод родродрпНродрокрпБродрпНрод ройрпНройрпБ роОродро╛ро╡родрпБ рокрпБро░ро┐ропрпБродро╛</td>\n",
       "      <td>question_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>рокроЪрпНроЪрпИ роорпКро│роХро╛ роХро╛ро░роорпН vicky роЕроорпНрооро╛ рокрпБрогрпНроЯрпИ роиро╛ро▒рпБроорпН ЁЯШЖ</td>\n",
       "      <td>question_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>роОройрпНрой роЙроЯроорпНрокрпБ роЯро╛ роЪро╛рооро┐- роЪрпБроорпНрооро╛ ро╡ро│рпБро╡ро│рпБройрпБ.. роорпБро▓рпИ ...</td>\n",
       "      <td>question_1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6774</th>\n",
       "      <td>ЁЯШнЁЯШнЁЯШн роТроорпНрооро╛ро│ рокроЯро┐роХрпНроХро▓рпН рокрпБрогрпНроЯ ЁЯШнЁЯШнЁЯШн</td>\n",
       "      <td>question_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6775</th>\n",
       "      <td>ЁЯЩДЁЯЩДЁЯЩДЁЯЩД роОройрпНрой роОро┤ро╡рпБропро╛ роЗродрпБ...   роЗродрпЖро▓рпНро▓ро╛роорпН роТро░рпБ рокрпЖро░рпБроорпИ...</td>\n",
       "      <td>question_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6776</th>\n",
       "      <td>ЁЯЪироОроХрпНро╕рпН рокро┐ро░ро╕рпН рокрпЗро░рпНро▓рпН роХрокрпНрокро▓рпН родрпА ро╡ро┐рокродрпНродрпБроХрпНроХрпБ роЙро│рпНро│...</td>\n",
       "      <td>question_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6777</th>\n",
       "      <td>ЁЯдг ЁЯдг роЪро▓рпНро▓ро┐ роЬро╛родро┐ ро╡рпЖро▒ро┐ роорпБроЯрпНроЯро╛ рокрпБрогрпНроЯ роЙроЩрпНроХ рокрпКрогрпНрогрпБроЩрпН...</td>\n",
       "      <td>question_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6778</th>\n",
       "      <td>ЁЯдгЁЯдгЁЯдг роирпА роЪрпКро▓рпНро▒родрпБ роОро▓рпНро▓ро╛роорпБроорпН роЕроирпНрод родро┐роорпНроХро╡рпЛроЯ родроорпНрокро┐  ...</td>\n",
       "      <td>question_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6779 rows ├Ч 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text   unique_id  ta_a1  \\\n",
       "0        *1. роорпБро░роЪрпКро▓ро┐ роЕро▓рпБро╡ро▓роХроорпН роЕроорпИроирпНродрпБро│рпНро│ роЗроЯроорпН рокроЮрпНроЪрооро┐...  question_1    NaN   \n",
       "1        роЪрпЛродрпНродрпБроХрпНроХрпБ рокро┐роЪрпНроЪрпИ роОроЯрпБроХрпНроХро┐ро▒ роХроЯроЩрпНроХро╛ро░ роиро╛ропрпНроХро│рпБроХ...  question_1    NaN   \n",
       "2              родродрпНродрокрпБродрпНрод родродрпНродрокрпБродрпНрод ройрпНройрпБ роОродро╛ро╡родрпБ рокрпБро░ро┐ропрпБродро╛  question_1    NaN   \n",
       "3         рокроЪрпНроЪрпИ роорпКро│роХро╛ роХро╛ро░роорпН vicky роЕроорпНрооро╛ рокрпБрогрпНроЯрпИ роиро╛ро▒рпБроорпН ЁЯШЖ  question_1    NaN   \n",
       "4       роОройрпНрой роЙроЯроорпНрокрпБ роЯро╛ роЪро╛рооро┐- роЪрпБроорпНрооро╛ ро╡ро│рпБро╡ро│рпБройрпБ.. роорпБро▓рпИ ...  question_1    1.0   \n",
       "...                                                 ...         ...    ...   \n",
       "6774                      ЁЯШнЁЯШнЁЯШн роТроорпНрооро╛ро│ рокроЯро┐роХрпНроХро▓рпН рокрпБрогрпНроЯ ЁЯШнЁЯШнЁЯШн  question_1    NaN   \n",
       "6775  ЁЯЩДЁЯЩДЁЯЩДЁЯЩД роОройрпНрой роОро┤ро╡рпБропро╛ роЗродрпБ...   роЗродрпЖро▓рпНро▓ро╛роорпН роТро░рпБ рокрпЖро░рпБроорпИ...  question_1    NaN   \n",
       "6776  ЁЯЪироОроХрпНро╕рпН рокро┐ро░ро╕рпН рокрпЗро░рпНро▓рпН роХрокрпНрокро▓рпН родрпА ро╡ро┐рокродрпНродрпБроХрпНроХрпБ роЙро│рпНро│...  question_1    NaN   \n",
       "6777  ЁЯдг ЁЯдг роЪро▓рпНро▓ро┐ роЬро╛родро┐ ро╡рпЖро▒ро┐ роорпБроЯрпНроЯро╛ рокрпБрогрпНроЯ роЙроЩрпНроХ рокрпКрогрпНрогрпБроЩрпН...  question_1    NaN   \n",
       "6778  ЁЯдгЁЯдгЁЯдг роирпА роЪрпКро▓рпНро▒родрпБ роОро▓рпНро▓ро╛роорпБроорпН роЕроирпНрод родро┐роорпНроХро╡рпЛроЯ родроорпНрокро┐  ...  question_1    NaN   \n",
       "\n",
       "      ta_a2  ta_a3  ta_a4  ta_a5  ta_a6  label  \n",
       "0       NaN    0.0    0.0    0.0    0.0      0  \n",
       "1       NaN    NaN    0.0    NaN    NaN      0  \n",
       "2       NaN    NaN    NaN    0.0    NaN      0  \n",
       "3       NaN    NaN    NaN    1.0    NaN      1  \n",
       "4       NaN    NaN    NaN    NaN    NaN      1  \n",
       "...     ...    ...    ...    ...    ...    ...  \n",
       "6774    NaN    1.0    NaN    NaN    NaN      1  \n",
       "6775    NaN    NaN    NaN    0.0    NaN      0  \n",
       "6776    0.0    NaN    NaN    NaN    NaN      0  \n",
       "6777    0.0    NaN    NaN    NaN    NaN      0  \n",
       "6778    NaN    1.0    NaN    NaN    NaN      1  \n",
       "\n",
       "[6779 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2= pd.read_csv('train_ta_l1.csv')\n",
    "d2\n",
    "d2 = d2.rename(columns={'key' : 'unique_id', 'sentence' : 'text'})\n",
    "d2.to_csv('updated_train_ta_l1.csv', index=False)\n",
    "# d2\n",
    "\n",
    "# Convert annotator columns to numeric without replacing NaNs\n",
    "d2[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5', 'ta_a6']] = d2[\n",
    "    ['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5', 'ta_a6']\n",
    "].apply(pd.to_numeric, errors='coerce')  # NaNs are retained\n",
    "\n",
    "# Compute 'label' based on majority voting while ignoring NaNs\n",
    "d2['label'] = (\n",
    "    d2[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5', 'ta_a6']].mean(axis=1, skipna=True) >= 0.5\n",
    ").astype(int)\n",
    "\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c9e4dbb-9d84-4648-a243-8fd092baf0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary label ('hate' or 'not_hate')\n",
    "def determine_binary_label(label):\n",
    "    return 'hate' if label == 1 else 'not_hate'\n",
    "\n",
    "d2['binary_label'] = d2['label'].apply(determine_binary_label)\n",
    "\n",
    "# # Reorder columns\n",
    "d2 = d2[['unique_id', 'text', 'binary_label', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "719d89c4-fa91-4492-91f2-d215ec6d19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.to_csv('updated_train_ta_l1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f479ff8-3035-4683-b64a-638f61749c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>роорпБро░роЪрпКро▓ро┐ роЕро▓рпБро╡ро▓роХроорпН роЕроорпИроирпНродрпБро│рпНро│ роЗроЯроорпН рокроЮрпНроЪрооро┐...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>роЪрпЛродрпНродрпБроХрпНроХрпБ рокро┐роЪрпНроЪрпИ роОроЯрпБроХрпНроХро┐ро▒ роХроЯроЩрпНроХро╛ро░ роиро╛ропрпНроХро│рпБроХ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>родродрпНродрокрпБродрпНрод родродрпНродрокрпБродрпНрод ройрпНройрпБ роОродро╛ро╡родрпБ рокрпБро░ро┐ропрпБродро╛</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>рокроЪрпНроЪрпИ роорпКро│роХро╛ роХро╛ро░роорпН vicky роЕроорпНрооро╛ рокрпБрогрпНроЯрпИ роиро╛ро▒рпБроорпН</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>роОройрпНрой роЙроЯроорпНрокрпБ роЯро╛ роЪро╛рооро┐  роЪрпБроорпНрооро╛ ро╡ро│рпБро╡ро│рпБройрпБ   роорпБро▓рпИ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0         роорпБро░роЪрпКро▓ро┐ роЕро▓рпБро╡ро▓роХроорпН роЕроорпИроирпНродрпБро│рпНро│ роЗроЯроорпН рокроЮрпНроЪрооро┐...\n",
       "1     роЪрпЛродрпНродрпБроХрпНроХрпБ рокро┐роЪрпНроЪрпИ роОроЯрпБроХрпНроХро┐ро▒ роХроЯроЩрпНроХро╛ро░ роиро╛ропрпНроХро│рпБроХ...\n",
       "2           родродрпНродрокрпБродрпНрод родродрпНродрокрпБродрпНрод ройрпНройрпБ роОродро╛ро╡родрпБ рокрпБро░ро┐ропрпБродро╛\n",
       "3       рокроЪрпНроЪрпИ роорпКро│роХро╛ роХро╛ро░роорпН vicky роЕроорпНрооро╛ рокрпБрогрпНроЯрпИ роиро╛ро▒рпБроорпН \n",
       "4    роОройрпНрой роЙроЯроорпНрокрпБ роЯро╛ роЪро╛рооро┐  роЪрпБроорпНрооро╛ ро╡ро│рпБро╡ро│рпБройрпБ   роорпБро▓рпИ ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"\n",
    "                               u\"\\U0001F700-\\U0001F77F\"\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', ' ', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'<.*?>+', ' ', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', ' ', text)\n",
    "    text = re.sub(r'<handle replaced>', '', text)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "## Apply the written function ##\n",
    "d2.loc[:, 'text'] = d2['text'].apply(lambda x: normalize_text(x))\n",
    "processed_list = []\n",
    "for j in d2['text']:\n",
    "    process = j.replace('...','')\n",
    "    processed_list.append(process)\n",
    "\n",
    "df_processed = pd.DataFrame(processed_list)\n",
    "df_processed.columns = ['text']\n",
    "df_processed.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7adbf55-c7e4-484e-8617-b1461d6d09bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]], shape=(6779, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = list(df_processed['text'])\n",
    "y = d2[['label']].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bab68908-8e8a-4787-8491-0f95feaa4140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1557 2466  444 ...    0    0    0]\n",
      " [1396  323 2468 ...    0    0    0]\n",
      " [ 135  520 2470 ...    0    0    0]\n",
      " ...\n",
      " [3879 2142 1100 ...    0    0    0]\n",
      " [ 406  430  602 ...    0    0    0]\n",
      " [   4  850   13 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Activation, Dropout, Dense, Flatten,\n",
    "    Bidirectional, GRU, concatenate, SpatialDropout1D,\n",
    "    GlobalMaxPooling1D, GlobalAveragePooling1D, Conv1D,\n",
    "    Embedding, Input, Concatenate\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "######## Textual Features for Embedding ###################\n",
    "max_len = 100\n",
    "max_features = 4479\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding\n",
    "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "\n",
    "print(X)  # Check the processed sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b0ffd54-30a5-46dd-8c5f-eb4aceb97381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1], shape=(6779,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y.ravel())\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6efbc130-1845-4a50-8d68-90e08db81643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], shape=(6779, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y, num_classes=2)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5683d31-a5e2-4171-8b08-4a20a90e0ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.loc[:, 'binary_label'] = d2['label'].apply(determine_binary_label)\n",
    "\n",
    "# # Reorder columns\n",
    "d2 = d2[['unique_id', 'text', 'binary_label', 'label']]\n",
    "\n",
    "d2.to_csv('updated_test_en_l1.csv', index=False)\n",
    "\n",
    "d2.loc[:, 'text'] = d2['text'].apply(lambda x: normalize_text(x))\n",
    "processed_list = []\n",
    "for j in d2['text']:\n",
    "    process = j.replace('...','')\n",
    "    processed_list.append(process)\n",
    "\n",
    "df_processed = pd.DataFrame(processed_list)\n",
    "df_processed.columns = ['text']\n",
    "df_processed.head(n=5)\n",
    "\n",
    "X = list(df_processed['text'])\n",
    "y = d2[['label']].values\n",
    "\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding\n",
    "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "\n",
    "y = label_encoder.fit_transform(y.ravel())\n",
    "\n",
    "y = to_categorical(y, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "105a07e4-6fb8-481a-a23f-bd30b59a1ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (4479, 50)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load GloVe embeddings from JSON\n",
    "with open('glove_embeddings.json', encoding=\"utf8\") as f:\n",
    "    embeddings_list = json.load(f)\n",
    "\n",
    "# Convert the list of vectors to a dictionary with word indices as keys\n",
    "embeddings_dictionary = {str(i): vector for i, vector in enumerate(embeddings_list)}\n",
    "\n",
    "# Define tokenizer \n",
    "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_features, vocab_size)  # Limit vocab to max_features\n",
    "\n",
    "# Get embedding dimension (from first vector in list)\n",
    "embed_size = len(embeddings_list[0]) if embeddings_list else 0\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "\n",
    "# Fill embedding matrix with corresponding word vectors\n",
    "for word, index in word_index.items():\n",
    "    if index >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_dictionary.get(word) or embeddings_dictionary.get(str(index))\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = np.asarray(embedding_vector, dtype=np.float32)\n",
    "\n",
    "print(\"Embedding matrix shape:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75d0dd00-f84d-46a9-b60a-0ab81c8d5f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5423\n",
      "Validation samples: 1356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krmri\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">тФПтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФ│тФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФ│тФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФУ\n",
       "тФГ<span style=\"font-weight: bold\"> Layer (type)                         </span>тФГ<span style=\"font-weight: bold\"> Output Shape                </span>тФГ<span style=\"font-weight: bold\">         Param # </span>тФГ\n",
       "тФбтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтХЗтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтХЗтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФй\n",
       "тФВ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             тФВ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 тФВ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> тФВ\n",
       "тФЬтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФд\n",
       "тФВ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                тФВ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             тФВ         <span style=\"color: #00af00; text-decoration-color: #00af00\">223,950</span> тФВ\n",
       "тФЬтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФд\n",
       "тФВ spatial_dropout1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>) тФВ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             тФВ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> тФВ\n",
       "тФЬтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФд\n",
       "тФВ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                      тФВ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             тФВ           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,464</span> тФВ\n",
       "тФЬтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФд\n",
       "тФВ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        тФВ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            тФВ         <span style=\"color: #00af00; text-decoration-color: #00af00\">197,632</span> тФВ\n",
       "тФЬтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФд\n",
       "тФВ global_average_pooling1d             тФВ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 тФВ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> тФВ\n",
       "тФВ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)             тФВ                             тФВ                 тФВ\n",
       "тФЬтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФд\n",
       "тФВ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        тФВ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 тФВ          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> тФВ\n",
       "тФЬтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФд\n",
       "тФВ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    тФВ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 тФВ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> тФВ\n",
       "тФЬтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФд\n",
       "тФВ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      тФВ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   тФВ             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> тФВ\n",
       "тФФтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ┤тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ┤тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФШ\n",
       "</pre>\n"
      ],
      "text/plain": [
       "тФПтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФ│тФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФ│тФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФУ\n",
       "тФГ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mтФГ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mтФГ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mтФГ\n",
       "тФбтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтХЗтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтХЗтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФй\n",
       "тФВ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             тФВ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 тФВ               \u001b[38;5;34m0\u001b[0m тФВ\n",
       "тФЬтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФд\n",
       "тФВ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                тФВ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m50\u001b[0m)             тФВ         \u001b[38;5;34m223,950\u001b[0m тФВ\n",
       "тФЬтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФд\n",
       "тФВ spatial_dropout1d (\u001b[38;5;33mSpatialDropout1D\u001b[0m) тФВ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m50\u001b[0m)             тФВ               \u001b[38;5;34m0\u001b[0m тФВ\n",
       "тФЬтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФд\n",
       "тФВ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                      тФВ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)             тФВ           \u001b[38;5;34m6,464\u001b[0m тФВ\n",
       "тФЬтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФд\n",
       "тФВ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        тФВ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)            тФВ         \u001b[38;5;34m197,632\u001b[0m тФВ\n",
       "тФЬтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФд\n",
       "тФВ global_average_pooling1d             тФВ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 тФВ               \u001b[38;5;34m0\u001b[0m тФВ\n",
       "тФВ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)             тФВ                             тФВ                 тФВ\n",
       "тФЬтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФд\n",
       "тФВ dense (\u001b[38;5;33mDense\u001b[0m)                        тФВ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 тФВ          \u001b[38;5;34m32,896\u001b[0m тФВ\n",
       "тФЬтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФд\n",
       "тФВ dropout (\u001b[38;5;33mDropout\u001b[0m)                    тФВ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 тФВ               \u001b[38;5;34m0\u001b[0m тФВ\n",
       "тФЬтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ╝тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФд\n",
       "тФВ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      тФВ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   тФВ             \u001b[38;5;34m258\u001b[0m тФВ\n",
       "тФФтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ┤тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФ┤тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФШ\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">461,200</span> (1.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m461,200\u001b[0m (1.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">237,250</span> (926.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m237,250\u001b[0m (926.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">223,950</span> (874.80 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m223,950\u001b[0m (874.80 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.5531 - loss: 0.6887 - macro_f1_score: 0.5531  \n",
      "Epoch 1: macro_f1_score improved from -inf to 0.56113, saving model to models_ta_task1_m1\\best_model_ta_task1_m1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 104ms/step - accuracy: 0.5531 - loss: 0.6886 - macro_f1_score: 0.5531 - val_accuracy: 0.5737 - val_loss: 0.6784 - val_macro_f1_score: 0.5737\n",
      "Epoch 2/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.5536 - loss: 0.6841 - macro_f1_score: 0.5536 \n",
      "Epoch 2: macro_f1_score improved from 0.56113 to 0.56537, saving model to models_ta_task1_m1\\best_model_ta_task1_m1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - accuracy: 0.5537 - loss: 0.6841 - macro_f1_score: 0.5537 - val_accuracy: 0.5715 - val_loss: 0.6700 - val_macro_f1_score: 0.5715\n",
      "Epoch 3/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.5819 - loss: 0.6729 - macro_f1_score: 0.5819 \n",
      "Epoch 3: macro_f1_score improved from 0.56537 to 0.58436, saving model to models_ta_task1_m1\\best_model_ta_task1_m1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.5819 - loss: 0.6729 - macro_f1_score: 0.5819 - val_accuracy: 0.5715 - val_loss: 0.6684 - val_macro_f1_score: 0.5715\n",
      "Epoch 4/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.5935 - loss: 0.6606 - macro_f1_score: 0.5935 \n",
      "Epoch 4: macro_f1_score improved from 0.58436 to 0.59727, saving model to models_ta_task1_m1\\best_model_ta_task1_m1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.5935 - loss: 0.6606 - macro_f1_score: 0.5935 - val_accuracy: 0.5841 - val_loss: 0.6623 - val_macro_f1_score: 0.5841\n",
      "Epoch 5/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.5810 - loss: 0.6659 - macro_f1_score: 0.5810 \n",
      "Epoch 5: macro_f1_score did not improve from 0.59727\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.5811 - loss: 0.6658 - macro_f1_score: 0.5811 - val_accuracy: 0.5841 - val_loss: 0.6642 - val_macro_f1_score: 0.5841\n",
      "Epoch 6/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.6144 - loss: 0.6504 - macro_f1_score: 0.6144\n",
      "Epoch 6: macro_f1_score improved from 0.59727 to 0.60465, saving model to models_ta_task1_m1\\best_model_ta_task1_m1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 106ms/step - accuracy: 0.6144 - loss: 0.6504 - macro_f1_score: 0.6144 - val_accuracy: 0.5892 - val_loss: 0.6612 - val_macro_f1_score: 0.5892\n",
      "Epoch 7/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.6069 - loss: 0.6437 - macro_f1_score: 0.6069 \n",
      "Epoch 7: macro_f1_score did not improve from 0.60465\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 108ms/step - accuracy: 0.6069 - loss: 0.6437 - macro_f1_score: 0.6069 - val_accuracy: 0.5767 - val_loss: 0.6690 - val_macro_f1_score: 0.5767\n",
      "Epoch 8/15\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.5985 - loss: 0.6506 - macro_f1_score: 0.5985 \n",
      "Epoch 8: macro_f1_score did not improve from 0.60465\n",
      "\u001b[1m170/170\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 108ms/step - accuracy: 0.5985 - loss: 0.6506 - macro_f1_score: 0.5985 - val_accuracy: 0.5664 - val_loss: 0.6709 - val_macro_f1_score: 0.5664\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43/43\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step  \n",
      "\n",
      "Validation Results:\n",
      "Precision: 0.5779\n",
      "Recall: 0.5892\n",
      "weighted F1 Score: 0.5268\n",
      "macro F1 Score: 0.4945\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not_hate       0.59      0.89      0.71       778\n",
      "        hate       0.55      0.18      0.28       578\n",
      "\n",
      "    accuracy                           0.59      1356\n",
      "   macro avg       0.57      0.54      0.49      1356\n",
      "weighted avg       0.58      0.59      0.53      1356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, SpatialDropout1D, Conv1D,\n",
    "    Bidirectional, LSTM, Dense, Dropout,\n",
    "    GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Configure GPU for optimal performance\n",
    "def configure_gpu():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Enable memory growth for each GPU\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "            # Use mixed precision for better performance\n",
    "            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "            tf.keras.mixed_precision.set_global_policy(policy)\n",
    "            print('Mixed precision enabled')\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "configure_gpu()\n",
    "\n",
    "# Model Definition\n",
    "def create_cnn_bilstm_model(max_len, max_features, embedding_matrix, embed_size=300):\n",
    "    \"\"\"\n",
    "    Creates the CNN-BiLSTM model architecture as described in the paper\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(max_len,))\n",
    "    \n",
    "    # Embedding layer with pretrained weights (GloVe/FastText)\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=max_features,\n",
    "        output_dim=embed_size,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_len,\n",
    "        trainable=False  # As per paper, embeddings are non-trainable\n",
    "    )(input_layer)\n",
    "    \n",
    "    # Spatial Dropout to prevent overfitting (as mentioned in paper)\n",
    "    spatial_dropout = SpatialDropout1D(0.2)(embedding_layer)\n",
    "    \n",
    "    # CNN Layer (as described in paper)\n",
    "    conv_layer = Conv1D(\n",
    "        filters=64,  # As per paper\n",
    "        kernel_size=2,  # As per paper\n",
    "        activation='relu',\n",
    "        padding='same'\n",
    "    )(spatial_dropout)\n",
    "    \n",
    "    # Bidirectional LSTM Layer (as described in paper)\n",
    "    bilstm_layer = Bidirectional(\n",
    "        LSTM(\n",
    "            units=128,  # As per paper\n",
    "            return_sequences=True,\n",
    "            dropout=0.1,  # As per paper\n",
    "            recurrent_dropout=0.1  # As per paper\n",
    "        )\n",
    "    )(conv_layer)\n",
    "    \n",
    "    # Global Average Pooling (as per paper)\n",
    "    gap_layer = GlobalAveragePooling1D()(bilstm_layer)\n",
    "    \n",
    "    # Dense layer (as per paper)\n",
    "    dense_layer = Dense(128, activation='relu')(gap_layer)\n",
    "    dropout_layer = Dropout(0.1)(dense_layer)  # Additional dropout as per paper\n",
    "    \n",
    "    # Output layer (use float32 for softmax for numerical stability)\n",
    "    output_layer = Dense(2, activation='softmax', dtype='float32')(dropout_layer)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Custom macroF1 Score Metric\n",
    "class MacroF1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes = 2, name='macro_f1_score', **kwargs):\n",
    "        super(MacroF1Score, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Convert probabilities to predicted class indices\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        \n",
    "        # Convert one-hot encoded y_true to class indices if needed\n",
    "        if len(y_true.shape) > 1 and y_true.shape[-1] > 1:\n",
    "            y_true = tf.argmax(y_true, axis=-1)\n",
    "        \n",
    "        # Initialize confusion matrix\n",
    "        conf_matrix = tf.math.confusion_matrix(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            num_classes=self.num_classes,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        # Calculate TP, FP, FN for each class\n",
    "        diag = tf.linalg.diag_part(conf_matrix)\n",
    "        row_sum = tf.reduce_sum(conf_matrix, axis=1)\n",
    "        col_sum = tf.reduce_sum(conf_matrix, axis=0)\n",
    "        \n",
    "        tp = diag\n",
    "        fp = col_sum - diag\n",
    "        fn = row_sum - diag\n",
    "        \n",
    "        # Update the state variables\n",
    "        self.tp.assign_add(tf.reduce_sum(tp))\n",
    "        self.fp.assign_add(tf.reduce_sum(fp))\n",
    "        self.fn.assign_add(tf.reduce_sum(fn))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        # Calculate precision and recall\n",
    "        precision = self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())\n",
    "        recall = self.tp / (self.tp + self.fn + tf.keras.backend.epsilon())\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "        \n",
    "        # Return macro F1 (average of per-class F1 scores)\n",
    "        return f1\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.tp.assign(0.)\n",
    "        self.fp.assign(0.)\n",
    "        self.fn.assign(0.)\n",
    "        self.count.assign(0.)\n",
    "            \n",
    "# Model Training\n",
    "def train_and_validate_model(model, X_train, y_train, X_val, y_val, batch_size=32, epochs=15, model_dir='models_ta_task1_m1'):\n",
    "    \"\"\"\n",
    "    Trains the CNN-BiLSTM model with early stopping and model checkpointing\n",
    "    Returns the best model and training history\n",
    "    \"\"\"\n",
    "    # Create directory for saving models if it doesn't exist\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='macro_f1_score',\n",
    "        patience=2,\n",
    "        restore_best_weights=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        os.path.join(model_dir, 'best_model_ta_task1_m1.h5'),  # Save entire model\n",
    "        monitor='macro_f1_score',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Compile model with Adam optimizer (as per paper)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', MacroF1Score(num_classes=2)]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping, model_checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Load the best model found during training\n",
    "    best_model = load_model(os.path.join(model_dir, 'best_model_ta_task1_m1.h5'), \n",
    "                          custom_objects={'MacroF1Score': MacroF1Score})\n",
    "    \n",
    "    return history, best_model\n",
    "\n",
    "# Plot Training History\n",
    "def plot_training_history(history, plot_dir='plots_nlp_project_ta_task1_m1'):\n",
    "    \"\"\"\n",
    "    Plots training history (accuracy and loss curves)\n",
    "    Saves plots to specified directory\n",
    "    \"\"\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, 'training_history_ta_task1.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Validation Evaluation\n",
    "def evaluate_validation(model, X_val, y_val, plot_dir='plots_nlp_project_ta_task1_m1'):\n",
    "    \"\"\"\n",
    "    Evaluates the model on validation data and saves metrics and plots\n",
    "    \"\"\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_pred_proba = model.predict(X_val, batch_size=32)\n",
    "    \n",
    "    # Convert to class labels\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    y_true = np.argmax(y_val, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=['not_hate', 'hate'])\n",
    "    \n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Not Hate', 'Hate'],\n",
    "                yticklabels=['Not Hate', 'Hate'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix (Validation)')\n",
    "    plt.savefig(os.path.join(plot_dir, 'confusion_matrix_val_ta_task1.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score_weighted': weighted_f1,\n",
    "        'f1_score_macro': macro_f1,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "\n",
    "# Main Execution for Training and Validation\n",
    "if __name__ == \"__main__\":\n",
    "    # Split into train (80%) and validation (20%)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Validation samples: {len(X_val)}\")\n",
    "    \n",
    "    # Create model\n",
    "    embed_size = embedding_matrix.shape[1]\n",
    "    model = create_cnn_bilstm_model(max_len, max_features, embedding_matrix, embed_size)\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model (5 epochs as per paper)\n",
    "    history, trained_model = train_and_validate_model(\n",
    "        model, X_train, y_train, X_val, y_val,\n",
    "        batch_size=32,  # As per paper for Tasks 1 & 3\n",
    "        epochs=15  \n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_results = evaluate_validation(trained_model, X_val, y_val)\n",
    "    \n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(f\"Precision: {val_results['precision']:.4f}\")\n",
    "    print(f\"Recall: {val_results['recall']:.4f}\")\n",
    "    print(f\"weighted F1 Score: {val_results['f1_score_weighted']:.4f}\")\n",
    "    print(f\"macro F1 Score: {val_results['f1_score_macro']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(val_results['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b83ebe51-f8c6-4689-95b9-16992f585346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ta_a1</th>\n",
       "      <th>ta_a2</th>\n",
       "      <th>ta_a3</th>\n",
       "      <th>ta_a4</th>\n",
       "      <th>ta_a5</th>\n",
       "      <th>ta_a6</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ро╡рпИро░роорпБродрпНродрпБ роТро░рпБ роХро╛роо рооро┐ро░рпБроХроорпН роОройрпНрокродрпБ роЪро┐ройро┐рооро╛ родрпБро▒...</td>\n",
       "      <td>question_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#4YrsOfValiantVIVEGAM  #Valimai #AjithKumar   ...</td>\n",
       "      <td>question_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#AmbedkarBlueShirtRally  роЗроирпНрод рокрпЛро░ро╛роЯрпНроЯродрпНродрпБроХрпНроХрпБ ...</td>\n",
       "      <td>question_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#BREAKING | родро┐ро░рпБроЪрпНроЪро┐ рооро╛ро╡роЯрпНроЯроорпН  роорогрокрпНрокро╛ро▒рпИропрпИ роЕроЯрпБрод...</td>\n",
       "      <td>question_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Bachelor ЁЯШдЁЯШдЁЯШдЁЯШдЁЯШдрокроЯрооро╛роЯро╛ роЗродрпБ роХрпЛродрпНродро╛ &lt;handle repla...</td>\n",
       "      <td>question_1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>ЁЯШВЁЯШВЁЯШВ роКроорпНрокрпБ</td>\n",
       "      <td>question_1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>ЁЯШД родрооро┐ро┤рпН родрпЖро░ро┐роЮрпНроЪро╡ройрпН\"родро╛ройрпН роЙроЩрпНроХрпКроорпНрооро╛ро▓ роХрпБрогрпНроЯро┐ роЕроЯро┐роХ...</td>\n",
       "      <td>question_1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>ЁЯШЕЁЯШЕЁЯШВЁЯШВ роирпА родро╛ройрпН рокроЩрпН роЕро╡ройрпЛро▓рпБроХрпНроХрпБ роХро░рпЖроХрпНроЯро╛ роЖрой роЖро│рпБ.. ЁЯШО...</td>\n",
       "      <td>question_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>ЁЯШ║тЬП тАФ роЖрооро╛роорпН ро╡рпЗроЪ роЖрооро╛роорпН ро╡рпЗроЪ роЕропрпНроорпН роЪрпНро▓рпАрокро┐ройрпН роЕропрпНроорпН ...</td>\n",
       "      <td>question_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>є╛Уж :)- Gay  ЁЯЗ▒ЁЯЗ░:)- рокрпКроЯро┐ропройрпН рооро╛ро╕рпНроЯро░рпН ЁЯШВ</td>\n",
       "      <td>question_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1135 rows ├Ч 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text   unique_id  ta_a1  \\\n",
       "0        ро╡рпИро░роорпБродрпНродрпБ роТро░рпБ роХро╛роо рооро┐ро░рпБроХроорпН роОройрпНрокродрпБ роЪро┐ройро┐рооро╛ родрпБро▒...  question_1    NaN   \n",
       "1     #4YrsOfValiantVIVEGAM  #Valimai #AjithKumar   ...  question_1    0.0   \n",
       "2     #AmbedkarBlueShirtRally  роЗроирпНрод рокрпЛро░ро╛роЯрпНроЯродрпНродрпБроХрпНроХрпБ ...  question_1    0.0   \n",
       "3     #BREAKING | родро┐ро░рпБроЪрпНроЪро┐ рооро╛ро╡роЯрпНроЯроорпН  роорогрокрпНрокро╛ро▒рпИропрпИ роЕроЯрпБрод...  question_1    0.0   \n",
       "4     #Bachelor ЁЯШдЁЯШдЁЯШдЁЯШдЁЯШдрокроЯрооро╛роЯро╛ роЗродрпБ роХрпЛродрпНродро╛ <handle repla...  question_1    1.0   \n",
       "...                                                 ...         ...    ...   \n",
       "1130                                          ЁЯШВЁЯШВЁЯШВ роКроорпНрокрпБ  question_1    1.0   \n",
       "1131  ЁЯШД родрооро┐ро┤рпН родрпЖро░ро┐роЮрпНроЪро╡ройрпН\"родро╛ройрпН роЙроЩрпНроХрпКроорпНрооро╛ро▓ роХрпБрогрпНроЯро┐ роЕроЯро┐роХ...  question_1    1.0   \n",
       "1132  ЁЯШЕЁЯШЕЁЯШВЁЯШВ роирпА родро╛ройрпН рокроЩрпН роЕро╡ройрпЛро▓рпБроХрпНроХрпБ роХро░рпЖроХрпНроЯро╛ роЖрой роЖро│рпБ.. ЁЯШО...  question_1    NaN   \n",
       "1133  ЁЯШ║тЬП тАФ роЖрооро╛роорпН ро╡рпЗроЪ роЖрооро╛роорпН ро╡рпЗроЪ роЕропрпНроорпН роЪрпНро▓рпАрокро┐ройрпН роЕропрпНроорпН ...  question_1    NaN   \n",
       "1134                 є╛Уж :)- Gay  ЁЯЗ▒ЁЯЗ░:)- рокрпКроЯро┐ропройрпН рооро╛ро╕рпНроЯро░рпН ЁЯШВ  question_1    NaN   \n",
       "\n",
       "      ta_a2  ta_a3  ta_a4  ta_a5  ta_a6  label  \n",
       "0       NaN    0.0    0.0    NaN    0.0      0  \n",
       "1       NaN    1.0    0.0    NaN    NaN      0  \n",
       "2       0.0    0.0    NaN    NaN    NaN      0  \n",
       "3       NaN    0.0    0.0    0.0    0.0      0  \n",
       "4       0.0    NaN    0.0    NaN    NaN      0  \n",
       "...     ...    ...    ...    ...    ...    ...  \n",
       "1130    1.0    NaN    0.0    NaN    NaN      1  \n",
       "1131    1.0    NaN    NaN    1.0    NaN      1  \n",
       "1132    NaN    1.0    0.0    NaN    1.0      1  \n",
       "1133    0.0    NaN    1.0    1.0    NaN      1  \n",
       "1134    1.0    0.0    1.0    NaN    NaN      1  \n",
       "\n",
       "[1135 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2 = pd.read_csv('test_ta_l1.csv', engine='python', on_bad_lines='skip')\n",
    "d2\n",
    "d2 = d2.rename(columns={'key' : 'unique_id', 'sentence' : 'text'})\n",
    "d2.to_csv('updated_test_ta_l1.csv', index=False)\n",
    "# d2\n",
    "# Convert annotator columns to numeric without replacing NaNs\n",
    "d2[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5', 'ta_a6']] = d2[\n",
    "    ['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5', 'ta_a6']\n",
    "].apply(pd.to_numeric, errors='coerce')  # NaNs are retained\n",
    "\n",
    "# Compute 'label' based on majority voting while ignoring NaNs\n",
    "d2['label'] = (\n",
    "    d2[['ta_a1', 'ta_a2', 'ta_a3', 'ta_a4', 'ta_a5', 'ta_a6']].mean(axis=1, skipna=True) >= 0.5\n",
    ").astype(int)\n",
    "\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0670aee5-6652-477b-8426-ebd85e2f6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.loc[:, 'binary_label'] = d2['label'].apply(determine_binary_label)\n",
    "\n",
    "# # Reorder columns\n",
    "d2 = d2[['unique_id', 'text', 'binary_label', 'label']]\n",
    "\n",
    "d2.to_csv('updated_test_hi_l1.csv', index=False)\n",
    "\n",
    "d2.loc[:, 'text'] = d2['text'].apply(lambda x: normalize_text(x))\n",
    "processed_list = []\n",
    "for j in d2['text']:\n",
    "    process = j.replace('...','')\n",
    "    processed_list.append(process)\n",
    "\n",
    "df_processed = pd.DataFrame(processed_list)\n",
    "df_processed.columns = ['text']\n",
    "df_processed.head(n=5)\n",
    "\n",
    "X = list(df_processed['text'])\n",
    "y = d2[['label']].values\n",
    "\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding\n",
    "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "\n",
    "y = label_encoder.fit_transform(y.ravel())\n",
    "\n",
    "y = to_categorical(y, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f0dc9ce-6ef7-4298-bb3a-88e84b0245c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\\Test Results:\n",
      "Precision: 0.5779\n",
      "Recall: 0.5892\n",
      "weighted F1 Score: 0.5268\n",
      "macro F1 Score: 0.4945\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not_hate       0.59      0.89      0.71       778\n",
      "        hate       0.55      0.18      0.28       578\n",
      "\n",
      "    accuracy                           0.59      1356\n",
      "   macro avg       0.57      0.54      0.49      1356\n",
      "weighted avg       0.58      0.59      0.53      1356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = evaluate_validation(trained_model, X, y)\n",
    "\n",
    "print(r\"\\Test Results:\")\n",
    "print(f\"Precision: {val_results['precision']:.4f}\")\n",
    "print(f\"Recall: {val_results['recall']:.4f}\")\n",
    "print(f\"weighted F1 Score: {val_results['f1_score_weighted']:.4f}\")\n",
    "print(f\"macro F1 Score: {val_results['f1_score_macro']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(val_results['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c84d1-331e-43c6-9885-f184209f6766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
