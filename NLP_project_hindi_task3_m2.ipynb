{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9427ae6-8f3b-47a2-bc02-c11052e049de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from skmultilearn.adapt import MLkNN\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import hamming_loss, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a806a63c-472d-4203-8635-0ed0cf477185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged dataset shape: (6196, 6)\n",
      "  unique_id                                               text binary_label_1  \\\n",
      "0      id_0    भारत में तीन किसान कानून बिल वापस लेने पर भक...       not_hate   \n",
      "1      id_1    राजस्थान  अजीबोगरीब: गांव की खुशहाली के लिए ...       not_hate   \n",
      "2      id_2    सलमान ने की राखी के विरोधी की वकालत तो 'आइटम...       not_hate   \n",
      "3      id_3  !!हर शब्द अमॄतम!!  पुरानी एक कहावत है... टूटी ...       not_hate   \n",
      "4      id_4  \"PM मोदी जी \" की माँ के लिए अपशब्द कहने वाला द...           hate   \n",
      "\n",
      "   label_1 binary_label_3  label_3  \n",
      "0        0   not_explicit        0  \n",
      "1        0   not_explicit        0  \n",
      "2        0   not_explicit        0  \n",
      "3        0   not_explicit        0  \n",
      "4        1       explicit        1  \n",
      "\n",
      "Label distribution:\n",
      "Label 1 (gendered abuse): {'not_hate': 4426, 'hate': 1770}\n",
      "Label 3 (explicit/aggressive): {'not_explicit': 3320, 'explicit': 2876}\n"
     ]
    }
   ],
   "source": [
    "# Process label 1 dataset for Hindi\n",
    "df_l1 = pd.read_csv('train_hi_l1.csv')\n",
    "df_l1 = df_l1.rename(columns={'key': 'unique_id', 'sentence': 'text'})\n",
    "\n",
    "# Convert annotator columns to numeric without replacing NaNs (only 5 annotators)\n",
    "df_l1[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']] = df_l1[\n",
    "    ['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']\n",
    "].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Compute 'label_1' based on majority voting while ignoring NaNs\n",
    "df_l1['label_1'] = (df_l1[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']].mean(axis=1, skipna=True) >= 0.5).astype(int)\n",
    "\n",
    "# Create proper binary label for task 1 (gendered abuse)\n",
    "df_l1['binary_label_1'] = df_l1['label_1'].apply(lambda x: 'hate' if x == 1 else 'not_hate')\n",
    "\n",
    "# Process label 3 dataset for Hindi\n",
    "df_l3 = pd.read_csv('train_hi_l3.csv')\n",
    "df_l3 = df_l3.rename(columns={'key': 'unique_id', 'sentence': 'text'})\n",
    "\n",
    "# Convert annotator columns to numeric without replacing NaNs (only 5 annotators)\n",
    "df_l3[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']] = df_l3[\n",
    "    ['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']\n",
    "].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Compute 'label_3' based on majority voting while ignoring NaNs\n",
    "df_l3['label_3'] = (df_l3[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']].mean(axis=1, skipna=True) >= 0.5).astype(int)\n",
    "\n",
    "# Create proper binary label for task 3 (explicit/aggressive)\n",
    "df_l3['binary_label_3'] = df_l3['label_3'].apply(lambda x: 'explicit' if x == 1 else 'not_explicit')\n",
    "\n",
    "# Select columns for merging\n",
    "df_l1_slim = df_l1[['text', 'label_1', 'binary_label_1']]\n",
    "df_l3_slim = df_l3[['text', 'label_3', 'binary_label_3']]\n",
    "\n",
    "# Merge the datasets based on text field\n",
    "merged_df = pd.merge(df_l1_slim, df_l3_slim, on='text', how='inner')\n",
    "\n",
    "# Add a unique_id column to the merged dataset\n",
    "merged_df['unique_id'] = [f'id_{i}' for i in range(len(merged_df))]\n",
    "\n",
    "# Reorder columns\n",
    "merged_df = merged_df[['unique_id', 'text', 'binary_label_1', 'label_1', 'binary_label_3', 'label_3']]\n",
    "\n",
    "# Save the merged dataset\n",
    "merged_df.to_csv('train_hi_task3.csv', index=False)\n",
    "\n",
    "# Display information\n",
    "print(f\"Final merged dataset shape: {merged_df.shape}\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# Check label distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(f\"Label 1 (gendered abuse): {merged_df['binary_label_1'].value_counts().to_dict()}\")\n",
    "print(f\"Label 3 (explicit/aggressive): {merged_df['binary_label_3'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56e57892-9673-4973-845d-1c683eb71b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original vs Processed Text Samples:\n",
      "Original:   भारत में तीन किसान कानून बिल वापस लेने पर भक्तों की जो हालत हुई अब यह किसी से छुपा नहीं है उसके इलावा जो टट्टी खोर पत्रकार थे उनकी भी हालत खराब हुई है किसी को मुंह दिखाने के लायक नहीं है अब यह लोग यहां तक कि अपने घर में भी मुंह दिखाने के लायक नहीं रहे।\n",
      "Processed:   भारत में तीन किसान कानून बिल वापस लेने पर भक्तों की जो हालत हुई अब यह किसी से छुपा नहीं है उसके इलावा जो टट्टी खोर पत्रकार थे उनकी भी हालत खराब हुई है किसी को मुंह दिखाने के लायक नहीं है अब यह लोग यहां तक कि अपने घर में भी मुंह दिखाने के लायक नहीं रहे।\n",
      "--------------------------------------------------\n",
      "Original:   राजस्थान  अजीबोगरीब: गांव की खुशहाली के लिए रात भर किन्नर करते है यह काम...\n",
      "Processed:   राजस्थान  अजीबोगरीब  गांव की खुशहाली के लिए रात भर किन्नर करते है यह काम   \n",
      "--------------------------------------------------\n",
      "Original:   सलमान ने की राखी के विरोधी की वकालत तो 'आइटम गर्ल' ने दी धमकी! #RakhiSawant #SalmanKhan\n",
      "Processed:   सलमान ने की राखी के विरोधी की वकालत तो  आइटम गर्ल  ने दी धमकी   rakhisawant  salmankhan\n",
      "--------------------------------------------------\n",
      "\n",
      "Processed dataset shape: (6196, 7)\n",
      "  unique_id                                               text  \\\n",
      "0      id_0    भारत में तीन किसान कानून बिल वापस लेने पर भक...   \n",
      "1      id_1    राजस्थान  अजीबोगरीब: गांव की खुशहाली के लिए ...   \n",
      "2      id_2    सलमान ने की राखी के विरोधी की वकालत तो 'आइटम...   \n",
      "3      id_3  !!हर शब्द अमॄतम!!  पुरानी एक कहावत है... टूटी ...   \n",
      "4      id_4  \"PM मोदी जी \" की माँ के लिए अपशब्द कहने वाला द...   \n",
      "\n",
      "                                      processed_text binary_label_1  label_1  \\\n",
      "0    भारत में तीन किसान कानून बिल वापस लेने पर भक...       not_hate        0   \n",
      "1    राजस्थान  अजीबोगरीब  गांव की खुशहाली के लिए ...       not_hate        0   \n",
      "2    सलमान ने की राखी के विरोधी की वकालत तो  आइटम...       not_hate        0   \n",
      "3    हर शब्द अमॄतम    पुरानी एक कहावत है    टूटी ...       not_hate        0   \n",
      "4   pm मोदी जी   की माँ के लिए अपशब्द कहने वाला द...           hate        1   \n",
      "\n",
      "  binary_label_3  label_3  \n",
      "0   not_explicit        0  \n",
      "1   not_explicit        0  \n",
      "2   not_explicit        0  \n",
      "3   not_explicit        0  \n",
      "4       explicit        1  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"\n",
    "                               u\"\\U0001F700-\\U0001F77F\"\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', ' ', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'<.*?>+', ' ', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', ' ', text)\n",
    "    text = re.sub(r'<handle replaced>', '', text)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "# Load the multi-task dataset\n",
    "merged_df = pd.read_csv('train_hi_task3.csv')\n",
    "\n",
    "# Apply text normalization\n",
    "merged_df['processed_text'] = merged_df['text'].apply(lambda x: normalize_text(x))\n",
    "\n",
    "# Further processing to remove '...'\n",
    "merged_df['processed_text'] = merged_df['processed_text'].str.replace('...', '')\n",
    "\n",
    "# Display samples of processed text\n",
    "print(\"\\nOriginal vs Processed Text Samples:\")\n",
    "for i in range(3):\n",
    "    print(f\"Original: {merged_df['text'].iloc[i]}\")\n",
    "    print(f\"Processed: {merged_df['processed_text'].iloc[i]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Keep all columns but add processed text\n",
    "merged_df_final = merged_df[['unique_id', 'text', 'processed_text', 'binary_label_1', 'label_1', 'binary_label_3', 'label_3']]\n",
    "\n",
    "\n",
    "print(f\"\\nProcessed dataset shape: {merged_df_final.shape}\")\n",
    "print(merged_df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d27d15ae-3610-4e60-bb2c-9a5ee63ef930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (processed text)\n",
    "X = list(merged_df_final['processed_text'])\n",
    "\n",
    "# Extract labels for both tasks\n",
    "y_task1 = merged_df_final['label_1'].values\n",
    "y_task3 = merged_df_final['label_3'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "107b18f1-2c47-4cfe-8236-dae8496b6772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  63    5  896 ...    0    0    0]\n",
      " [ 629 1047    3 ...    0    0    0]\n",
      " [1783   21    3 ...    0    0    0]\n",
      " ...\n",
      " [   5  357 3995 ...    0    0    0]\n",
      " [   6  201 1216 ...    0    0    0]\n",
      " [ 205  205  688 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Activation, Dropout, Dense, Flatten,\n",
    "    Bidirectional, GRU, concatenate, SpatialDropout1D,\n",
    "    GlobalMaxPooling1D, GlobalAveragePooling1D, Conv1D,\n",
    "    Embedding, Input, Concatenate\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "######## Textual Features for Embedding ###################\n",
    "max_len = 100\n",
    "max_features = 4479\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding\n",
    "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "\n",
    "print(X)  # Check the processed sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7dd867b-3d19-4903-920b-591ceb9144ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_task1 = label_encoder.fit_transform(y_task1)\n",
    "y_task3 = label_encoder.fit_transform(y_task3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "521f9012-2aea-415f-8a7f-3d586bec4693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_task1 = to_categorical(y_task1, num_classes=2)\n",
    "y_task3 = to_categorical(y_task3, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3a062cb-b02c-450b-9f76-8cacc13245a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (4479, 50)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load GloVe embeddings from JSON\n",
    "with open('glove_embeddings.json', encoding=\"utf8\") as f:\n",
    "    embeddings_list = json.load(f)\n",
    "\n",
    "# Convert the list of vectors to a dictionary with word indices as keys\n",
    "embeddings_dictionary = {str(i): vector for i, vector in enumerate(embeddings_list)}\n",
    "\n",
    "# Define tokenizer \n",
    "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_features, vocab_size)  # Limit vocab to max_features\n",
    "\n",
    "# Get embedding dimension (from first vector in list)\n",
    "embed_size = len(embeddings_list[0]) if embeddings_list else 0\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "\n",
    "# Fill embedding matrix with corresponding word vectors\n",
    "for word, index in word_index.items():\n",
    "    if index >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_dictionary.get(word) or embeddings_dictionary.get(str(index))\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = np.asarray(embedding_vector, dtype=np.float32)\n",
    "\n",
    "print(\"Embedding matrix shape:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12bd7dc9-5f63-4038-9b6a-912dffa09c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4956\n",
      "Validation samples: 1240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krmri\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">223,950</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ spatial_dropout1d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)            │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">138,240</span> │ spatial_dropout1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional_1               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">123,648</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)               │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ multi_head_attention          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)          │                           │                 │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│                               │                           │                 │ multi_head_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_average_pooling1d      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                               │                           │                 │ global_max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task1_dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task3_dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task1_bn1                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ task1_dense1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task3_bn1                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ task3_dense1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ task1_bn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ task3_bn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task1_dense2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task3_dense2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task1_bn2                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ task1_dense2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task3_bn2                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ task3_dense2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ task1_bn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ task3_bn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task1_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task3_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m50\u001b[0m)           │         \u001b[38;5;34m223,950\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ spatial_dropout1d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m50\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)            │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)          │         \u001b[38;5;34m138,240\u001b[0m │ spatial_dropout1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bidirectional_1               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │         \u001b[38;5;34m123,648\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)               │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ multi_head_attention          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │          \u001b[38;5;34m66,048\u001b[0m │ bidirectional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)          │                           │                 │ bidirectional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │ bidirectional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│                               │                           │                 │ multi_head_attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_average_pooling1d      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ global_max_pooling1d          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate_1 (\u001b[38;5;33mConcatenate\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ global_average_pooling1d[\u001b[38;5;34m…\u001b[0m │\n",
       "│                               │                           │                 │ global_max_pooling1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task1_dense1 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │         \u001b[38;5;34m131,328\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task3_dense1 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │         \u001b[38;5;34m131,328\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task1_bn1                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │           \u001b[38;5;34m1,024\u001b[0m │ task1_dense1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task3_bn1                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │           \u001b[38;5;34m1,024\u001b[0m │ task3_dense1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ task1_bn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ task3_bn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task1_dense2 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │          \u001b[38;5;34m32,896\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task3_dense2 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │          \u001b[38;5;34m32,896\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task1_bn2                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │             \u001b[38;5;34m512\u001b[0m │ task1_dense2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task3_bn2                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │             \u001b[38;5;34m512\u001b[0m │ task3_dense2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ task1_bn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ task3_bn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task1_output (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                 │             \u001b[38;5;34m258\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ task3_output (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                 │             \u001b[38;5;34m258\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">883,922</span> (3.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m883,922\u001b[0m (3.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">882,386</span> (3.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m882,386\u001b[0m (3.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 1.8256 - task1_output_accuracy: 0.5667 - task1_output_loss: 0.9057 - task1_output_macro_f1_score: 0.5667 - task3_output_accuracy: 0.5335 - task3_output_loss: 0.9181 - task3_output_macro_f1_score: 0.5335 \n",
      "Epoch 1: val_loss improved from inf to 1.31090, saving model to models_hi_multi_task_gru\\best_model_hi_multi_task_gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 229ms/step - loss: 1.8243 - task1_output_accuracy: 0.5670 - task1_output_loss: 0.9049 - task1_output_macro_f1_score: 0.5670 - task3_output_accuracy: 0.5335 - task3_output_loss: 0.9177 - task3_output_macro_f1_score: 0.5335 - val_loss: 1.3109 - val_task1_output_accuracy: 0.7145 - val_task1_output_loss: 0.6224 - val_task1_output_macro_f1_score: 0.7145 - val_task3_output_accuracy: 0.5258 - val_task3_output_loss: 0.6864 - val_task3_output_macro_f1_score: 0.5258\n",
      "Epoch 2/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - loss: 1.4537 - task1_output_accuracy: 0.6556 - task1_output_loss: 0.6830 - task1_output_macro_f1_score: 0.6556 - task3_output_accuracy: 0.5363 - task3_output_loss: 0.7689 - task3_output_macro_f1_score: 0.5363 \n",
      "Epoch 2: val_loss improved from 1.31090 to 1.28674, saving model to models_hi_multi_task_gru\\best_model_hi_multi_task_gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 214ms/step - loss: 1.4535 - task1_output_accuracy: 0.6557 - task1_output_loss: 0.6829 - task1_output_macro_f1_score: 0.6557 - task3_output_accuracy: 0.5363 - task3_output_loss: 0.7688 - task3_output_macro_f1_score: 0.5363 - val_loss: 1.2867 - val_task1_output_accuracy: 0.7145 - val_task1_output_loss: 0.5986 - val_task1_output_macro_f1_score: 0.7145 - val_task3_output_accuracy: 0.5298 - val_task3_output_loss: 0.6862 - val_task3_output_macro_f1_score: 0.5298\n",
      "Epoch 3/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 1.3735 - task1_output_accuracy: 0.6708 - task1_output_loss: 0.6536 - task1_output_macro_f1_score: 0.6708 - task3_output_accuracy: 0.5614 - task3_output_loss: 0.7181 - task3_output_macro_f1_score: 0.5614 \n",
      "Epoch 3: val_loss improved from 1.28674 to 1.27309, saving model to models_hi_multi_task_gru\\best_model_hi_multi_task_gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 206ms/step - loss: 1.3734 - task1_output_accuracy: 0.6708 - task1_output_loss: 0.6535 - task1_output_macro_f1_score: 0.6708 - task3_output_accuracy: 0.5614 - task3_output_loss: 0.7181 - task3_output_macro_f1_score: 0.5614 - val_loss: 1.2731 - val_task1_output_accuracy: 0.7169 - val_task1_output_loss: 0.5931 - val_task1_output_macro_f1_score: 0.7169 - val_task3_output_accuracy: 0.5935 - val_task3_output_loss: 0.6788 - val_task3_output_macro_f1_score: 0.5935\n",
      "Epoch 4/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - loss: 1.3304 - task1_output_accuracy: 0.6839 - task1_output_loss: 0.6321 - task1_output_macro_f1_score: 0.6839 - task3_output_accuracy: 0.5533 - task3_output_loss: 0.6966 - task3_output_macro_f1_score: 0.5533 \n",
      "Epoch 4: val_loss did not improve from 1.27309\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 213ms/step - loss: 1.3304 - task1_output_accuracy: 0.6840 - task1_output_loss: 0.6320 - task1_output_macro_f1_score: 0.6840 - task3_output_accuracy: 0.5534 - task3_output_loss: 0.6965 - task3_output_macro_f1_score: 0.5534 - val_loss: 1.2940 - val_task1_output_accuracy: 0.7145 - val_task1_output_loss: 0.5963 - val_task1_output_macro_f1_score: 0.7145 - val_task3_output_accuracy: 0.5403 - val_task3_output_loss: 0.6960 - val_task3_output_macro_f1_score: 0.5403\n",
      "Epoch 5/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - loss: 1.3011 - task1_output_accuracy: 0.6962 - task1_output_loss: 0.6201 - task1_output_macro_f1_score: 0.6962 - task3_output_accuracy: 0.5883 - task3_output_loss: 0.6792 - task3_output_macro_f1_score: 0.5883 \n",
      "Epoch 5: val_loss did not improve from 1.27309\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 219ms/step - loss: 1.3010 - task1_output_accuracy: 0.6962 - task1_output_loss: 0.6201 - task1_output_macro_f1_score: 0.6962 - task3_output_accuracy: 0.5884 - task3_output_loss: 0.6792 - task3_output_macro_f1_score: 0.5884 - val_loss: 1.2813 - val_task1_output_accuracy: 0.6806 - val_task1_output_loss: 0.6059 - val_task1_output_macro_f1_score: 0.6806 - val_task3_output_accuracy: 0.6032 - val_task3_output_loss: 0.6741 - val_task3_output_macro_f1_score: 0.6032\n",
      "Epoch 6/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - loss: 1.2352 - task1_output_accuracy: 0.6970 - task1_output_loss: 0.5931 - task1_output_macro_f1_score: 0.6970 - task3_output_accuracy: 0.6421 - task3_output_loss: 0.6403 - task3_output_macro_f1_score: 0.6421 \n",
      "Epoch 6: val_loss improved from 1.27309 to 1.23975, saving model to models_hi_multi_task_gru\\best_model_hi_multi_task_gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 231ms/step - loss: 1.2352 - task1_output_accuracy: 0.6970 - task1_output_loss: 0.5931 - task1_output_macro_f1_score: 0.6970 - task3_output_accuracy: 0.6421 - task3_output_loss: 0.6403 - task3_output_macro_f1_score: 0.6421 - val_loss: 1.2398 - val_task1_output_accuracy: 0.7185 - val_task1_output_loss: 0.5876 - val_task1_output_macro_f1_score: 0.7185 - val_task3_output_accuracy: 0.6282 - val_task3_output_loss: 0.6511 - val_task3_output_macro_f1_score: 0.6282\n",
      "Epoch 7/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - loss: 1.1679 - task1_output_accuracy: 0.7044 - task1_output_loss: 0.5760 - task1_output_macro_f1_score: 0.7044 - task3_output_accuracy: 0.6803 - task3_output_loss: 0.5901 - task3_output_macro_f1_score: 0.6803 \n",
      "Epoch 7: val_loss improved from 1.23975 to 1.19863, saving model to models_hi_multi_task_gru\\best_model_hi_multi_task_gru.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 230ms/step - loss: 1.1677 - task1_output_accuracy: 0.7045 - task1_output_loss: 0.5759 - task1_output_macro_f1_score: 0.7045 - task3_output_accuracy: 0.6804 - task3_output_loss: 0.5900 - task3_output_macro_f1_score: 0.6804 - val_loss: 1.1986 - val_task1_output_accuracy: 0.7274 - val_task1_output_loss: 0.5615 - val_task1_output_macro_f1_score: 0.7274 - val_task3_output_accuracy: 0.6597 - val_task3_output_loss: 0.6351 - val_task3_output_macro_f1_score: 0.6597\n",
      "Epoch 8/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - loss: 1.0640 - task1_output_accuracy: 0.7380 - task1_output_loss: 0.5214 - task1_output_macro_f1_score: 0.7380 - task3_output_accuracy: 0.7230 - task3_output_loss: 0.5408 - task3_output_macro_f1_score: 0.7230 \n",
      "Epoch 8: val_loss did not improve from 1.19863\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 247ms/step - loss: 1.0640 - task1_output_accuracy: 0.7379 - task1_output_loss: 0.5215 - task1_output_macro_f1_score: 0.7379 - task3_output_accuracy: 0.7230 - task3_output_loss: 0.5407 - task3_output_macro_f1_score: 0.7230 - val_loss: 1.2658 - val_task1_output_accuracy: 0.7395 - val_task1_output_loss: 0.5711 - val_task1_output_macro_f1_score: 0.7395 - val_task3_output_accuracy: 0.6452 - val_task3_output_loss: 0.6939 - val_task3_output_macro_f1_score: 0.6452\n",
      "Epoch 9/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - loss: 0.9987 - task1_output_accuracy: 0.7411 - task1_output_loss: 0.5034 - task1_output_macro_f1_score: 0.7411 - task3_output_accuracy: 0.7575 - task3_output_loss: 0.4935 - task3_output_macro_f1_score: 0.7575 \n",
      "Epoch 9: val_loss did not improve from 1.19863\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 252ms/step - loss: 0.9986 - task1_output_accuracy: 0.7412 - task1_output_loss: 0.5033 - task1_output_macro_f1_score: 0.7412 - task3_output_accuracy: 0.7576 - task3_output_loss: 0.4935 - task3_output_macro_f1_score: 0.7576 - val_loss: 1.3909 - val_task1_output_accuracy: 0.6597 - val_task1_output_loss: 0.6154 - val_task1_output_macro_f1_score: 0.6597 - val_task3_output_accuracy: 0.6548 - val_task3_output_loss: 0.7739 - val_task3_output_macro_f1_score: 0.6548\n",
      "Epoch 10/15\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - loss: 0.9393 - task1_output_accuracy: 0.7510 - task1_output_loss: 0.4874 - task1_output_macro_f1_score: 0.7510 - task3_output_accuracy: 0.7877 - task3_output_loss: 0.4501 - task3_output_macro_f1_score: 0.7877 \n",
      "Epoch 10: val_loss did not improve from 1.19863\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 252ms/step - loss: 0.9392 - task1_output_accuracy: 0.7511 - task1_output_loss: 0.4873 - task1_output_macro_f1_score: 0.7511 - task3_output_accuracy: 0.7877 - task3_output_loss: 0.4501 - task3_output_macro_f1_score: 0.7877 - val_loss: 1.3271 - val_task1_output_accuracy: 0.7306 - val_task1_output_loss: 0.6037 - val_task1_output_macro_f1_score: 0.7306 - val_task3_output_accuracy: 0.6524 - val_task3_output_loss: 0.7223 - val_task3_output_macro_f1_score: 0.6524\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 83ms/step\n",
      "\n",
      "Task 1 (Gendered Abuse) Validation Results:\n",
      "Precision: 0.6959\n",
      "Recall: 0.7274\n",
      "weighted F1 Score: 0.6632\n",
      "macro F1 Score: 0.5347\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not_hate       0.74      0.96      0.83       886\n",
      "        hate       0.59      0.15      0.24       354\n",
      "\n",
      "    accuracy                           0.73      1240\n",
      "   macro avg       0.66      0.55      0.53      1240\n",
      "weighted avg       0.70      0.73      0.66      1240\n",
      "\n",
      "\n",
      "Task 3 (Explicit Language) Validation Results:\n",
      "Precision: 0.6624\n",
      "Recall: 0.6597\n",
      "weighted F1 Score: 0.6598\n",
      "macro F1 Score: 0.6597\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "not_explicit       0.69      0.63      0.66       652\n",
      "    explicit       0.63      0.69      0.66       588\n",
      "\n",
      "    accuracy                           0.66      1240\n",
      "   macro avg       0.66      0.66      0.66      1240\n",
      "weighted avg       0.66      0.66      0.66      1240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, SpatialDropout1D, Conv1D,\n",
    "    Bidirectional, LSTM, GRU, Dense, Dropout,\n",
    "    GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Configure GPU for optimal performance\n",
    "def configure_gpu():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Enable memory growth for each GPU\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "            # Use mixed precision for better performance\n",
    "            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "            tf.keras.mixed_precision.set_global_policy(policy)\n",
    "            print('Mixed precision enabled')\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "configure_gpu()\n",
    "\n",
    "# Multi-Task Model Definition with GRU and Attention\n",
    "def create_multi_task_gru_attention_model(max_len, max_features, embedding_matrix, embed_size=300):\n",
    "    \"\"\"\n",
    "    Creates a multi-task GRU model with hierarchical attention mechanism for joint prediction of\n",
    "    gendered abuse (task1) and explicit language (task3)\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(max_len,))\n",
    "    \n",
    "    # Embedding layer with pretrained weights\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=max_features,\n",
    "        output_dim=embed_size,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_len,\n",
    "        trainable=True  # Make embeddings trainable for fine-tuning\n",
    "    )(input_layer)\n",
    "    \n",
    "    # Spatial Dropout with higher rate\n",
    "    spatial_dropout = SpatialDropout1D(0.3)(embedding_layer)\n",
    "    \n",
    "    # Multiple GRU layers with different window sizes\n",
    "    gru_layer1 = Bidirectional(\n",
    "        GRU(\n",
    "            units=128,\n",
    "            return_sequences=True,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(1e-5)\n",
    "        )\n",
    "    )(spatial_dropout)\n",
    "    \n",
    "    gru_layer2 = Bidirectional(\n",
    "        GRU(\n",
    "            units=64,\n",
    "            return_sequences=True,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2\n",
    "        )\n",
    "    )(gru_layer1)\n",
    "    \n",
    "    # Multi-head self-attention (simplified version)\n",
    "    attention_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=8,\n",
    "        key_dim=16\n",
    "    )(gru_layer2, gru_layer2)\n",
    "    \n",
    "    # Skip connection\n",
    "    concat_layer = tf.keras.layers.Concatenate()([gru_layer2, attention_layer])\n",
    "    \n",
    "    # Feature extraction with pooling operations\n",
    "    avg_pool = GlobalAveragePooling1D()(concat_layer)\n",
    "    max_pool = GlobalMaxPooling1D()(concat_layer)\n",
    "    \n",
    "    # Combine pooled features\n",
    "    shared_features = tf.keras.layers.Concatenate()([avg_pool, max_pool])\n",
    "    \n",
    "    # Task-specific layers for Task 1 (Gendered Abuse)\n",
    "    task1_dense1 = Dense(256, activation='relu', name='task1_dense1')(shared_features)\n",
    "    task1_bn1 = tf.keras.layers.BatchNormalization(name='task1_bn1')(task1_dense1)\n",
    "    task1_dropout1 = Dropout(0.3)(task1_bn1)\n",
    "    \n",
    "    task1_dense2 = Dense(128, activation='relu', name='task1_dense2')(task1_dropout1)\n",
    "    task1_bn2 = tf.keras.layers.BatchNormalization(name='task1_bn2')(task1_dense2)\n",
    "    task1_dropout2 = Dropout(0.2)(task1_bn2)\n",
    "    \n",
    "    task1_output = Dense(2, activation='softmax', name='task1_output', dtype='float32')(task1_dropout2)\n",
    "    \n",
    "    # Task-specific layers for Task 3 (Explicit Language)\n",
    "    task3_dense1 = Dense(256, activation='relu', name='task3_dense1')(shared_features)\n",
    "    task3_bn1 = tf.keras.layers.BatchNormalization(name='task3_bn1')(task3_dense1)\n",
    "    task3_dropout1 = Dropout(0.3)(task3_bn1)\n",
    "    \n",
    "    task3_dense2 = Dense(128, activation='relu', name='task3_dense2')(task3_dropout1)\n",
    "    task3_bn2 = tf.keras.layers.BatchNormalization(name='task3_bn2')(task3_dense2)\n",
    "    task3_dropout2 = Dropout(0.2)(task3_bn2)\n",
    "    \n",
    "    task3_output = Dense(2, activation='softmax', name='task3_output', dtype='float32')(task3_dropout2)\n",
    "    \n",
    "    # Create model with multiple outputs\n",
    "    model = Model(inputs=input_layer, outputs=[task1_output, task3_output])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Custom MacroF1Score Metric for multi-task learning\n",
    "class MacroF1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes=2, name='macro_f1_score', **kwargs):\n",
    "        super(MacroF1Score, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Convert probabilities to predicted class indices\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        \n",
    "        # Convert one-hot encoded y_true to class indices if needed\n",
    "        if len(y_true.shape) > 1 and y_true.shape[-1] > 1:\n",
    "            y_true = tf.argmax(y_true, axis=-1)\n",
    "        \n",
    "        # Initialize confusion matrix\n",
    "        conf_matrix = tf.math.confusion_matrix(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            num_classes=self.num_classes,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        # Calculate TP, FP, FN for each class\n",
    "        diag = tf.linalg.diag_part(conf_matrix)\n",
    "        row_sum = tf.reduce_sum(conf_matrix, axis=1)\n",
    "        col_sum = tf.reduce_sum(conf_matrix, axis=0)\n",
    "        \n",
    "        tp = diag\n",
    "        fp = col_sum - diag\n",
    "        fn = row_sum - diag\n",
    "        \n",
    "        # Update the state variables\n",
    "        self.tp.assign_add(tf.reduce_sum(tp))\n",
    "        self.fp.assign_add(tf.reduce_sum(fp))\n",
    "        self.fn.assign_add(tf.reduce_sum(fn))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        # Calculate precision and recall\n",
    "        precision = self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())\n",
    "        recall = self.tp / (self.tp + self.fn + tf.keras.backend.epsilon())\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "        \n",
    "        # Return macro F1 (average of per-class F1 scores)\n",
    "        return f1\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.tp.assign(0.)\n",
    "        self.fp.assign(0.)\n",
    "        self.fn.assign(0.)\n",
    "        self.count.assign(0.)\n",
    "\n",
    "# Model Training for multi-task learning\n",
    "def train_and_validate_multi_task_model(model, X_train, y_train_task1, y_train_task3, \n",
    "                                         X_val, y_val_task1, y_val_task3, \n",
    "                                         batch_size=32, epochs=15, model_dir='models_hi_multi_task_gru'):\n",
    "    \"\"\"\n",
    "    Trains the multi-task GRU-Attention model with early stopping and model checkpointing\n",
    "    Returns the best model and training history\n",
    "    \"\"\"\n",
    "    # Create directory for saving models if it doesn't exist\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        os.path.join(model_dir, 'best_model_hi_multi_task_gru.h5'),\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Compile model with Adam optimizer\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss={\n",
    "            'task1_output': 'categorical_crossentropy',\n",
    "            'task3_output': 'categorical_crossentropy'\n",
    "        },\n",
    "        loss_weights={\n",
    "            'task1_output': 1.0,  # Weight for gendered abuse task\n",
    "            'task3_output': 1.0   # Weight for explicit language task\n",
    "        },\n",
    "        metrics={\n",
    "            'task1_output': ['accuracy', MacroF1Score(num_classes=2)],\n",
    "            'task3_output': ['accuracy', MacroF1Score(num_classes=2)]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, \n",
    "        {'task1_output': y_train_task1, 'task3_output': y_train_task3},\n",
    "        validation_data=(\n",
    "            X_val, \n",
    "            {'task1_output': y_val_task1, 'task3_output': y_val_task3}\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping, model_checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Load the best model found during training\n",
    "    best_model = load_model(\n",
    "        os.path.join(model_dir, 'best_model_hi_multi_task_gru.h5'), \n",
    "        custom_objects={'MacroF1Score': MacroF1Score}\n",
    "    )\n",
    "    \n",
    "    return history, best_model\n",
    "\n",
    "# Plot Training History for multi-task model\n",
    "def plot_multi_task_training_history(history, plot_dir='plots_hi_multi_task_gru'):\n",
    "    \"\"\"\n",
    "    Plots training history for both tasks (accuracy and loss curves)\n",
    "    Saves plots to specified directory\n",
    "    \"\"\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    # Task 1 (Gendered Abuse) plots\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot task1 accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history.history['task1_output_accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_task1_output_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Task 1 (Gendered Abuse) Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot task1 loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history.history['task1_output_loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_task1_output_loss'], label='Validation Loss')\n",
    "    plt.title('Task 1 (Gendered Abuse) Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Task 3 (Explicit Language) plots\n",
    "    # Plot task3 accuracy\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history.history['task3_output_accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_task3_output_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Task 3 (Explicit Language) Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot task3 loss\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(history.history['task3_output_loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_task3_output_loss'], label='Validation Loss')\n",
    "    plt.title('Task 3 (Explicit Language) Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, 'training_history_hi_multi_task_gru.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot combined loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Total Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Total Validation Loss')\n",
    "    plt.title('Multi-Task Model Combined Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, 'combined_loss_hi_multi_task_gru.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot macro F1 scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['task1_output_macro_f1_score'], label='Task1 Train F1')\n",
    "    plt.plot(history.history['val_task1_output_macro_f1_score'], label='Task1 Val F1')\n",
    "    plt.plot(history.history['task3_output_macro_f1_score'], label='Task3 Train F1')\n",
    "    plt.plot(history.history['val_task3_output_macro_f1_score'], label='Task3 Val F1')\n",
    "    plt.title('Multi-Task Model Macro F1 Scores')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, 'f1_scores_hi_multi_task_gru.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Evaluation function for multi-task model\n",
    "def evaluate_multi_task_validation(model, X_val, y_val_task1, y_val_task3, plot_dir='plots_hi_multi_task_gru'):\n",
    "    \"\"\"\n",
    "    Evaluates the multi-task model on validation data and saves metrics and plots\n",
    "    \"\"\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_pred_task1, y_pred_task3 = model.predict(X_val, batch_size=32)\n",
    "    \n",
    "    # Convert to class labels\n",
    "    y_pred_task1_labels = np.argmax(y_pred_task1, axis=1)\n",
    "    y_true_task1_labels = np.argmax(y_val_task1, axis=1)\n",
    "    \n",
    "    y_pred_task3_labels = np.argmax(y_pred_task3, axis=1)\n",
    "    y_true_task3_labels = np.argmax(y_val_task3, axis=1)\n",
    "    \n",
    "    # Task 1 (Gendered Abuse) metrics\n",
    "    task1_precision = precision_score(y_true_task1_labels, y_pred_task1_labels, average='weighted')\n",
    "    task1_recall = recall_score(y_true_task1_labels, y_pred_task1_labels, average='weighted')\n",
    "    task1_weighted_f1 = f1_score(y_true_task1_labels, y_pred_task1_labels, average='weighted')\n",
    "    task1_macro_f1 = f1_score(y_true_task1_labels, y_pred_task1_labels, average='macro')\n",
    "    \n",
    "    # Task 3 (Explicit Language) metrics\n",
    "    task3_precision = precision_score(y_true_task3_labels, y_pred_task3_labels, average='weighted')\n",
    "    task3_recall = recall_score(y_true_task3_labels, y_pred_task3_labels, average='weighted')\n",
    "    task3_weighted_f1 = f1_score(y_true_task3_labels, y_pred_task3_labels, average='weighted')\n",
    "    task3_macro_f1 = f1_score(y_true_task3_labels, y_pred_task3_labels, average='macro')\n",
    "    \n",
    "    # Classification reports\n",
    "    task1_report = classification_report(y_true_task1_labels, y_pred_task1_labels, \n",
    "                                        target_names=['not_hate', 'hate'])\n",
    "    \n",
    "    task3_report = classification_report(y_true_task3_labels, y_pred_task3_labels, \n",
    "                                        target_names=['not_explicit', 'explicit'])\n",
    "    \n",
    "    # Confusion matrices\n",
    "    task1_conf_matrix = confusion_matrix(y_true_task1_labels, y_pred_task1_labels)\n",
    "    task3_conf_matrix = confusion_matrix(y_true_task3_labels, y_pred_task3_labels)\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    # Task 1 confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(task1_conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Not Hate', 'Hate'],\n",
    "                yticklabels=['Not Hate', 'Hate'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Task 1 (Gendered Abuse) Confusion Matrix')\n",
    "    plt.savefig(os.path.join(plot_dir, 'task1_confusion_matrix_hi_multi_task_gru.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Task 3 confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(task3_conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Not Explicit', 'Explicit'],\n",
    "                yticklabels=['Not Explicit', 'Explicit'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Task 3 (Explicit Language) Confusion Matrix')\n",
    "    plt.savefig(os.path.join(plot_dir, 'task3_confusion_matrix_hi_multi_task_gru.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'task1': {\n",
    "            'precision': task1_precision,\n",
    "            'recall': task1_recall,\n",
    "            'f1_score_weighted': task1_weighted_f1,\n",
    "            'f1_score_macro': task1_macro_f1,\n",
    "            'classification_report': task1_report,\n",
    "            'confusion_matrix': task1_conf_matrix\n",
    "        },\n",
    "        'task3': {\n",
    "            'precision': task3_precision,\n",
    "            'recall': task3_recall,\n",
    "            'f1_score_weighted': task3_weighted_f1,\n",
    "            'f1_score_macro': task3_macro_f1,\n",
    "            'classification_report': task3_report,\n",
    "            'confusion_matrix': task3_conf_matrix\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Main Execution for Training and Validation\n",
    "if __name__ == \"__main__\":\n",
    "    # Split into train (80%) and validation (20%)\n",
    "    X_train, X_val, y_train_task1, y_val_task1, y_train_task3, y_val_task3 = train_test_split(\n",
    "        X, y_task1, y_task3, test_size=0.2, random_state=42, stratify=y_task1\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Validation samples: {len(X_val)}\")\n",
    "    \n",
    "    # Create multi-task GRU-Attention model\n",
    "    embed_size = embedding_matrix.shape[1]\n",
    "    model = create_multi_task_gru_attention_model(max_len, max_features, embedding_matrix, embed_size)\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Train multi-task model\n",
    "    history, trained_model = train_and_validate_multi_task_model(\n",
    "        model, X_train, y_train_task1, y_train_task3, \n",
    "        X_val, y_val_task1, y_val_task3,\n",
    "        batch_size=32,\n",
    "        epochs=15  \n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_multi_task_training_history(history)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_results = evaluate_multi_task_validation(\n",
    "        trained_model, X_val, y_val_task1, y_val_task3\n",
    "    )\n",
    "    \n",
    "    # Print Task 1 (Gendered Abuse) results\n",
    "    print(\"\\nTask 1 (Gendered Abuse) Validation Results:\")\n",
    "    print(f\"Precision: {val_results['task1']['precision']:.4f}\")\n",
    "    print(f\"Recall: {val_results['task1']['recall']:.4f}\")\n",
    "    print(f\"weighted F1 Score: {val_results['task1']['f1_score_weighted']:.4f}\")\n",
    "    print(f\"macro F1 Score: {val_results['task1']['f1_score_macro']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(val_results['task1']['classification_report'])\n",
    "    \n",
    "    # Print Task 3 (Explicit Language) results\n",
    "    print(\"\\nTask 3 (Explicit Language) Validation Results:\")\n",
    "    print(f\"Precision: {val_results['task3']['precision']:.4f}\")\n",
    "    print(f\"Recall: {val_results['task3']['recall']:.4f}\")\n",
    "    print(f\"weighted F1 Score: {val_results['task3']['f1_score_weighted']:.4f}\")\n",
    "    print(f\"macro F1 Score: {val_results['task3']['f1_score_macro']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(val_results['task3']['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c670d942-8e97-4ea8-b847-00f627e9185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged dataset shape: (1517, 6)\n",
      "  unique_id                                               text binary_label_1  \\\n",
      "0      id_0  #BandraStation #SharadPawar #Muradabad  अगर अभ...       not_hate   \n",
      "1      id_1  #ConspiracyAgainstIndia  सुन लो रे देश के गद्द...       not_hate   \n",
      "2      id_2  #MarathaReservation : महाराष्ट्र में जश्न का म...       not_hate   \n",
      "3      id_3  #RheaChakraborty aap Mahesh bhatt se madad kiy...           hate   \n",
      "4      id_4  #SecularMaskOfd हमे पता नहि क्या कहते है ,इतना...       not_hate   \n",
      "\n",
      "   label_1 binary_label_3  label_3  \n",
      "0        0   not_explicit        0  \n",
      "1        0   not_explicit        0  \n",
      "2        0   not_explicit        0  \n",
      "3        1       explicit        1  \n",
      "4        0   not_explicit        0  \n",
      "\n",
      "Label distribution:\n",
      "Label 1 (gendered abuse): {'not_hate': 1159, 'hate': 358}\n",
      "Label 3 (explicit/aggressive): {'not_explicit': 1159, 'explicit': 358}\n"
     ]
    }
   ],
   "source": [
    "# Process label 1 dataset for Hindi\n",
    "df_l1 = pd.read_csv('test_hi_l1.csv', engine='python', on_bad_lines='skip')\n",
    "df_l1 = df_l1.rename(columns={'key': 'unique_id', 'sentence': 'text'})\n",
    "\n",
    "# Convert annotator columns to numeric without replacing NaNs (only 5 annotators)\n",
    "df_l1[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']] = df_l1[\n",
    "    ['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']\n",
    "].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Compute 'label_1' based on majority voting while ignoring NaNs\n",
    "df_l1['label_1'] = (df_l1[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']].mean(axis=1, skipna=True) >= 0.5).astype(int)\n",
    "\n",
    "# Create proper binary label for task 1 (gendered abuse)\n",
    "df_l1['binary_label_1'] = df_l1['label_1'].apply(lambda x: 'hate' if x == 1 else 'not_hate')\n",
    "\n",
    "# Process label 3 dataset for Hindi\n",
    "df_l3 = pd.read_csv('test_hi_l1.csv', engine='python', on_bad_lines='skip')\n",
    "df_l3 = df_l3.rename(columns={'key': 'unique_id', 'sentence': 'text'})\n",
    "\n",
    "# Convert annotator columns to numeric without replacing NaNs (only 5 annotators)\n",
    "df_l3[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']] = df_l3[\n",
    "    ['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']\n",
    "].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Compute 'label_3' based on majority voting while ignoring NaNs\n",
    "df_l3['label_3'] = (df_l3[['hi_a1', 'hi_a2', 'hi_a3', 'hi_a4', 'hi_a5']].mean(axis=1, skipna=True) >= 0.5).astype(int)\n",
    "\n",
    "# Create proper binary label for task 3 (explicit/aggressive)\n",
    "df_l3['binary_label_3'] = df_l3['label_3'].apply(lambda x: 'explicit' if x == 1 else 'not_explicit')\n",
    "\n",
    "# Select columns for merging\n",
    "df_l1_slim = df_l1[['text', 'label_1', 'binary_label_1']]\n",
    "df_l3_slim = df_l3[['text', 'label_3', 'binary_label_3']]\n",
    "\n",
    "# Merge the datasets based on text field\n",
    "merged_df = pd.merge(df_l1_slim, df_l3_slim, on='text', how='inner')\n",
    "\n",
    "# Add a unique_id column to the merged dataset\n",
    "merged_df['unique_id'] = [f'id_{i}' for i in range(len(merged_df))]\n",
    "\n",
    "# Reorder columns\n",
    "merged_df = merged_df[['unique_id', 'text', 'binary_label_1', 'label_1', 'binary_label_3', 'label_3']]\n",
    "\n",
    "# Save the merged dataset\n",
    "merged_df.to_csv('test_hi_task3.csv', index=False)\n",
    "\n",
    "# Display information\n",
    "print(f\"Final merged dataset shape: {merged_df.shape}\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# Check label distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(f\"Label 1 (gendered abuse): {merged_df['binary_label_1'].value_counts().to_dict()}\")\n",
    "print(f\"Label 3 (explicit/aggressive): {merged_df['binary_label_3'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a8e91d4-3863-493d-8097-af46e8ca143a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original vs Processed Text Samples:\n",
      "Original: #BandraStation #SharadPawar #Muradabad  अगर अभी आपको होश आ गया हो तो ये मान लीजिये की अब आप भारत के नागरिक नही है , अब आप आने वाले समय के शरिया कानून का पालन करते नजर आएंगे जहां काफिरों के लिए अलग क़ानून होगा और मुसलमानों के लिए अलग।  #गजवायेHind की शुरूआत हो चुकी है #10ईयर और।\n",
      "Processed:  bandrastation  sharadpawar  muradabad  अगर अभी आपको होश आ गया हो तो ये मान लीजिये की अब आप भारत के नागरिक नही है   अब आप आने वाले समय के शरिया कानून का पालन करते नजर आएंगे जहां काफिरों के लिए अलग क़ानून होगा और मुसलमानों के लिए अलग।   गजवायेhind की शुरूआत हो चुकी है    और।\n",
      "--------------------------------------------------\n",
      "Original: #ConspiracyAgainstIndia  सुन लो रे देश के गद्दारों घर के और बाहर वाले भी यदि तुम अभी भी नहीं सुधरे ना तो तुम्हारी गांड में छाता डालकर मोर बना दूंगा\n",
      "Processed:  conspiracyagainstindia  सुन लो रे देश के गद्दारों घर के और बाहर वाले भी यदि तुम अभी भी नहीं सुधरे ना तो तुम्हारी गांड में छाता डालकर मोर बना दूंगा\n",
      "--------------------------------------------------\n",
      "Original: #MarathaReservation : महाराष्ट्र में जश्न का माहौल  #Devendrafadnavis #Marathas #Maratha #Reservation <handle replaced><handle replaced><handle replaced><handle replaced><handle replaced><handle replaced><handle replaced><handle replaced><handle replaced>  \n",
      "Processed:  marathareservation   महाराष्ट्र में जश्न का माहौल   devendrafadnavis  marathas  maratha  reservation            \n",
      "--------------------------------------------------\n",
      "\n",
      "Processed dataset shape: (1517, 7)\n",
      "  unique_id                                               text  \\\n",
      "0      id_0  #BandraStation #SharadPawar #Muradabad  अगर अभ...   \n",
      "1      id_1  #ConspiracyAgainstIndia  सुन लो रे देश के गद्द...   \n",
      "2      id_2  #MarathaReservation : महाराष्ट्र में जश्न का म...   \n",
      "3      id_3  #RheaChakraborty aap Mahesh bhatt se madad kiy...   \n",
      "4      id_4  #SecularMaskOfd हमे पता नहि क्या कहते है ,इतना...   \n",
      "\n",
      "                                      processed_text binary_label_1  label_1  \\\n",
      "0   bandrastation  sharadpawar  muradabad  अगर अभ...       not_hate        0   \n",
      "1   conspiracyagainstindia  सुन लो रे देश के गद्द...       not_hate        0   \n",
      "2   marathareservation   महाराष्ट्र में जश्न का म...       not_hate        0   \n",
      "3   rheachakraborty aap mahesh bhatt se madad kiy...           hate        1   \n",
      "4   secularmaskofd हमे पता नहि क्या कहते है  इतना...       not_hate        0   \n",
      "\n",
      "  binary_label_3  label_3  \n",
      "0   not_explicit        0  \n",
      "1   not_explicit        0  \n",
      "2   not_explicit        0  \n",
      "3       explicit        1  \n",
      "4   not_explicit        0  \n"
     ]
    }
   ],
   "source": [
    "# Apply text normalization\n",
    "merged_df['processed_text'] = merged_df['text'].apply(lambda x: normalize_text(x))\n",
    "\n",
    "# Further processing to remove '...'\n",
    "merged_df['processed_text'] = merged_df['processed_text'].str.replace('...', '')\n",
    "\n",
    "# Display samples of processed text\n",
    "print(\"\\nOriginal vs Processed Text Samples:\")\n",
    "for i in range(3):\n",
    "    print(f\"Original: {merged_df['text'].iloc[i]}\")\n",
    "    print(f\"Processed: {merged_df['processed_text'].iloc[i]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Keep all columns but add processed text\n",
    "merged_df_final = merged_df[['unique_id', 'text', 'processed_text', 'binary_label_1', 'label_1', 'binary_label_3', 'label_3']]\n",
    "\n",
    "\n",
    "print(f\"\\nProcessed dataset shape: {merged_df_final.shape}\")\n",
    "print(merged_df_final.head())\n",
    "\n",
    "# Extract features (processed text)\n",
    "X = list(merged_df_final['processed_text'])\n",
    "\n",
    "# Extract labels for both tasks\n",
    "y_task1 = merged_df_final['label_1'].values\n",
    "y_task3 = merged_df_final['label_3'].values\n",
    "tokenizer.fit_on_texts(X)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding\n",
    "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "\n",
    "y_task1 = label_encoder.fit_transform(y_task1)\n",
    "y_task3 = label_encoder.fit_transform(y_task3)\n",
    "\n",
    "y_task1 = to_categorical(y_task1, num_classes=2)\n",
    "y_task3 = to_categorical(y_task3, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "479a8898-e59b-40b3-b81b-9610ebd77d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step\n",
      "\n",
      "Task 1 (Gendered Abuse) Test Results:\n",
      "Precision: 0.6594\n",
      "Recall: 0.7521\n",
      "weighted F1 Score: 0.6721\n",
      "macro F1 Score: 0.4654\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not_hate       0.77      0.97      0.86      1159\n",
      "        hate       0.31      0.04      0.07       358\n",
      "\n",
      "    accuracy                           0.75      1517\n",
      "   macro avg       0.54      0.51      0.47      1517\n",
      "weighted avg       0.66      0.75      0.67      1517\n",
      "\n",
      "\n",
      "Task 3 (Explicit Language) Test Results:\n",
      "Precision: 0.6912\n",
      "Recall: 0.5241\n",
      "weighted F1 Score: 0.5578\n",
      "macro F1 Score: 0.5006\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "not_explicit       0.82      0.48      0.61      1159\n",
      "    explicit       0.28      0.65      0.39       358\n",
      "\n",
      "    accuracy                           0.52      1517\n",
      "   macro avg       0.55      0.57      0.50      1517\n",
      "weighted avg       0.69      0.52      0.56      1517\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "test_results = evaluate_multi_task_validation(\n",
    "    trained_model, X, y_task1, y_task3\n",
    ")\n",
    "\n",
    "# Print Task 1 (Gendered Abuse) results\n",
    "print(\"\\nTask 1 (Gendered Abuse) Test Results:\")\n",
    "print(f\"Precision: {test_results['task1']['precision']:.4f}\")\n",
    "print(f\"Recall: {test_results['task1']['recall']:.4f}\")\n",
    "print(f\"weighted F1 Score: {test_results['task1']['f1_score_weighted']:.4f}\")\n",
    "print(f\"macro F1 Score: {test_results['task1']['f1_score_macro']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(test_results['task1']['classification_report'])\n",
    "\n",
    "# Print Task 3 (Explicit Language) results\n",
    "print(\"\\nTask 3 (Explicit Language) Test Results:\")\n",
    "print(f\"Precision: {test_results['task3']['precision']:.4f}\")\n",
    "print(f\"Recall: {test_results['task3']['recall']:.4f}\")\n",
    "print(f\"weighted F1 Score: {test_results['task3']['f1_score_weighted']:.4f}\")\n",
    "print(f\"macro F1 Score: {test_results['task3']['f1_score_macro']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(test_results['task3']['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e2e4e-bcf7-488d-9524-e8014e873c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
